{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33f85e9d-215f-45c6-95ad-d56e6e3d3759",
   "metadata": {
    "id": "33f85e9d-215f-45c6-95ad-d56e6e3d3759"
   },
   "source": [
    "#### CS20M059 Shibobrota Das | CS20M007 Abhishek Kumar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6700a5ec-7c66-45f5-9539-a593bb36ddc6",
   "metadata": {
    "id": "6700a5ec-7c66-45f5-9539-a593bb36ddc6"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b51cb32-fdbd-4f91-96ca-91ee41122b6b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b51cb32-fdbd-4f91-96ca-91ee41122b6b",
    "outputId": "7ba68143-c4f2-49b5-d083-8b245fd9b02d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 686kB 9.2MB/s eta 0:00:01\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-addons -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a08fb4b5-e93b-4c82-bc45-e2a22a7dee95",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a08fb4b5-e93b-4c82-bc45-e2a22a7dee95",
    "outputId": "55823e8a-5d43-47e4-f211-b8773fea1280"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 1.8MB 8.4MB/s \n",
      "\u001b[K     |████████████████████████████████| 133kB 51.7MB/s \n",
      "\u001b[K     |████████████████████████████████| 174kB 38.2MB/s \n",
      "\u001b[K     |████████████████████████████████| 102kB 12.8MB/s \n",
      "\u001b[K     |████████████████████████████████| 71kB 8.2MB/s \n",
      "\u001b[?25h  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c70406c-fe4d-43f1-93a4-9dc617b301da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5c70406c-fe4d-43f1-93a4-9dc617b301da",
    "outputId": "f04d4e3f-585e-4d71-ada8-83db1612d003"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using numpy: 1.19.5\n",
      "Using tensorflow: 2.5.0\n",
      "Using tensorflow Addons: 0.13.0\n",
      "Using keras: 2.5.0\n",
      "Using pandas: 1.2.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import GradientTape\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU, SimpleRNN, SimpleRNNCell, LSTMCell, GRUCell\n",
    "from keras.models import Sequential\n",
    "from keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy\n",
    "import time\n",
    "import sys\n",
    "import datetime\n",
    "from sklearn.utils import shuffle\n",
    "import wandb\n",
    "# import nltk\n",
    "import csv\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "print(\"Using numpy:\",np.__version__)\n",
    "print(\"Using tensorflow:\",tf.__version__)\n",
    "print(\"Using tensorflow Addons:\",tfa.__version__)\n",
    "print(\"Using keras:\",keras.__version__)\n",
    "print(\"Using pandas:\",pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf43393-e89f-46f4-a711-f2e7078e9e85",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 631
    },
    "id": "ddf43393-e89f-46f4-a711-f2e7078e9e85",
    "outputId": "928563a1-ca11-4647-e59b-2ecd1757b9ff"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2ieo5mk6) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:2ieo5mk6). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">crisp-water-211</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203\" target=\"_blank\">https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203/runs/6g6t6l2c\" target=\"_blank\">https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203/runs/6g6t6l2c</a><br/>\n",
       "                Run data is saved locally in <code>/content/drive/My Drive/A3-checkpoints/wandb/run-20210520_185916-6g6t6l2c</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(6g6t6l2c)</h1><iframe src=\"https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203/runs/6g6t6l2c\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fa6c5de0bd0>"
      ]
     },
     "execution_count": 119,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wandb.init(project='Assignment 3', entity='iitm-cs6910-jan-may-2021-cs20m059-cs20m007')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cd226e1-96b2-4440-a310-15a8cf2f0035",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4cd226e1-96b2-4440-a310-15a8cf2f0035",
    "outputId": "77f6e150-a1e9-4fdd-8fbb-91afc05dd3de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/My Drive/DL-A3 Dataset/dakshina_dataset_v1.0/hi\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd '/content/drive/My Drive/DL-A3 Dataset/dakshina_dataset_v1.0/hi/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559c92d9-374e-4e0f-b2d1-7f17d895cc04",
   "metadata": {
    "id": "559c92d9-374e-4e0f-b2d1-7f17d895cc04"
   },
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee85d467-1fac-40d1-a7ca-b00faf2d3d4b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ee85d467-1fac-40d1-a7ca-b00faf2d3d4b",
    "outputId": "bd5ad0b6-4205-4998-92ae-ee2ef88a1724"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded to Dataframes!\n"
     ]
    }
   ],
   "source": [
    "val_df = pd.read_csv(\"./lexicons/hi.translit.sampled.dev.tsv\", sep='\\t', header=None)\n",
    "train_df = pd.read_csv(\"./lexicons/hi.translit.sampled.train.tsv\", sep='\\t', header=None)\n",
    "test_df = pd.read_csv(\"./lexicons/hi.translit.sampled.test.tsv\", sep='\\t', header=None)\n",
    "print(\"Data Loaded to Dataframes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13ae78d1-de5a-435a-8b39-088bd63f8e2c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "13ae78d1-de5a-435a-8b39-088bd63f8e2c",
    "outputId": "a6d3d8a2-6b85-4b3f-9756-37e8c7af2a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/A3-checkpoints\n"
     ]
    }
   ],
   "source": [
    "%cd '/content/drive/My Drive/A3-checkpoints/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa6b5e6-844e-4fb9-9eeb-bcedd0cf6fd1",
   "metadata": {
    "id": "caa6b5e6-844e-4fb9-9eeb-bcedd0cf6fd1"
   },
   "source": [
    "#### Dataset Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fefa85bd-2299-4503-8d29-458451185e8f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "fefa85bd-2299-4503-8d29-458451185e8f",
    "outputId": "2c61f86e-47d3-43c6-ab96-6beb9d416cd5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7933</th>\n",
       "      <td>किड्स</td>\n",
       "      <td>kids</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24721</th>\n",
       "      <td>प्रसाधन</td>\n",
       "      <td>prasadhan</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37294</th>\n",
       "      <td>वेक्स</td>\n",
       "      <td>wax</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1  2\n",
       "7933     किड्स       kids  6\n",
       "24721  प्रसाधन  prasadhan  2\n",
       "37294    वेक्स        wax  2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627e8bdb-9997-43b6-ba7e-e05879c1c4c0",
   "metadata": {
    "id": "627e8bdb-9997-43b6-ba7e-e05879c1c4c0"
   },
   "source": [
    "## Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53abf538-55d3-46e0-9532-f0aded169217",
   "metadata": {
    "id": "53abf538-55d3-46e0-9532-f0aded169217"
   },
   "outputs": [],
   "source": [
    "sos = \"@\"\n",
    "eos = \"#\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7035ff0-c322-4ef6-98ac-eae0416ad3a7",
   "metadata": {
    "id": "e7035ff0-c322-4ef6-98ac-eae0416ad3a7"
   },
   "outputs": [],
   "source": [
    "class LexDataset:\n",
    "    def __init__(self, input_tensor, target_tensor, batch_size):\n",
    "        self.input_tensor = input_tensor\n",
    "        self.target_tensor = target_tensor\n",
    "        self.batch = tf.data.Dataset.from_tensor_slices((self.input_tensor, self.target_tensor)).shuffle(len(self.input_tensor)).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce3cd7e2-7e30-44bb-8a27-77dbf944bea0",
   "metadata": {
    "id": "ce3cd7e2-7e30-44bb-8a27-77dbf944bea0"
   },
   "outputs": [],
   "source": [
    "class TransliterationDatatset:\n",
    "    def __init__(self, df_list, batch_size = 64):\n",
    "        \n",
    "        self.input_tokenizer = None\n",
    "        self.target_tokenizer = None\n",
    "        self.train = None\n",
    "        self.val = None\n",
    "        self.test = None\n",
    "        self.batch_size = batch_size\n",
    "        # Load Data\n",
    "        self.load_dataset(df_list)\n",
    "        # Other parameters\n",
    "        self.num_input_tokens = len(self.input_tokenizer.index_word)+1\n",
    "        self.num_target_tokens = len(self.target_tokenizer.index_word)+1\n",
    "        self.max_input_seq_length = np.max([self.train.input_tensor.shape[1], self.val.input_tensor.shape[1], self.test.input_tensor.shape[1]])\n",
    "        self.max_target_seq_length = np.max([self.train.target_tensor.shape[1], self.val.target_tensor.shape[1], self.test.target_tensor.shape[1]])\n",
    "        \n",
    "    def preprocess_word(self, w):\n",
    "        return sos + str(w) + eos\n",
    "    \n",
    "    def print_input(self, tensor):\n",
    "        for t in tensor:\n",
    "            if t != 0:\n",
    "                print(f'{t} ----> {self.input_tokenizer.index_word[t]}')\n",
    "                \n",
    "    def print_target(self, tensor):\n",
    "        for t in tensor:\n",
    "            if t != 0:\n",
    "                print(f'{t} ----> {self.target_tokenizer.index_word[t]}')\n",
    "    \n",
    "    def create_dataset(self, data_frame):\n",
    "        input_words = []\n",
    "        target_words = []\n",
    "        # Shuffle the data_frame before creating dataset\n",
    "        df = data_frame\n",
    "        for i in range(5):\n",
    "            df = shuffle(df)\n",
    "        for x, y in zip(df[1], df[0]):\n",
    "            input_words.append(self.preprocess_word(x))\n",
    "            target_words.append(self.preprocess_word(y))\n",
    "        return (input_words, target_words)\n",
    "    \n",
    "    def load_dataset(self, df_list):\n",
    "        # df_list should have train -> val -> test in sequence\n",
    "        \n",
    "        self.input_tokenizer = Tokenizer(num_words = None, char_level = True)\n",
    "        self.target_tokenizer = Tokenizer(num_words = None, char_level = True)\n",
    "        \n",
    "        ds_list = []\n",
    "        \n",
    "        for df in df_list:\n",
    "            # Get the words list\n",
    "            (input_words, target_words) = self.create_dataset(df)\n",
    "            # Fit on the set of words\n",
    "            self.input_tokenizer.fit_on_texts(input_words)\n",
    "            self.target_tokenizer.fit_on_texts(target_words)\n",
    "            ds_list.append((input_words, target_words))\n",
    "                    \n",
    "        self.target_tokenizer.index_word.update({0:\" \"})\n",
    "        self.input_tokenizer.index_word.update({0:\" \"})\n",
    "        \n",
    "        input_word_len = []\n",
    "        target_word_len = []\n",
    "        \n",
    "        tensor_list = []\n",
    "        \n",
    "        for i, (input_words, target_words) in enumerate(ds_list):\n",
    "            input_tensor = self.input_tokenizer.texts_to_sequences(input_words)\n",
    "            target_tensor = self.target_tokenizer.texts_to_sequences(target_words)\n",
    "            tensor_list.append((input_tensor, target_tensor))\n",
    "            input_word_len.append(np.max([len(x) for x in input_tensor]))\n",
    "            target_word_len.append(np.max([len(x) for x in target_tensor]))\n",
    "        \n",
    "        for i, (input_tensor, target_tensor) in enumerate(tensor_list):\n",
    "            \n",
    "            input_tensor = pad_sequences(input_tensor, padding='post', maxlen = np.max(input_word_len))\n",
    "            target_tensor = pad_sequences(target_tensor, padding='post', maxlen = np.max(target_word_len))\n",
    "            \n",
    "            if i == 0:\n",
    "                self.train = LexDataset(input_tensor, target_tensor, self.batch_size)\n",
    "            elif i == 1:\n",
    "                self.val = LexDataset(input_tensor, target_tensor, self.batch_size)\n",
    "            else:\n",
    "                self.test = LexDataset(input_tensor, target_tensor, self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63b06317-dab4-4455-812e-1b34679e27b1",
   "metadata": {
    "id": "63b06317-dab4-4455-812e-1b34679e27b1",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = TransliterationDatatset([train_df, val_df, test_df], 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb817ef-9206-4341-807a-4b4db2113340",
   "metadata": {
    "id": "1fb817ef-9206-4341-807a-4b4db2113340"
   },
   "source": [
    "#### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1da1956c-d5be-4d10-a263-f565341e5d6a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1da1956c-d5be-4d10-a263-f565341e5d6a",
    "outputId": "394e25f3-17c7-45a6-ee13-256b4d740821",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((44204, 22), (44204, 21))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training data\n",
    "dataset.train.input_tensor.shape, dataset.train.target_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a008930-d6da-4be0-898c-153f49b8845a",
   "metadata": {
    "id": "3a008930-d6da-4be0-898c-153f49b8845a"
   },
   "source": [
    "#### Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3c2a7df-ec58-40b6-9ba1-6b7782fdfa0c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3c2a7df-ec58-40b6-9ba1-6b7782fdfa0c",
    "outputId": "2de042fe-d861-40da-ace5-00c13b9f70fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4358, 22), (4358, 21))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation data\n",
    "dataset.val.input_tensor.shape, dataset.val.target_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da416c97-6ef6-4855-8968-dfcd1fe8ff5a",
   "metadata": {
    "id": "da416c97-6ef6-4855-8968-dfcd1fe8ff5a"
   },
   "source": [
    "#### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa011970-5547-4783-a89d-8ab8c3438fa6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fa011970-5547-4783-a89d-8ab8c3438fa6",
    "outputId": "cdaff5ef-5447-445e-b40c-acd72b9c82e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4502, 22), (4502, 21))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test data\n",
    "dataset.test.input_tensor.shape, dataset.test.target_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7ada4f-733d-4301-93b2-1ade66cde9a8",
   "metadata": {
    "id": "8a7ada4f-733d-4301-93b2-1ade66cde9a8"
   },
   "source": [
    "#### Number of Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bba2c1c4-7fb6-454b-8210-13398ed7406b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bba2c1c4-7fb6-454b-8210-13398ed7406b",
    "outputId": "0b27cbe7-d682-4d8f-9f09-6a8f8d0c050b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 67)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of tokens\n",
    "dataset.num_input_tokens, dataset.num_target_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a249a5-d5ea-46c6-ab4f-a7d0e08b05f3",
   "metadata": {
    "id": "01a249a5-d5ea-46c6-ab4f-a7d0e08b05f3"
   },
   "source": [
    "#### Maximum Sequence Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c96afcb-d6ff-4b44-89f2-5d0797709ce4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7c96afcb-d6ff-4b44-89f2-5d0797709ce4",
    "outputId": "18aa41f7-c55e-404b-97c7-7945daefb4a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 21)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max seq length\n",
    "dataset.max_input_seq_length, dataset.max_target_seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef24fd33-e9fd-47c7-be52-62705caf2ebc",
   "metadata": {
    "id": "ef24fd33-e9fd-47c7-be52-62705caf2ebc"
   },
   "source": [
    "#### Example batch - dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3c95122-8a50-482b-ae5d-994a74f31765",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3c95122-8a50-482b-ae5d-994a74f31765",
    "outputId": "59ce31d7-118a-4d7d-f58c-3a6c7ce2b8d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([128, 22]), TensorShape([128, 21]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset.train.batch))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc25ddd3-8cd9-4f65-98de-b4f5bddf8c13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cc25ddd3-8cd9-4f65-98de-b4f5bddf8c13",
    "outputId": "bc9bd63c-1d72-43c1-d7fe-4afb6358022f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 ----> @\n",
      "9 ----> s\n",
      "6 ----> h\n",
      "10 ----> e\n",
      "7 ----> r\n",
      "3 ----> #\n"
     ]
    }
   ],
   "source": [
    "dataset.print_input(example_input_batch[2].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc385626-2c47-48cf-88c6-a8bce1ab1cdb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cc385626-2c47-48cf-88c6-a8bce1ab1cdb",
    "outputId": "a5f3e01c-9bd2-4ee2-f3be-0f9d99cb4472"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ----> @\n",
      "27 ----> श\n",
      "14 ----> े\n",
      "4 ----> र\n",
      "2 ----> #\n"
     ]
    }
   ],
   "source": [
    "dataset.print_target(example_target_batch[2].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81646f07-05bf-4c75-aaf4-ce964bf9e458",
   "metadata": {
    "id": "81646f07-05bf-4c75-aaf4-ce964bf9e458"
   },
   "source": [
    "## Encoder and Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29f2c233-4f9a-471a-8ff9-dfe5950a4963",
   "metadata": {
    "id": "29f2c233-4f9a-471a-8ff9-dfe5950a4963"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz, dropout=0.2, layer_type=\"GRU\"):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.layer_type = layer_type\n",
    "\n",
    "        ##-------- RNN layer in Encoder ------- ##\n",
    "        if self.layer_type == \"LSTM\":\n",
    "            self.layer = LSTM(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       dropout = dropout,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "            \n",
    "        elif self.layer_type == \"GRU\":\n",
    "            self.layer = GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       dropout = dropout,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "           \n",
    "        else:\n",
    "            self.layer = SimpleRNN(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       dropout = dropout,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, inputs, hidden):\n",
    "        inputs = self.embedding(inputs)\n",
    "        if self.layer_type == \"LSTM\":\n",
    "            output, h, c = self.layer(inputs, initial_state = hidden)\n",
    "            return output, h, c\n",
    "        else:\n",
    "            output, h = self.layer(inputs, initial_state = hidden)\n",
    "            return output, h, None\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        if self.layer_type == \"LSTM\":\n",
    "            return [tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))]\n",
    "        else:\n",
    "            return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59648301-2c83-4d6e-b066-3bb2ff53650c",
   "metadata": {
    "id": "59648301-2c83-4d6e-b066-3bb2ff53650c"
   },
   "outputs": [],
   "source": [
    "vocab_inp_size = dataset.num_input_tokens\n",
    "embedding_dim = 64\n",
    "units = 256\n",
    "BATCH_SIZE = dataset.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8911625-ecf9-41b6-9234-7f241a71dd4a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8911625-ecf9-41b6-9234-7f241a71dd4a",
    "outputId": "6e833b6b-7b94-4132-b99a-7960817fabbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (128, 22, 256)\n",
      "Encoder Hidden state shape: (batch size, units) (128, 256)\n",
      "Encoder c vector shape: (batch size, units) (128, 256)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE, 0.2, \"LSTM\")\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden, sample_cell = encoder(example_input_batch, sample_hidden)\n",
    "print('Encoder output shape: (batch size, sequence length, units)', sample_output.shape)\n",
    "print('Encoder Hidden state shape: (batch size, units)', sample_hidden.shape)\n",
    "if encoder.layer_type == \"LSTM\":\n",
    "    print ('Encoder c vector shape: (batch size, units) {}'.format(sample_cell.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adbd8f1a-53b4-4d8d-bff6-478f2718bcd4",
   "metadata": {
    "id": "adbd8f1a-53b4-4d8d-bff6-478f2718bcd4"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, dropout=0.2, layer_type=\"GRU\"):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.layer_type = layer_type\n",
    "        \n",
    "        ##-------- RNN layer in Encoder ------- ##\n",
    "        if self.layer_type == \"LSTM\":\n",
    "            self.layer = LSTM(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       dropout = dropout,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "            \n",
    "        elif self.layer_type == \"GRU\":\n",
    "            self.layer = GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       dropout = dropout,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "           \n",
    "        else:\n",
    "            self.layer = SimpleRNN(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       dropout = dropout,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU/SimpleRNN\n",
    "        if self.layer_type != \"LSTM\":\n",
    "            output, state_h = self.layer(x, initial_state = hidden)\n",
    "        else:\n",
    "            output, state_h, state_c = self.layer(x, initial_state = hidden)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        # return x, state\n",
    "        if self.layer_type != \"LSTM\":\n",
    "            return x, state_h, None\n",
    "        else:\n",
    "            return x, state_h, state_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7fb80f6-6d51-4b88-89aa-b10d4cbcb950",
   "metadata": {
    "id": "f7fb80f6-6d51-4b88-89aa-b10d4cbcb950"
   },
   "outputs": [],
   "source": [
    "vocab_tar_size = dataset.num_target_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4dd296f-1d52-4b1a-9caa-e56ae23af6ea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4dd296f-1d52-4b1a-9caa-e56ae23af6ea",
    "outputId": "e8eaaa9d-89d6-4a08-b49d-338c7c23d72c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (128, 67)\n",
      "Decoder Hidden state shape: (batch size, units) (128, 256)\n",
      "Encoder c vector shape: (batch size, units) (128, 256)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, 0.2, \"LSTM\")\n",
    "\n",
    "if decoder.layer_type != \"LSTM\":\n",
    "    sample_decoder_output, sample_decoder_hidden, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_hidden)\n",
    "else:\n",
    "    sample_decoder_output, sample_decoder_hidden, sample_decoder_cell = decoder(tf.random.uniform((BATCH_SIZE, 1)), [sample_hidden, sample_cell])\n",
    "\n",
    "print('Decoder output shape: (batch_size, vocab size)', sample_decoder_output.shape)\n",
    "print('Decoder Hidden state shape: (batch size, units)', sample_decoder_hidden.shape)\n",
    "if encoder.layer_type == \"LSTM\":\n",
    "    print ('Encoder c vector shape: (batch size, units) {}'.format(sample_decoder_cell.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d718ae-5b4a-472f-9d62-e3b72a468217",
   "metadata": {
    "id": "51d718ae-5b4a-472f-9d62-e3b72a468217"
   },
   "source": [
    "## Optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d21003ab-70ee-455d-a0bd-4e16a87f6c70",
   "metadata": {
    "id": "d21003ab-70ee-455d-a0bd-4e16a87f6c70"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Nadam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    loss_ = tf.reduce_mean(loss_)\n",
    "    return loss_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e633463-5d0c-4080-9e0d-5cc8cb88a9b9",
   "metadata": {
    "id": "7e633463-5d0c-4080-9e0d-5cc8cb88a9b9"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "90973b6a-e060-4e22-94f6-2400e7d700f2",
   "metadata": {
    "id": "90973b6a-e060-4e22-94f6-2400e7d700f2"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden, enc_cell = encoder(inp, enc_hidden)\n",
    "        \n",
    "        if decoder.layer_type != \"LSTM\":\n",
    "            dec_hidden = enc_hidden\n",
    "        else:\n",
    "            dec_hidden, dec_cell = enc_hidden, enc_cell\n",
    "\n",
    "        dec_input = tf.expand_dims([dataset.target_tokenizer.word_index[sos]] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            \n",
    "            # passing enc_output to the decoder\n",
    "            if decoder.layer_type != \"LSTM\":\n",
    "                predictions, _, _ = decoder(dec_input, dec_hidden)\n",
    "            else:\n",
    "                predictions, _, _ = decoder(dec_input, [dec_hidden, dec_cell])\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df721112-cd36-4e6b-8608-3a1e06fcab84",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "df721112-cd36-4e6b-8608-3a1e06fcab84",
    "outputId": "f0be295c-2c75-4c88-d4a5-6de4e6646ad5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.5269\n",
      "Epoch 1 Batch 100 Loss 1.0398\n",
      "Epoch 1 Batch 200 Loss 1.0099\n",
      "Epoch 1 Batch 300 Loss 0.9548\n",
      "Epoch 1 Loss 1.0231\n",
      "Time taken for 1 epoch 225.84 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "steps_per_epoch = len(dataset.train.input_tensor)//BATCH_SIZE\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.train.batch.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss/steps_per_epoch:.4f}')\n",
    "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6421ac1f-81fc-4cec-9094-88b21ba143a6",
   "metadata": {
    "id": "6421ac1f-81fc-4cec-9094-88b21ba143a6"
   },
   "source": [
    "## Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5643c560-2f13-43ec-aba5-8fd6eb23762a",
   "metadata": {
    "id": "5643c560-2f13-43ec-aba5-8fd6eb23762a"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((dataset.max_target_seq_length, dataset.max_input_seq_length))\n",
    "\n",
    "    sentence = dataset.preprocess_word(sentence)\n",
    "\n",
    "    inputs = [dataset.input_tokenizer.word_index[i] for i in sentence]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=dataset.max_input_seq_length,\n",
    "                                                         padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    if encoder.layer_type != \"LSTM\":\n",
    "        hidden = [tf.zeros((1, units))]\n",
    "    else:\n",
    "        hidden = [tf.zeros((1, units)), tf.zeros((1, units))]\n",
    "    \n",
    "    enc_out, enc_hidden, enc_cell = encoder(inputs, hidden)\n",
    "    \n",
    "    if decoder.layer_type != \"LSTM\":\n",
    "        dec_hidden = enc_hidden\n",
    "    else:\n",
    "        dec_hidden, dec_cell = enc_hidden, enc_cell\n",
    "\n",
    "    dec_input = tf.expand_dims([dataset.target_tokenizer.word_index[sos]], 0)\n",
    "\n",
    "    for t in range(dataset.max_target_seq_length):\n",
    "        # passing enc_output to the decoder\n",
    "        if decoder.layer_type != \"LSTM\":\n",
    "            predictions, _, _ = decoder(dec_input, dec_hidden)\n",
    "        else:\n",
    "            predictions, _, _ = decoder(dec_input, [dec_hidden, dec_cell])\n",
    "                \n",
    "        \n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "        result += dataset.target_tokenizer.index_word[predicted_id]\n",
    "\n",
    "        if dataset.target_tokenizer.index_word[predicted_id] == eos:\n",
    "            return result, sentence\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9db2c059-3601-459b-b285-08cb6bda4f84",
   "metadata": {
    "id": "9db2c059-3601-459b-b285-08cb6bda4f84"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence = evaluate(sentence)\n",
    "\n",
    "    print('Input:', sentence)\n",
    "    print('Predicted translation:', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "00985e4f-7438-416c-93b0-647556ce517b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "id": "00985e4f-7438-416c-93b0-647556ce517b",
    "outputId": "b17f9a4a-7a05-4775-f18c-ad380b62781e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: @saa#\n",
      "Predicted translation: स्र#\n"
     ]
    }
   ],
   "source": [
    "translate(\"saa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "A7n3LGKsiN8e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A7n3LGKsiN8e",
    "outputId": "11ecf21f-7a56-4658-f57c-d689b1883459"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9,  6,  5, 19, 11, 19,  7, 11,  8,  1])"
      ]
     },
     "execution_count": 99,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = dataset.input_tokenizer.texts_to_sequences(\"shibobrota\")\n",
    "np.reshape(sequence, len(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u_Y_4LsajI-B",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u_Y_4LsajI-B",
    "outputId": "d12ff317-4c22-4cca-ab34-27b1f66ef754"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s', 'h', 'i', 'b', 'o', 'b', 'r', 'o', 't', 'a']"
      ]
     },
     "execution_count": 100,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = dataset.input_tokenizer.sequences_to_texts(sequence)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "zU-lii9njfj4",
   "metadata": {
    "id": "zU-lii9njfj4"
   },
   "outputs": [],
   "source": [
    "def save_predictions(data_frame, name):\n",
    "    accuracy_count = 0;\n",
    "    with open(name, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"INPUT\", \"PREDICTION\", \"TRUE\"])\n",
    "        for i, (inp, trg) in enumerate(zip(data_frame[1], data_frame[0])): \n",
    "            result, sentence, attention_plot = evaluate(inp)\n",
    "            writer.writerow([inp, result[:-1], trg])\n",
    "            print(inp, result[:-1], trg)\n",
    "            if result[:-1] == trg:\n",
    "                accuracy_count += 1\n",
    "            if (i+1) % 100 == 0 or i+1 == data_frame.size:\n",
    "                print(\"Accuracy\", (accuracy_count / (i+1)))\n",
    "\n",
    "    return accuracy_count/data_frame.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kkzktH4_qvaB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kkzktH4_qvaB",
    "outputId": "2b53e599-55b1-4135-a3a3-547267683c53"
   },
   "outputs": [],
   "source": [
    "save_predictions(test_df, \"new_without_attn_predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Without Attention-Copy1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
