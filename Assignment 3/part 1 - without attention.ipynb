{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33f85e9d-215f-45c6-95ad-d56e6e3d3759",
   "metadata": {
    "id": "33f85e9d-215f-45c6-95ad-d56e6e3d3759"
   },
   "source": [
    "#### CS20M059 Shibobrota Das | CS20M007 Abhishek Kumar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7c046e-517c-4c7f-bbe9-010f5499438c",
   "metadata": {
    "id": "ff7c046e-517c-4c7f-bbe9-010f5499438c"
   },
   "source": [
    "#### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba063b7-f71a-44c6-a6de-81bd75238c95",
   "metadata": {
    "id": "8ba063b7-f71a-44c6-a6de-81bd75238c95"
   },
   "source": [
    "This Assignment is developed on colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc949732-73e2-48b4-97ac-e4e74435c1a3",
   "metadata": {
    "id": "fc949732-73e2-48b4-97ac-e4e74435c1a3"
   },
   "source": [
    "The dataset is downloaded into Google Drive at path ```/content/drive/My Drive/DL-A3 Dataset/```. Before running this code, make sure to download and extract the dataset in the mentioned google drive path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2504a976-9cc8-45be-b2ad-7c05fe570fa2",
   "metadata": {
    "id": "2504a976-9cc8-45be-b2ad-7c05fe570fa2"
   },
   "source": [
    "This script requires you to mount google drive on colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6700a5ec-7c66-45f5-9539-a593bb36ddc6",
   "metadata": {
    "id": "6700a5ec-7c66-45f5-9539-a593bb36ddc6"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a08fb4b5-e93b-4c82-bc45-e2a22a7dee95",
   "metadata": {
    "id": "a08fb4b5-e93b-4c82-bc45-e2a22a7dee95"
   },
   "outputs": [],
   "source": [
    "!pip install wandb -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c70406c-fe4d-43f1-93a4-9dc617b301da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5c70406c-fe4d-43f1-93a4-9dc617b301da",
    "outputId": "3d1b8399-94ac-4477-c246-c715a12b3c52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using numpy: 1.19.5\n",
      "Using tensorflow: 2.5.0\n",
      "Using keras: 2.5.0\n",
      "Using pandas: 1.2.4\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mlp\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "from tensorflow import GradientTape\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU, SimpleRNN, SimpleRNNCell, LSTMCell, GRUCell\n",
    "from keras.models import Sequential\n",
    "from keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy\n",
    "import time\n",
    "import sys\n",
    "import datetime\n",
    "from sklearn.utils import shuffle\n",
    "import wandb\n",
    "# import nltk\n",
    "import csv\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "print(\"Using numpy:\",np.__version__)\n",
    "print(\"Using tensorflow:\",tf.__version__)\n",
    "print(\"Using keras:\",keras.__version__)\n",
    "print(\"Using pandas:\",pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ddf43393-e89f-46f4-a711-f2e7078e9e85",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 631
    },
    "id": "ddf43393-e89f-46f4-a711-f2e7078e9e85",
    "outputId": "e1ab6790-f38e-4d40-a89f-9d1ec6e500c0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2p502ao2) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 11940<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.06MB of 0.06MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>D:\\Deep Learning\\cs6910-assignments\\Assignment 3\\wandb\\run-20210525_224133-2p502ao2\\logs\\debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>D:\\Deep Learning\\cs6910-assignments\\Assignment 3\\wandb\\run-20210525_224133-2p502ao2\\logs\\debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>_runtime</td><td>7387</td></tr><tr><td>_timestamp</td><td>1621970085</td></tr><tr><td>_step</td><td>4</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>_runtime</td><td>▁▁███</td></tr><tr><td>_timestamp</td><td>▁▁███</td></tr><tr><td>_step</td><td>▁▃▅▆█</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 5 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">rose-sponge-433</strong>: <a href=\"https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203/runs/2p502ao2\" target=\"_blank\">https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203/runs/2p502ao2</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:2p502ao2). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">fine-durian-435</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203\" target=\"_blank\">https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203/runs/2l85h7ug\" target=\"_blank\">https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203/runs/2l85h7ug</a><br/>\n",
       "                Run data is saved locally in <code>D:\\Deep Learning\\cs6910-assignments\\Assignment 3\\wandb\\run-20210526_004542-2l85h7ug</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(2l85h7ug)</h1><iframe src=\"https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203/runs/2l85h7ug\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x17b3e4c2a90>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wandb.init(project='Assignment 3', entity='iitm-cs6910-jan-may-2021-cs20m059-cs20m007')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cd226e1-96b2-4440-a310-15a8cf2f0035",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4cd226e1-96b2-4440-a310-15a8cf2f0035",
    "outputId": "526ea2cc-d41c-4d43-ce04-8ff87ed743b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "/content/drive/My Drive/DL-A3 Dataset/dakshina_dataset_v1.0/hi\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd '/content/drive/My Drive/DL-A3 Dataset/dakshina_dataset_v1.0/hi/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559c92d9-374e-4e0f-b2d1-7f17d895cc04",
   "metadata": {
    "id": "559c92d9-374e-4e0f-b2d1-7f17d895cc04"
   },
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee85d467-1fac-40d1-a7ca-b00faf2d3d4b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ee85d467-1fac-40d1-a7ca-b00faf2d3d4b",
    "outputId": "483321ef-ad78-41fc-d7f7-dcaa6fdb7b66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded to Dataframes!\n"
     ]
    }
   ],
   "source": [
    "val_df = pd.read_csv(\"./lexicons/hi.translit.sampled.dev.tsv\", sep='\\t', header=None)\n",
    "train_df = pd.read_csv(\"./lexicons/hi.translit.sampled.train.tsv\", sep='\\t', header=None)\n",
    "test_df = pd.read_csv(\"./lexicons/hi.translit.sampled.test.tsv\", sep='\\t', header=None)\n",
    "print(\"Data Loaded to Dataframes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13ae78d1-de5a-435a-8b39-088bd63f8e2c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "13ae78d1-de5a-435a-8b39-088bd63f8e2c",
    "outputId": "23c8b001-9cb0-4593-b89f-aa6193631caa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/A3-checkpoints\n"
     ]
    }
   ],
   "source": [
    "%cd '/content/drive/My Drive/A3-checkpoints/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa6b5e6-844e-4fb9-9eeb-bcedd0cf6fd1",
   "metadata": {
    "id": "caa6b5e6-844e-4fb9-9eeb-bcedd0cf6fd1"
   },
   "source": [
    "#### Dataset Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fefa85bd-2299-4503-8d29-458451185e8f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "fefa85bd-2299-4503-8d29-458451185e8f",
    "outputId": "598730a3-0dca-4e4e-a70b-e0643e799aff"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29734</th>\n",
       "      <td>मदीना</td>\n",
       "      <td>medina</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27496</th>\n",
       "      <td>बियर</td>\n",
       "      <td>bear</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29583</th>\n",
       "      <td>मछली</td>\n",
       "      <td>machali</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0        1  2\n",
       "29734  मदीना   medina  1\n",
       "27496   बियर     bear  1\n",
       "29583   मछली  machali  1"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627e8bdb-9997-43b6-ba7e-e05879c1c4c0",
   "metadata": {
    "id": "627e8bdb-9997-43b6-ba7e-e05879c1c4c0"
   },
   "source": [
    "## Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53abf538-55d3-46e0-9532-f0aded169217",
   "metadata": {
    "id": "53abf538-55d3-46e0-9532-f0aded169217"
   },
   "outputs": [],
   "source": [
    "sos = \"@\"\n",
    "eos = \"#\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7035ff0-c322-4ef6-98ac-eae0416ad3a7",
   "metadata": {
    "id": "e7035ff0-c322-4ef6-98ac-eae0416ad3a7"
   },
   "outputs": [],
   "source": [
    "class LexDataset:\n",
    "    def __init__(self, input_tensor, target_tensor, batch_size):\n",
    "        self.input_tensor = input_tensor\n",
    "        self.target_tensor = target_tensor\n",
    "        self.batch = tf.data.Dataset.from_tensor_slices((self.input_tensor, self.target_tensor)).shuffle(len(self.input_tensor)).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce3cd7e2-7e30-44bb-8a27-77dbf944bea0",
   "metadata": {
    "id": "ce3cd7e2-7e30-44bb-8a27-77dbf944bea0"
   },
   "outputs": [],
   "source": [
    "class TransliterationDatatset:\n",
    "    def __init__(self, df_list, batch_size = 64):\n",
    "        \n",
    "        self.input_tokenizer = None\n",
    "        self.target_tokenizer = None\n",
    "        self.train = None\n",
    "        self.val = None\n",
    "        self.test = None\n",
    "        self.batch_size = batch_size\n",
    "        # Load Data\n",
    "        self.load_dataset(df_list)\n",
    "        # Other parameters\n",
    "        self.num_input_tokens = len(self.input_tokenizer.index_word)+1\n",
    "        self.num_target_tokens = len(self.target_tokenizer.index_word)+1\n",
    "        self.max_input_seq_length = np.max([self.train.input_tensor.shape[1], self.val.input_tensor.shape[1], self.test.input_tensor.shape[1]])\n",
    "        self.max_target_seq_length = np.max([self.train.target_tensor.shape[1], self.val.target_tensor.shape[1], self.test.target_tensor.shape[1]])\n",
    "        \n",
    "    def preprocess_word(self, w):\n",
    "        return sos + str(w) + eos\n",
    "    \n",
    "    def print_input(self, tensor):\n",
    "        for t in tensor:\n",
    "            if t != 0:\n",
    "                print(f'{t} ----> {self.input_tokenizer.index_word[t]}')\n",
    "\n",
    "    def get_target_word(self, tensor):\n",
    "        word = []\n",
    "        for t in tensor:\n",
    "            if t != 0:\n",
    "                word.append(self.input_tokenizer.index_word[t])\n",
    "        return \"\".join([ch for ch in word])\n",
    "                \n",
    "    def print_target(self, tensor):\n",
    "        for t in tensor:\n",
    "            if t != 0:\n",
    "                print(f'{t} ----> {self.target_tokenizer.index_word[t]}')\n",
    "    \n",
    "    def create_dataset(self, data_frame):\n",
    "        input_words = []\n",
    "        target_words = []\n",
    "        # Shuffle the data_frame before creating dataset\n",
    "        df = data_frame\n",
    "        for i in range(5):\n",
    "            df = shuffle(df)\n",
    "        for x, y in zip(df[1], df[0]):\n",
    "            input_words.append(self.preprocess_word(x))\n",
    "            target_words.append(self.preprocess_word(y))\n",
    "        return (input_words, target_words)\n",
    "    \n",
    "    def load_dataset(self, df_list):\n",
    "        # df_list should have train -> val -> test in sequence\n",
    "        \n",
    "        self.input_tokenizer = Tokenizer(num_words = None, char_level = True)\n",
    "        self.target_tokenizer = Tokenizer(num_words = None, char_level = True)\n",
    "        \n",
    "        ds_list = []\n",
    "        \n",
    "        for df in df_list:\n",
    "            # Get the words list\n",
    "            (input_words, target_words) = self.create_dataset(df)\n",
    "            # Fit on the set of words\n",
    "            self.input_tokenizer.fit_on_texts(input_words)\n",
    "            self.target_tokenizer.fit_on_texts(target_words)\n",
    "            ds_list.append((input_words, target_words))\n",
    "                    \n",
    "        self.target_tokenizer.index_word.update({0:\" \"})\n",
    "        self.input_tokenizer.index_word.update({0:\" \"})\n",
    "        \n",
    "        input_word_len = []\n",
    "        target_word_len = []\n",
    "        \n",
    "        tensor_list = []\n",
    "        \n",
    "        for i, (input_words, target_words) in enumerate(ds_list):\n",
    "            input_tensor = self.input_tokenizer.texts_to_sequences(input_words)\n",
    "            target_tensor = self.target_tokenizer.texts_to_sequences(target_words)\n",
    "            tensor_list.append((input_tensor, target_tensor))\n",
    "            input_word_len.append(np.max([len(x) for x in input_tensor]))\n",
    "            target_word_len.append(np.max([len(x) for x in target_tensor]))\n",
    "        \n",
    "        for i, (input_tensor, target_tensor) in enumerate(tensor_list):\n",
    "            \n",
    "            input_tensor = pad_sequences(input_tensor, padding='post', maxlen = np.max(input_word_len))\n",
    "            target_tensor = pad_sequences(target_tensor, padding='post', maxlen = np.max(target_word_len))\n",
    "            \n",
    "            if i == 0:\n",
    "                self.train = LexDataset(input_tensor, target_tensor, self.batch_size)\n",
    "            elif i == 1:\n",
    "                self.val = LexDataset(input_tensor, target_tensor, self.batch_size)\n",
    "            else:\n",
    "                self.test = LexDataset(input_tensor, target_tensor, self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63b06317-dab4-4455-812e-1b34679e27b1",
   "metadata": {
    "id": "63b06317-dab4-4455-812e-1b34679e27b1",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = TransliterationDatatset([train_df, val_df, test_df], 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb817ef-9206-4341-807a-4b4db2113340",
   "metadata": {
    "id": "1fb817ef-9206-4341-807a-4b4db2113340"
   },
   "source": [
    "#### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1da1956c-d5be-4d10-a263-f565341e5d6a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1da1956c-d5be-4d10-a263-f565341e5d6a",
    "outputId": "51b63214-a892-4c30-c603-f00a102de0ff",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((44204, 22), (44204, 21))"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training data\n",
    "dataset.train.input_tensor.shape, dataset.train.target_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a008930-d6da-4be0-898c-153f49b8845a",
   "metadata": {
    "id": "3a008930-d6da-4be0-898c-153f49b8845a"
   },
   "source": [
    "#### Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3c2a7df-ec58-40b6-9ba1-6b7782fdfa0c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3c2a7df-ec58-40b6-9ba1-6b7782fdfa0c",
    "outputId": "3e4be635-8597-41b3-ffce-dedfc2410ee6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4358, 22), (4358, 21))"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation data\n",
    "dataset.val.input_tensor.shape, dataset.val.target_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da416c97-6ef6-4855-8968-dfcd1fe8ff5a",
   "metadata": {
    "id": "da416c97-6ef6-4855-8968-dfcd1fe8ff5a"
   },
   "source": [
    "#### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa011970-5547-4783-a89d-8ab8c3438fa6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fa011970-5547-4783-a89d-8ab8c3438fa6",
    "outputId": "53b85865-5c1c-4ef5-a242-54dd4be4a61b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4502, 22), (4502, 21))"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test data\n",
    "dataset.test.input_tensor.shape, dataset.test.target_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7ada4f-733d-4301-93b2-1ade66cde9a8",
   "metadata": {
    "id": "8a7ada4f-733d-4301-93b2-1ade66cde9a8"
   },
   "source": [
    "#### Number of Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bba2c1c4-7fb6-454b-8210-13398ed7406b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bba2c1c4-7fb6-454b-8210-13398ed7406b",
    "outputId": "9fcf48b0-1406-48ec-f6fc-7f14b2bbac6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 67)"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of tokens\n",
    "dataset.num_input_tokens, dataset.num_target_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a249a5-d5ea-46c6-ab4f-a7d0e08b05f3",
   "metadata": {
    "id": "01a249a5-d5ea-46c6-ab4f-a7d0e08b05f3"
   },
   "source": [
    "#### Maximum Sequence Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c96afcb-d6ff-4b44-89f2-5d0797709ce4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7c96afcb-d6ff-4b44-89f2-5d0797709ce4",
    "outputId": "1211eb03-b299-4a5c-ea45-76dadc4dc69b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 21)"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max seq length\n",
    "dataset.max_input_seq_length, dataset.max_target_seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef24fd33-e9fd-47c7-be52-62705caf2ebc",
   "metadata": {
    "id": "ef24fd33-e9fd-47c7-be52-62705caf2ebc"
   },
   "source": [
    "#### Example batch - dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3c95122-8a50-482b-ae5d-994a74f31765",
   "metadata": {
    "id": "a3c95122-8a50-482b-ae5d-994a74f31765"
   },
   "outputs": [],
   "source": [
    "# example_input_batch, example_target_batch = next(iter(dataset.train.batch))\n",
    "# example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc25ddd3-8cd9-4f65-98de-b4f5bddf8c13",
   "metadata": {
    "id": "cc25ddd3-8cd9-4f65-98de-b4f5bddf8c13"
   },
   "outputs": [],
   "source": [
    "# dataset.print_input(example_input_batch[2].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc385626-2c47-48cf-88c6-a8bce1ab1cdb",
   "metadata": {
    "id": "cc385626-2c47-48cf-88c6-a8bce1ab1cdb"
   },
   "outputs": [],
   "source": [
    "# dataset.print_target(example_target_batch[2].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81646f07-05bf-4c75-aaf4-ce964bf9e458",
   "metadata": {
    "id": "81646f07-05bf-4c75-aaf4-ce964bf9e458"
   },
   "source": [
    "## Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29f2c233-4f9a-471a-8ff9-dfe5950a4963",
   "metadata": {
    "id": "29f2c233-4f9a-471a-8ff9-dfe5950a4963"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz, dropout=0.2, layer_type=\"GRU\", num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.layer_type = layer_type\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.rnn_layers = []\n",
    "\n",
    "        if self.layer_type == \"LSTM\":\n",
    "            for i in range(self.num_layers):\n",
    "                self.rnn_layers.append(LSTM(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       dropout = self.dropout,\n",
    "                                       recurrent_initializer='glorot_uniform'))\n",
    "        \n",
    "        elif self.layer_type == \"GRU\":\n",
    "            for i in range(self.num_layers):\n",
    "                self.rnn_layers.append(GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       dropout = self.dropout,\n",
    "                                       recurrent_initializer='glorot_uniform'))\n",
    "        \n",
    "        else:\n",
    "            for i in range(self.num_layers):\n",
    "                self.rnn_layers.append(SimpleRNN(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       dropout = self.dropout,\n",
    "                                       recurrent_initializer='glorot_uniform'))        \n",
    "\n",
    "\n",
    "    def call(self, inputs, hidden):\n",
    "        inputs = self.embedding(inputs)\n",
    "        state_h, state_c = [], []\n",
    "\n",
    "        if self.layer_type == \"LSTM\":\n",
    "            output, h, c = self.rnn_layers[0](inputs, initial_state = hidden)\n",
    "            state_h.append(h)\n",
    "            state_c.append(c)\n",
    "            for i in range(1, self.num_layers):\n",
    "                output, h, c = self.rnn_layers[i](output, initial_state = hidden)\n",
    "                state_h.append(h)\n",
    "                state_c.append(c)\n",
    "            return output, state_h, state_c\n",
    "        \n",
    "        elif self.layer_type == \"GRU\":\n",
    "            output, h = self.rnn_layers[0](inputs, initial_state = hidden)\n",
    "            state_h.append(h)\n",
    "            for i in range(1, self.num_layers):\n",
    "                output, h = self.rnn_layers[i](output, initial_state = hidden)\n",
    "                state_h.append(h)\n",
    "            return output, state_h, None\n",
    "           \n",
    "        else:\n",
    "            output, h = self.rnn_layers[0](inputs, initial_state = hidden)\n",
    "            state_h.append(h)\n",
    "            for i in range(1, self.num_layers):\n",
    "                output, h = self.rnn_layers[i](output, initial_state = hidden)\n",
    "                state_h.append(h)\n",
    "            return output, state_h, None\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        if self.layer_type == \"LSTM\":\n",
    "            return [tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))]\n",
    "        else:\n",
    "            return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ei4-t_PQdEBj",
   "metadata": {
    "id": "Ei4-t_PQdEBj"
   },
   "source": [
    "### Test Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59648301-2c83-4d6e-b066-3bb2ff53650c",
   "metadata": {
    "id": "59648301-2c83-4d6e-b066-3bb2ff53650c"
   },
   "outputs": [],
   "source": [
    "# vocab_inp_size = dataset.num_input_tokens\n",
    "# embedding_dim = 64\n",
    "# units = 256\n",
    "# BATCH_SIZE = dataset.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8911625-ecf9-41b6-9234-7f241a71dd4a",
   "metadata": {
    "id": "a8911625-ecf9-41b6-9234-7f241a71dd4a"
   },
   "outputs": [],
   "source": [
    "# encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE, 0.2, \"GRU\", 3)\n",
    "\n",
    "# # sample input\n",
    "# sample_hidden = encoder.initialize_hidden_state()\n",
    "# sample_output, sample_hidden, sample_cell = encoder(example_input_batch, sample_hidden)\n",
    "# print('Encoder output shape: (batch size, sequence length, units)', np.shape(sample_output))\n",
    "# print('Encoder Hidden state shape: (batch size, units)', np.shape(sample_hidden))\n",
    "# if encoder.layer_type == \"LSTM\":\n",
    "#     print ('Encoder c vector shape: (batch size, units) {}'.format(np.shape(sample_cell)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pAiyXfzkd43y",
   "metadata": {
    "id": "pAiyXfzkd43y"
   },
   "source": [
    "## Decoder Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "adbd8f1a-53b4-4d8d-bff6-478f2718bcd4",
   "metadata": {
    "id": "adbd8f1a-53b4-4d8d-bff6-478f2718bcd4"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, dropout=0.2, layer_type=\"GRU\", num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.layer_type = layer_type\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.rnn_layers = []\n",
    "        \n",
    "        if self.layer_type == \"LSTM\":\n",
    "            for i in range(self.num_layers):\n",
    "                self.rnn_layers.append(LSTM(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       dropout = self.dropout,\n",
    "                                       recurrent_initializer='glorot_uniform'))\n",
    "        \n",
    "        elif self.layer_type == \"GRU\":\n",
    "            for i in range(self.num_layers):\n",
    "                self.rnn_layers.append(GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       dropout = self.dropout,\n",
    "                                       recurrent_initializer='glorot_uniform'))\n",
    "           \n",
    "        else:\n",
    "            for i in range(self.num_layers):\n",
    "                self.rnn_layers.append(SimpleRNN(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       dropout = self.dropout,\n",
    "                                       recurrent_initializer='glorot_uniform'))\n",
    "        \n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, state_h, state_c=None):\n",
    "        inputs = self.embedding(inputs)\n",
    "\n",
    "        if self.layer_type == \"LSTM\":\n",
    "            output, h, c = self.rnn_layers[0](inputs, initial_state = [state_h[0], state_c[0]])\n",
    "            for i in range(1, self.num_layers):\n",
    "                output, h, c = self.rnn_layers[i](output, initial_state = [state_h[i], state_c[i]])\n",
    "        \n",
    "        elif self.layer_type == \"GRU\":\n",
    "            output, h = self.rnn_layers[0](inputs, initial_state = state_h[0])\n",
    "            for i in range(1, self.num_layers):\n",
    "                output, h = self.rnn_layers[i](output, initial_state = state_h[i])\n",
    "           \n",
    "        else:\n",
    "            output, h = self.rnn_layers[0](inputs, initial_state = state_h[0])\n",
    "            for i in range(1, self.num_layers):\n",
    "                output, h = self.rnn_layers[i](output, initial_state = state_h[i])\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        # return x, state\n",
    "        if self.layer_type != \"LSTM\":\n",
    "            return x, h, None\n",
    "        else:\n",
    "            return x, h, c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "N6Wr2JTSdI1L",
   "metadata": {
    "id": "N6Wr2JTSdI1L"
   },
   "source": [
    "### Test Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7fb80f6-6d51-4b88-89aa-b10d4cbcb950",
   "metadata": {
    "id": "f7fb80f6-6d51-4b88-89aa-b10d4cbcb950"
   },
   "outputs": [],
   "source": [
    "# vocab_tar_size = dataset.num_target_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4dd296f-1d52-4b1a-9caa-e56ae23af6ea",
   "metadata": {
    "id": "c4dd296f-1d52-4b1a-9caa-e56ae23af6ea"
   },
   "outputs": [],
   "source": [
    "# decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, 0.2, \"GRU\", 3)\n",
    "\n",
    "# if decoder.layer_type != \"LSTM\":\n",
    "#     sample_decoder_output, sample_decoder_hidden, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_hidden, sample_cell)\n",
    "# else:\n",
    "#     sample_decoder_output, sample_decoder_hidden, sample_decoder_cell = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_hidden, sample_cell)\n",
    "\n",
    "# print('Decoder output shape: (batch_size, vocab size)', sample_decoder_output.shape)\n",
    "# print('Decoder Hidden state shape: (batch size, units)', sample_decoder_hidden.shape)\n",
    "# if encoder.layer_type == \"LSTM\":\n",
    "#     print ('Encoder c vector shape: (batch size, units) {}'.format(sample_decoder_cell.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d718ae-5b4a-472f-9d62-e3b72a468217",
   "metadata": {
    "id": "51d718ae-5b4a-472f-9d62-e3b72a468217"
   },
   "source": [
    "## loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "opNBWllg215N",
   "metadata": {
    "id": "opNBWllg215N"
   },
   "outputs": [],
   "source": [
    "def loss_function(real, pred, loss_object):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yw8k1SYOYdiV",
   "metadata": {
    "id": "yw8k1SYOYdiV"
   },
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "uppr-mNwYkdK",
   "metadata": {
    "id": "uppr-mNwYkdK"
   },
   "outputs": [],
   "source": [
    "def accuracy(real, pred):\n",
    "    real = tf.cast(real, tf.int32)\n",
    "    pred = tf.cast(pred, tf.int32)\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(real, pred), tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e633463-5d0c-4080-9e0d-5cc8cb88a9b9",
   "metadata": {
    "id": "7e633463-5d0c-4080-9e0d-5cc8cb88a9b9"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "90973b6a-e060-4e22-94f6-2400e7d700f2",
   "metadata": {
    "id": "90973b6a-e060-4e22-94f6-2400e7d700f2"
   },
   "outputs": [],
   "source": [
    "def train_one_step():\n",
    "    @tf.function\n",
    "    def train_step(encoder, decoder, inp, targ, enc_hidden, optimizer, loss_object, is_val = False):\n",
    "        loss = 0\n",
    "        spc_loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden, enc_cell = encoder(inp, enc_hidden)\n",
    "            if decoder.layer_type != \"LSTM\":\n",
    "                dec_hidden = enc_hidden\n",
    "            else:\n",
    "                dec_hidden, dec_cell = enc_hidden, enc_cell\n",
    "\n",
    "            dec_input = tf.expand_dims([dataset.target_tokenizer.word_index[sos]] * dataset.batch_size, 1)\n",
    "\n",
    "            pred = None\n",
    "\n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                \n",
    "                # passing enc_output to the decoder\n",
    "                if decoder.layer_type != \"LSTM\":\n",
    "                    predictions, _, _ = decoder(dec_input, dec_hidden)\n",
    "                else:\n",
    "                    predictions, _, _ = decoder(dec_input, dec_hidden, dec_cell)\n",
    "                            \n",
    "                loss += loss_function(targ[:, t], predictions, loss_object)\n",
    "\n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "                if t == 1:\n",
    "                    pred = tf.expand_dims(tf.argmax(predictions, axis=-1), 1)\n",
    "                else:\n",
    "                    pred = tf.concat([pred, tf.expand_dims(tf.argmax(predictions, axis=-1), 1)], 1)\n",
    "        \n",
    "        batch_accuracy = accuracy(targ[:, 1:], pred)\n",
    "\n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "        if not is_val:\n",
    "            variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "            gradients = tape.gradient(loss, variables)\n",
    "\n",
    "            optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "        return batch_loss, batch_accuracy\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "BTdsr-zAzVrI",
   "metadata": {
    "id": "BTdsr-zAzVrI"
   },
   "outputs": [],
   "source": [
    "default_config = {\n",
    "    \"layer_type\": \"LSTM\",\n",
    "    \"units\": 256,\n",
    "    \"embedding_dim\": 16,\n",
    "    \"optimiser\": \"nadam\",\n",
    "    \"epochs\": 20,\n",
    "    \"dropout\": 0.0,\n",
    "    \"batch_size\": dataset.batch_size,\n",
    "    \"num_layers\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dJYmP35zDhZT",
   "metadata": {
    "id": "dJYmP35zDhZT"
   },
   "outputs": [],
   "source": [
    "def log_wandb(data):\n",
    "    wandb.log(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "attended-paradise",
   "metadata": {
    "id": "attended-paradise"
   },
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"name\": \"Assignment 3 - Without Attention \" + str(datetime.datetime.now().replace(microsecond=0).isoformat()),\n",
    "    \"method\": \"random\",\n",
    "    \"metric\":{\n",
    "        \"name\": \"loss\",\n",
    "        \"goal\": \"minimize\"\n",
    "    },\n",
    "    \"project\": 'Assignment 3',\n",
    "    \"parameters\": {\n",
    "        \"layer_type\": {\n",
    "            \"values\": [\"GRU\", \"LSTM\", \"SimpleRNN\"]\n",
    "        },\n",
    "        \"dropout\": {\n",
    "            \"values\": [0.0, 0.2]\n",
    "        },\n",
    "        \"units\": {\n",
    "            \"values\": [64, 256]\n",
    "        },\n",
    "        \"embedding_dim\": {\n",
    "            \"values\": [16, 64]\n",
    "        },\n",
    "        \"optimiser\": {\n",
    "            \"values\": [\"nadam\"]\n",
    "        },\n",
    "        \"epochs\": {\n",
    "            \"values\": [20]\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"values\": [dataset.batch_size]\n",
    "        },\n",
    "        \"num_layers\": {\n",
    "            \"values\": [1, 2]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "TbYNYFiki7Oo",
   "metadata": {
    "id": "TbYNYFiki7Oo"
   },
   "outputs": [],
   "source": [
    "def train(dataset, config, callback=None):\n",
    "\n",
    "    run_name = \"\".join(f\"{a}:{b} \" for (a, b) in config.items())\n",
    "    print(run_name)\n",
    "    # wandb.run.name = run_name\n",
    "\n",
    "    train_dataset = dataset.train\n",
    "    val_dataset = dataset.val\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Nadam()\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "    EPOCHS = config[\"epochs\"]\n",
    "    BATCH_SIZE = config[\"batch_size\"]\n",
    "    steps_per_epoch = len(train_dataset.input_tensor)//BATCH_SIZE\n",
    "    val_steps_per_epoch = len(val_dataset.input_tensor)//BATCH_SIZE\n",
    "    embedding_dim = config[\"embedding_dim\"]     \n",
    "    units = config[\"units\"]\n",
    "    layer_type = config[\"layer_type\"]\n",
    "    num_layers = config[\"num_layers\"]\n",
    "    dropout = config[\"dropout\"]\n",
    "\n",
    "    # Encoder\n",
    "    encoder = Encoder(dataset.num_input_tokens, embedding_dim, units, BATCH_SIZE, dropout, layer_type, num_layers)\n",
    "    # Decoder\n",
    "    decoder = Decoder(dataset.num_target_tokens, embedding_dim, units, BATCH_SIZE, dropout, layer_type, num_layers)\n",
    "\n",
    "    train_step = train_one_step()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        start = time.time()\n",
    "\n",
    "        enc_hidden = encoder.initialize_hidden_state()\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "        val_total_loss = 0\n",
    "        val_total_accuracy = 0\n",
    "\n",
    "        train_dataset.batch.shuffle(BATCH_SIZE*10)\n",
    "\n",
    "        for (batch, (inp, targ)) in enumerate(train_dataset.batch.take(steps_per_epoch)):\n",
    "            # Step Train\n",
    "            batch_loss, batch_accuracy = train_step(encoder, decoder, inp, targ, enc_hidden, optimizer, loss_object, False)\n",
    "            total_loss += batch_loss\n",
    "            total_accuracy += batch_accuracy\n",
    "            if batch % 100 == 0 or batch == steps_per_epoch-1:\n",
    "                print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f} Accuracy {batch_accuracy:.4f}')\n",
    "            \n",
    "            if callback != None:\n",
    "                callback({\"epoch\":epoch+1, \"loss\": batch_loss.numpy(), \"accuracy\":batch_accuracy})\n",
    "\n",
    "        print(f'Epoch {epoch+1} Loss {total_loss/steps_per_epoch:.4f} Acc {total_accuracy/steps_per_epoch:.4f}')\n",
    "        print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')\n",
    "        if callback != None:\n",
    "            callback({\"ep_training_loss\": total_loss/steps_per_epoch, \"ep_training_accuracy\": total_accuracy/steps_per_epoch})\n",
    "\n",
    "        val_dataset.batch.shuffle(BATCH_SIZE*10)\n",
    "\n",
    "        for (batch, (inp, targ)) in enumerate(val_dataset.batch.take(val_steps_per_epoch)):\n",
    "            val_batch_loss, val_batch_accuracy = train_step(encoder, decoder, inp, targ, enc_hidden, optimizer, loss_object, True)\n",
    "            val_total_loss += val_batch_loss\n",
    "            val_total_accuracy += val_batch_accuracy\n",
    "\n",
    "            if batch % 100 == 0 or batch == val_steps_per_epoch-1:\n",
    "                print(f'Epoch {epoch+1} Batch {batch} Val Loss {val_batch_loss.numpy():.4f} Val Accuracy {val_batch_accuracy:.4f}')\n",
    "\n",
    "            if callback != None:\n",
    "                callback({\"epoch\":epoch+1, \"val loss\": batch_loss.numpy(), \"val accuracy\":batch_accuracy})\n",
    "\n",
    "        print(f'Epoch {epoch+1} Val Loss {val_total_loss/val_steps_per_epoch:.4f} Val Acc {val_total_accuracy/val_steps_per_epoch:.4f}')\n",
    "        print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')\n",
    "\n",
    "        if callback != None:\n",
    "            callback({\"ep_val_loss\": val_total_loss/val_steps_per_epoch, \"ep_val_accuracy\": val_total_accuracy/val_steps_per_epoch})\n",
    "\n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "n-KRNMA8NH9I",
   "metadata": {
    "id": "n-KRNMA8NH9I"
   },
   "outputs": [],
   "source": [
    "# enc, dec = train(dataset, default_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ZL-pZrQgGFPJ",
   "metadata": {
    "id": "ZL-pZrQgGFPJ"
   },
   "outputs": [],
   "source": [
    "def sweep():\n",
    "\n",
    "    wandb.init(config=default_config, magic=True, project='Assignment 3', entity='iitm-cs6910-jan-may-2021-cs20m059-cs20m007')\n",
    "    config = wandb.config\n",
    "    \n",
    "    encoder, decoder = train(dataset, config, log_wandb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3L7FKLR5IUhG",
   "metadata": {
    "id": "3L7FKLR5IUhG"
   },
   "outputs": [],
   "source": [
    "# sweep_id = wandb.sweep(sweep_config, project='Assignment 3', entity='iitm-cs6910-jan-may-2021-cs20m059-cs20m007')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6q-AX2C8IX2T",
   "metadata": {
    "id": "6q-AX2C8IX2T"
   },
   "outputs": [],
   "source": [
    "# wandb.agent(\"ybc5yonl\", function=sweep, project='Assignment 3', entity='iitm-cs6910-jan-may-2021-cs20m059-cs20m007')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "RbkCI5pN0yCf",
   "metadata": {
    "id": "RbkCI5pN0yCf"
   },
   "outputs": [],
   "source": [
    "best_model_config = {\n",
    "    \"layer_type\": \"GRU\",\n",
    "    \"units\": 256,\n",
    "    \"embedding_dim\": 64,\n",
    "    \"optimiser\": \"nadam\",\n",
    "    \"epochs\": 20,\n",
    "    \"dropout\": 0.0,\n",
    "    \"batch_size\": dataset.batch_size,\n",
    "    \"num_layers\": 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "g8mrxShz01uv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g8mrxShz01uv",
    "outputId": "dadc218e-692e-48fe-976f-d4071aa06c11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_type:GRU units:256 embedding_dim:64 optimiser:nadam epochs:20 dropout:0.0 batch_size:128 num_layers:3 \n",
      "Epoch 1 Batch 0 Loss 1.4595 Accuracy 0.0043\n",
      "Epoch 1 Batch 100 Loss 1.0519 Accuracy 0.0785\n",
      "Epoch 1 Batch 200 Loss 0.9617 Accuracy 0.0898\n",
      "Epoch 1 Batch 300 Loss 0.8256 Accuracy 0.1160\n",
      "Epoch 1 Batch 344 Loss 0.8315 Accuracy 0.1273\n",
      "Epoch 1 Loss 0.9744 Acc 0.0843\n",
      "Time taken for 1 epoch 70.18 sec\n",
      "\n",
      "Epoch 1 Batch 0 Val Loss 0.8001 Val Accuracy 0.1207\n",
      "Epoch 1 Batch 33 Val Loss 0.7681 Val Accuracy 0.1168\n",
      "Epoch 1 Val Loss 0.7859 Val Acc 0.1173\n",
      "Time taken for 1 epoch 77.69 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.8048 Accuracy 0.1273\n",
      "Epoch 2 Batch 100 Loss 0.7522 Accuracy 0.1629\n",
      "Epoch 2 Batch 200 Loss 0.6192 Accuracy 0.1738\n",
      "Epoch 2 Batch 300 Loss 0.5532 Accuracy 0.1883\n",
      "Epoch 2 Batch 344 Loss 0.5371 Accuracy 0.1922\n",
      "Epoch 2 Loss 0.6584 Acc 0.1640\n",
      "Time taken for 1 epoch 30.61 sec\n",
      "\n",
      "Epoch 2 Batch 0 Val Loss 0.5478 Val Accuracy 0.1961\n",
      "Epoch 2 Batch 33 Val Loss 0.5144 Val Accuracy 0.1871\n",
      "Epoch 2 Val Loss 0.5062 Val Acc 0.1898\n",
      "Time taken for 1 epoch 31.46 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.5130 Accuracy 0.2121\n",
      "Epoch 3 Batch 100 Loss 0.4467 Accuracy 0.2160\n",
      "Epoch 3 Batch 200 Loss 0.4130 Accuracy 0.2262\n",
      "Epoch 3 Batch 300 Loss 0.3911 Accuracy 0.2395\n",
      "Epoch 3 Batch 344 Loss 0.3558 Accuracy 0.2523\n",
      "Epoch 3 Loss 0.4381 Acc 0.2219\n",
      "Time taken for 1 epoch 30.65 sec\n",
      "\n",
      "Epoch 3 Batch 0 Val Loss 0.3738 Val Accuracy 0.2203\n",
      "Epoch 3 Batch 33 Val Loss 0.3897 Val Accuracy 0.2324\n",
      "Epoch 3 Val Loss 0.3765 Val Acc 0.2241\n",
      "Time taken for 1 epoch 31.50 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.3529 Accuracy 0.2375\n",
      "Epoch 4 Batch 100 Loss 0.3205 Accuracy 0.2520\n",
      "Epoch 4 Batch 200 Loss 0.3315 Accuracy 0.2691\n",
      "Epoch 4 Batch 300 Loss 0.3156 Accuracy 0.2734\n",
      "Epoch 4 Batch 344 Loss 0.2669 Accuracy 0.2574\n",
      "Epoch 4 Loss 0.3241 Acc 0.2556\n",
      "Time taken for 1 epoch 30.88 sec\n",
      "\n",
      "Epoch 4 Batch 0 Val Loss 0.3316 Val Accuracy 0.2379\n",
      "Epoch 4 Batch 33 Val Loss 0.3119 Val Accuracy 0.2434\n",
      "Epoch 4 Val Loss 0.3225 Val Acc 0.2410\n",
      "Time taken for 1 epoch 31.72 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.2653 Accuracy 0.2855\n",
      "Epoch 5 Batch 100 Loss 0.2831 Accuracy 0.2922\n",
      "Epoch 5 Batch 200 Loss 0.2457 Accuracy 0.2773\n",
      "Epoch 5 Batch 300 Loss 0.2692 Accuracy 0.2902\n",
      "Epoch 5 Batch 344 Loss 0.2370 Accuracy 0.2715\n",
      "Epoch 5 Loss 0.2572 Acc 0.2763\n",
      "Time taken for 1 epoch 31.00 sec\n",
      "\n",
      "Epoch 5 Batch 0 Val Loss 0.3167 Val Accuracy 0.2516\n",
      "Epoch 5 Batch 33 Val Loss 0.2958 Val Accuracy 0.2527\n",
      "Epoch 5 Val Loss 0.2926 Val Acc 0.2500\n",
      "Time taken for 1 epoch 31.86 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.2446 Accuracy 0.3008\n",
      "Epoch 6 Batch 100 Loss 0.2455 Accuracy 0.2914\n",
      "Epoch 6 Batch 200 Loss 0.2109 Accuracy 0.2758\n",
      "Epoch 6 Batch 300 Loss 0.2036 Accuracy 0.2945\n",
      "Epoch 6 Batch 344 Loss 0.2013 Accuracy 0.3023\n",
      "Epoch 6 Loss 0.2122 Acc 0.2901\n",
      "Time taken for 1 epoch 30.96 sec\n",
      "\n",
      "Epoch 6 Batch 0 Val Loss 0.2898 Val Accuracy 0.2523\n",
      "Epoch 6 Batch 33 Val Loss 0.2467 Val Accuracy 0.2531\n",
      "Epoch 6 Val Loss 0.2802 Val Acc 0.2540\n",
      "Time taken for 1 epoch 31.83 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.2000 Accuracy 0.3086\n",
      "Epoch 7 Batch 100 Loss 0.1608 Accuracy 0.2906\n",
      "Epoch 7 Batch 200 Loss 0.1988 Accuracy 0.2961\n",
      "Epoch 7 Batch 300 Loss 0.1516 Accuracy 0.3020\n",
      "Epoch 7 Batch 344 Loss 0.1833 Accuracy 0.2918\n",
      "Epoch 7 Loss 0.1797 Acc 0.3000\n",
      "Time taken for 1 epoch 30.80 sec\n",
      "\n",
      "Epoch 7 Batch 0 Val Loss 0.2995 Val Accuracy 0.2445\n",
      "Epoch 7 Batch 33 Val Loss 0.2869 Val Accuracy 0.2641\n",
      "Epoch 7 Val Loss 0.2759 Val Acc 0.2567\n",
      "Time taken for 1 epoch 31.64 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.1521 Accuracy 0.3113\n",
      "Epoch 8 Batch 100 Loss 0.1687 Accuracy 0.3129\n",
      "Epoch 8 Batch 200 Loss 0.1783 Accuracy 0.3172\n",
      "Epoch 8 Batch 300 Loss 0.1838 Accuracy 0.2965\n",
      "Epoch 8 Batch 344 Loss 0.1579 Accuracy 0.3063\n",
      "Epoch 8 Loss 0.1545 Acc 0.3077\n",
      "Time taken for 1 epoch 30.96 sec\n",
      "\n",
      "Epoch 8 Batch 0 Val Loss 0.2520 Val Accuracy 0.2711\n",
      "Epoch 8 Batch 33 Val Loss 0.2746 Val Accuracy 0.2621\n",
      "Epoch 8 Val Loss 0.2756 Val Acc 0.2575\n",
      "Time taken for 1 epoch 31.84 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.1388 Accuracy 0.3074\n",
      "Epoch 9 Batch 100 Loss 0.1275 Accuracy 0.2977\n",
      "Epoch 9 Batch 200 Loss 0.1211 Accuracy 0.3137\n",
      "Epoch 9 Batch 300 Loss 0.1396 Accuracy 0.3160\n",
      "Epoch 9 Batch 344 Loss 0.1221 Accuracy 0.3082\n",
      "Epoch 9 Loss 0.1350 Acc 0.3135\n",
      "Time taken for 1 epoch 31.08 sec\n",
      "\n",
      "Epoch 9 Batch 0 Val Loss 0.3077 Val Accuracy 0.2695\n",
      "Epoch 9 Batch 33 Val Loss 0.2968 Val Accuracy 0.2461\n",
      "Epoch 9 Val Loss 0.2796 Val Acc 0.2589\n",
      "Time taken for 1 epoch 31.95 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.1110 Accuracy 0.3305\n",
      "Epoch 10 Batch 100 Loss 0.1344 Accuracy 0.3227\n",
      "Epoch 10 Batch 200 Loss 0.1507 Accuracy 0.3363\n",
      "Epoch 10 Batch 300 Loss 0.1134 Accuracy 0.3125\n",
      "Epoch 10 Batch 344 Loss 0.1355 Accuracy 0.3109\n",
      "Epoch 10 Loss 0.1200 Acc 0.3182\n",
      "Time taken for 1 epoch 30.95 sec\n",
      "\n",
      "Epoch 10 Batch 0 Val Loss 0.3080 Val Accuracy 0.2648\n",
      "Epoch 10 Batch 33 Val Loss 0.2715 Val Accuracy 0.2629\n",
      "Epoch 10 Val Loss 0.2801 Val Acc 0.2606\n",
      "Time taken for 1 epoch 31.81 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.0929 Accuracy 0.3367\n",
      "Epoch 11 Batch 100 Loss 0.1097 Accuracy 0.3199\n",
      "Epoch 11 Batch 200 Loss 0.0932 Accuracy 0.3234\n",
      "Epoch 11 Batch 300 Loss 0.0992 Accuracy 0.3191\n",
      "Epoch 11 Batch 344 Loss 0.1000 Accuracy 0.3266\n",
      "Epoch 11 Loss 0.1084 Acc 0.3215\n",
      "Time taken for 1 epoch 30.95 sec\n",
      "\n",
      "Epoch 11 Batch 0 Val Loss 0.2718 Val Accuracy 0.2637\n",
      "Epoch 11 Batch 33 Val Loss 0.2680 Val Accuracy 0.2637\n",
      "Epoch 11 Val Loss 0.2880 Val Acc 0.2592\n",
      "Time taken for 1 epoch 31.78 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.0854 Accuracy 0.3289\n",
      "Epoch 12 Batch 100 Loss 0.0861 Accuracy 0.3246\n",
      "Epoch 12 Batch 200 Loss 0.0999 Accuracy 0.3180\n",
      "Epoch 12 Batch 300 Loss 0.1087 Accuracy 0.3305\n",
      "Epoch 12 Batch 344 Loss 0.0930 Accuracy 0.3113\n",
      "Epoch 12 Loss 0.0993 Acc 0.3242\n",
      "Time taken for 1 epoch 30.77 sec\n",
      "\n",
      "Epoch 12 Batch 0 Val Loss 0.3229 Val Accuracy 0.2547\n",
      "Epoch 12 Batch 33 Val Loss 0.2456 Val Accuracy 0.2711\n",
      "Epoch 12 Val Loss 0.2924 Val Acc 0.2603\n",
      "Time taken for 1 epoch 31.64 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.0892 Accuracy 0.3242\n",
      "Epoch 13 Batch 100 Loss 0.0875 Accuracy 0.3352\n",
      "Epoch 13 Batch 200 Loss 0.1072 Accuracy 0.3539\n",
      "Epoch 13 Batch 300 Loss 0.0950 Accuracy 0.3254\n",
      "Epoch 13 Batch 344 Loss 0.0961 Accuracy 0.3273\n",
      "Epoch 13 Loss 0.0927 Acc 0.3262\n",
      "Time taken for 1 epoch 30.72 sec\n",
      "\n",
      "Epoch 13 Batch 0 Val Loss 0.3492 Val Accuracy 0.2516\n",
      "Epoch 13 Batch 33 Val Loss 0.3488 Val Accuracy 0.2566\n",
      "Epoch 13 Val Loss 0.3021 Val Acc 0.2587\n",
      "Time taken for 1 epoch 31.58 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.0879 Accuracy 0.3363\n",
      "Epoch 14 Batch 100 Loss 0.0903 Accuracy 0.3203\n",
      "Epoch 14 Batch 200 Loss 0.1006 Accuracy 0.3336\n",
      "Epoch 14 Batch 300 Loss 0.0977 Accuracy 0.3332\n",
      "Epoch 14 Batch 344 Loss 0.0930 Accuracy 0.3266\n",
      "Epoch 14 Loss 0.0879 Acc 0.3276\n",
      "Time taken for 1 epoch 30.80 sec\n",
      "\n",
      "Epoch 14 Batch 0 Val Loss 0.2564 Val Accuracy 0.2609\n",
      "Epoch 14 Batch 33 Val Loss 0.3805 Val Accuracy 0.2484\n",
      "Epoch 14 Val Loss 0.3076 Val Acc 0.2589\n",
      "Time taken for 1 epoch 31.66 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.0771 Accuracy 0.3367\n",
      "Epoch 15 Batch 100 Loss 0.0920 Accuracy 0.3207\n",
      "Epoch 15 Batch 200 Loss 0.0885 Accuracy 0.3293\n",
      "Epoch 15 Batch 300 Loss 0.0923 Accuracy 0.3328\n",
      "Epoch 15 Batch 344 Loss 0.0963 Accuracy 0.3168\n",
      "Epoch 15 Loss 0.0838 Acc 0.3288\n",
      "Time taken for 1 epoch 30.72 sec\n",
      "\n",
      "Epoch 15 Batch 0 Val Loss 0.3074 Val Accuracy 0.2570\n",
      "Epoch 15 Batch 33 Val Loss 0.2813 Val Accuracy 0.2629\n",
      "Epoch 15 Val Loss 0.3112 Val Acc 0.2598\n",
      "Time taken for 1 epoch 31.58 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.0702 Accuracy 0.3254\n",
      "Epoch 16 Batch 100 Loss 0.0698 Accuracy 0.3340\n",
      "Epoch 16 Batch 200 Loss 0.0805 Accuracy 0.3242\n",
      "Epoch 16 Batch 300 Loss 0.0794 Accuracy 0.3211\n",
      "Epoch 16 Batch 344 Loss 0.0928 Accuracy 0.3289\n",
      "Epoch 16 Loss 0.0811 Acc 0.3296\n",
      "Time taken for 1 epoch 30.74 sec\n",
      "\n",
      "Epoch 16 Batch 0 Val Loss 0.3164 Val Accuracy 0.2602\n",
      "Epoch 16 Batch 33 Val Loss 0.3445 Val Accuracy 0.2465\n",
      "Epoch 16 Val Loss 0.3133 Val Acc 0.2590\n",
      "Time taken for 1 epoch 31.59 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.0803 Accuracy 0.3477\n",
      "Epoch 17 Batch 100 Loss 0.0804 Accuracy 0.3570\n",
      "Epoch 17 Batch 200 Loss 0.0911 Accuracy 0.3266\n",
      "Epoch 17 Batch 300 Loss 0.1008 Accuracy 0.3340\n",
      "Epoch 17 Batch 344 Loss 0.0751 Accuracy 0.3305\n",
      "Epoch 17 Loss 0.0787 Acc 0.3301\n",
      "Time taken for 1 epoch 30.80 sec\n",
      "\n",
      "Epoch 17 Batch 0 Val Loss 0.3036 Val Accuracy 0.2609\n",
      "Epoch 17 Batch 33 Val Loss 0.3565 Val Accuracy 0.2531\n",
      "Epoch 17 Val Loss 0.3216 Val Acc 0.2589\n",
      "Time taken for 1 epoch 31.64 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.0828 Accuracy 0.3371\n",
      "Epoch 18 Batch 100 Loss 0.0908 Accuracy 0.3297\n",
      "Epoch 18 Batch 200 Loss 0.0883 Accuracy 0.3410\n",
      "Epoch 18 Batch 300 Loss 0.0746 Accuracy 0.3219\n",
      "Epoch 18 Batch 344 Loss 0.0879 Accuracy 0.3328\n",
      "Epoch 18 Loss 0.0775 Acc 0.3306\n",
      "Time taken for 1 epoch 30.87 sec\n",
      "\n",
      "Epoch 18 Batch 0 Val Loss 0.3623 Val Accuracy 0.2602\n",
      "Epoch 18 Batch 33 Val Loss 0.3068 Val Accuracy 0.2441\n",
      "Epoch 18 Val Loss 0.3211 Val Acc 0.2599\n",
      "Time taken for 1 epoch 31.75 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.0721 Accuracy 0.3254\n",
      "Epoch 19 Batch 100 Loss 0.0669 Accuracy 0.3234\n",
      "Epoch 19 Batch 200 Loss 0.0826 Accuracy 0.3199\n",
      "Epoch 19 Batch 300 Loss 0.0953 Accuracy 0.3492\n",
      "Epoch 19 Batch 344 Loss 0.0735 Accuracy 0.3164\n",
      "Epoch 19 Loss 0.0753 Acc 0.3311\n",
      "Time taken for 1 epoch 30.75 sec\n",
      "\n",
      "Epoch 19 Batch 0 Val Loss 0.3403 Val Accuracy 0.2582\n",
      "Epoch 19 Batch 33 Val Loss 0.3507 Val Accuracy 0.2648\n",
      "Epoch 19 Val Loss 0.3282 Val Acc 0.2587\n",
      "Time taken for 1 epoch 31.59 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.0673 Accuracy 0.3328\n",
      "Epoch 20 Batch 100 Loss 0.0727 Accuracy 0.3270\n",
      "Epoch 20 Batch 200 Loss 0.0925 Accuracy 0.3477\n",
      "Epoch 20 Batch 300 Loss 0.0732 Accuracy 0.3320\n",
      "Epoch 20 Batch 344 Loss 0.0770 Accuracy 0.3344\n",
      "Epoch 20 Loss 0.0745 Acc 0.3313\n",
      "Time taken for 1 epoch 30.73 sec\n",
      "\n",
      "Epoch 20 Batch 0 Val Loss 0.3169 Val Accuracy 0.2715\n",
      "Epoch 20 Batch 33 Val Loss 0.3352 Val Accuracy 0.2770\n",
      "Epoch 20 Val Loss 0.3301 Val Acc 0.2588\n",
      "Time taken for 1 epoch 31.60 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "enc, dec = train(dataset, best_model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6421ac1f-81fc-4cec-9094-88b21ba143a6",
   "metadata": {
    "id": "6421ac1f-81fc-4cec-9094-88b21ba143a6"
   },
   "source": [
    "## Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5643c560-2f13-43ec-aba5-8fd6eb23762a",
   "metadata": {
    "id": "5643c560-2f13-43ec-aba5-8fd6eb23762a"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder):\n",
    "    attention_plot = np.zeros((dataset.max_target_seq_length, dataset.max_input_seq_length))\n",
    "\n",
    "    sentence = dataset.preprocess_word(sentence)\n",
    "\n",
    "    inputs = [dataset.input_tokenizer.word_index[i] for i in sentence]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=dataset.max_input_seq_length,\n",
    "                                                         padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    if encoder.layer_type != \"LSTM\":\n",
    "        hidden = [tf.zeros((1, encoder.enc_units))]\n",
    "    else:\n",
    "        hidden = [tf.zeros((1, encoder.enc_units)), tf.zeros((1, encoder.enc_units))]\n",
    "    \n",
    "    enc_out, enc_hidden, enc_cell = encoder(inputs, hidden)\n",
    "    \n",
    "    if decoder.layer_type != \"LSTM\":\n",
    "        dec_hidden = enc_hidden\n",
    "    else:\n",
    "        dec_hidden, dec_cell = enc_hidden, enc_cell\n",
    "\n",
    "    dec_input = tf.expand_dims([dataset.target_tokenizer.word_index[sos]], 0)\n",
    "\n",
    "    for t in range(dataset.max_target_seq_length):\n",
    "        # passing enc_output to the decoder\n",
    "        if decoder.layer_type != \"LSTM\":\n",
    "            predictions, _, _ = decoder(dec_input, dec_hidden)\n",
    "        else:\n",
    "            predictions, _, _ = decoder(dec_input, dec_hidden, dec_cell)\n",
    "                \n",
    "        \n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "        result += dataset.target_tokenizer.index_word[predicted_id]\n",
    "\n",
    "        if dataset.target_tokenizer.index_word[predicted_id] == eos:\n",
    "            return result, sentence\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9db2c059-3601-459b-b285-08cb6bda4f84",
   "metadata": {
    "id": "9db2c059-3601-459b-b285-08cb6bda4f84"
   },
   "outputs": [],
   "source": [
    "def translate(sentence, encoder, decoder):\n",
    "    result, sentence = evaluate(sentence, encoder, decoder)\n",
    "\n",
    "    print('Input:', sentence)\n",
    "    print('Predicted translation:', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "00985e4f-7438-416c-93b0-647556ce517b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "00985e4f-7438-416c-93b0-647556ce517b",
    "outputId": "9a9697a2-6363-4388-c6a8-1a6e64dc248b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: @go#\n",
      "Predicted translation: गो#\n"
     ]
    }
   ],
   "source": [
    "translate(\"go\", enc, dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "u_Y_4LsajI-B",
   "metadata": {
    "id": "u_Y_4LsajI-B"
   },
   "outputs": [],
   "source": [
    "# wandb.run.name = \"Image - predictions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "zU-lii9njfj4",
   "metadata": {
    "id": "zU-lii9njfj4"
   },
   "outputs": [],
   "source": [
    "def save_predictions(data_frame, name):\n",
    "    accuracy_count = 0;\n",
    "    with open(name, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"INPUT\", \"PREDICTION\", \"TRUE\"])\n",
    "        for i, (inp, trg) in enumerate(zip(data_frame[1], data_frame[0])): \n",
    "            result, sentence = evaluate(inp, enc, dec)\n",
    "            writer.writerow([inp, result[:-1], trg])\n",
    "            print(inp, result[:-1], trg)\n",
    "            if result[:-1] == trg:\n",
    "                accuracy_count += 1\n",
    "            if (i+1) % 100 == 0 or i+1 == data_frame.size:\n",
    "                print(\"Accuracy\", (accuracy_count / (i+1)))\n",
    "                wandb.log({\"test_accuracy\": (accuracy_count / (i+1))})\n",
    "\n",
    "    return accuracy_count/data_frame.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kkzktH4_qvaB",
   "metadata": {
    "id": "kkzktH4_qvaB"
   },
   "outputs": [],
   "source": [
    "# save_predictions(test_df, str(datetime.datetime.now().strftime(\"%b-%d-%Y-%H-%M-%S\"))+\"without-attn-predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c33a642c-4e31-4386-83b8-c080e1c10d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d0e146-b906-4dee-b898-e429ad90077f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "multi_layered_without_attention_configurable.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
