{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "multi layered without attention - configurable.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2b73578deff345d98e144de7f704ec37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e2c78215a42c458b85c402be4f656bd4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f598b4972d7b4f01bb32b5402cedf33a",
              "IPY_MODEL_46b820e1bef54e84a4576f554f20e1aa"
            ]
          }
        },
        "e2c78215a42c458b85c402be4f656bd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f598b4972d7b4f01bb32b5402cedf33a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_f4a636e2521c4bc1bd6697db10cc5570",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6179e3ffb8a24e14b07d0988d6809ab8"
          }
        },
        "46b820e1bef54e84a4576f554f20e1aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1b798335412649a8b6e2f35f9a1a7cec",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c8bb11175192415e81384715f2df6962"
          }
        },
        "f4a636e2521c4bc1bd6697db10cc5570": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6179e3ffb8a24e14b07d0988d6809ab8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1b798335412649a8b6e2f35f9a1a7cec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c8bb11175192415e81384715f2df6962": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33f85e9d-215f-45c6-95ad-d56e6e3d3759"
      },
      "source": [
        "#### CS20M059 Shibobrota Das | CS20M007 Abhishek Kumar"
      ],
      "id": "33f85e9d-215f-45c6-95ad-d56e6e3d3759"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6700a5ec-7c66-45f5-9539-a593bb36ddc6"
      },
      "source": [
        "## Setup"
      ],
      "id": "6700a5ec-7c66-45f5-9539-a593bb36ddc6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b51cb32-fdbd-4f91-96ca-91ee41122b6b"
      },
      "source": [
        "!pip install tensorflow-addons -qqq"
      ],
      "id": "6b51cb32-fdbd-4f91-96ca-91ee41122b6b",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a08fb4b5-e93b-4c82-bc45-e2a22a7dee95"
      },
      "source": [
        "!pip install wandb -qqq"
      ],
      "id": "a08fb4b5-e93b-4c82-bc45-e2a22a7dee95",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c70406c-fe4d-43f1-93a4-9dc617b301da",
        "outputId": "ef9af1f2-dcdb-481c-f0c4-0c15d6108717"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow import GradientTape\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import datetime\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Embedding, LSTM, GRU, SimpleRNN, SimpleRNNCell, LSTMCell, GRUCell\n",
        "from keras.models import Sequential\n",
        "from keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy\n",
        "import time\n",
        "import sys\n",
        "import datetime\n",
        "from sklearn.utils import shuffle\n",
        "import wandb\n",
        "# import nltk\n",
        "import csv\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from matplotlib.font_manager import FontProperties\n",
        "\n",
        "print(\"Using numpy:\",np.__version__)\n",
        "print(\"Using tensorflow:\",tf.__version__)\n",
        "print(\"Using tensorflow Addons:\",tfa.__version__)\n",
        "print(\"Using keras:\",keras.__version__)\n",
        "print(\"Using pandas:\",pd.__version__)"
      ],
      "id": "5c70406c-fe4d-43f1-93a4-9dc617b301da",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using numpy: 1.19.5\n",
            "Using tensorflow: 2.4.1\n",
            "Using tensorflow Addons: 0.13.0\n",
            "Using keras: 2.4.0\n",
            "Using pandas: 1.1.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddf43393-e89f-46f4-a711-f2e7078e9e85"
      },
      "source": [
        "# wandb.init(project='Assignment 3', entity='iitm-cs6910-jan-may-2021-cs20m059-cs20m007')"
      ],
      "id": "ddf43393-e89f-46f4-a711-f2e7078e9e85",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cd226e1-96b2-4440-a310-15a8cf2f0035",
        "outputId": "7840882b-32fd-4a9a-87da-c75603feecc9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/My Drive/DL-A3 Dataset/dakshina_dataset_v1.0/hi/'"
      ],
      "id": "4cd226e1-96b2-4440-a310-15a8cf2f0035",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/DL-A3 Dataset/dakshina_dataset_v1.0/hi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "559c92d9-374e-4e0f-b2d1-7f17d895cc04"
      },
      "source": [
        "#### Load Data"
      ],
      "id": "559c92d9-374e-4e0f-b2d1-7f17d895cc04"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee85d467-1fac-40d1-a7ca-b00faf2d3d4b",
        "outputId": "d9bad088-da1e-4b71-e726-c6dc8e17dc14"
      },
      "source": [
        "val_df = pd.read_csv(\"./lexicons/hi.translit.sampled.dev.tsv\", sep='\\t', header=None)\n",
        "train_df = pd.read_csv(\"./lexicons/hi.translit.sampled.train.tsv\", sep='\\t', header=None)\n",
        "test_df = pd.read_csv(\"./lexicons/hi.translit.sampled.test.tsv\", sep='\\t', header=None)\n",
        "print(\"Data Loaded to Dataframes!\")"
      ],
      "id": "ee85d467-1fac-40d1-a7ca-b00faf2d3d4b",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Loaded to Dataframes!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13ae78d1-de5a-435a-8b39-088bd63f8e2c",
        "outputId": "de3a2f07-bdc6-4809-8622-b0199f7671cf"
      },
      "source": [
        "%cd '/content/drive/My Drive/A3-checkpoints/'"
      ],
      "id": "13ae78d1-de5a-435a-8b39-088bd63f8e2c",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/A3-checkpoints\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caa6b5e6-844e-4fb9-9eeb-bcedd0cf6fd1"
      },
      "source": [
        "#### Dataset Samples"
      ],
      "id": "caa6b5e6-844e-4fb9-9eeb-bcedd0cf6fd1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "fefa85bd-2299-4503-8d29-458451185e8f",
        "outputId": "ecbb777a-b314-4547-cb3f-ad941262a86c"
      },
      "source": [
        "train_df.sample(n=3)"
      ],
      "id": "fefa85bd-2299-4503-8d29-458451185e8f",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11951</th>\n",
              "      <td>चिटफंड</td>\n",
              "      <td>chitfund</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21380</th>\n",
              "      <td>पंखा</td>\n",
              "      <td>pankhaa</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43110</th>\n",
              "      <td>स्वीकृति</td>\n",
              "      <td>swikriti</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              0         1  2\n",
              "11951    चिटफंड  chitfund  3\n",
              "21380      पंखा   pankhaa  1\n",
              "43110  स्वीकृति  swikriti  1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "627e8bdb-9997-43b6-ba7e-e05879c1c4c0"
      },
      "source": [
        "## Preparing Dataset"
      ],
      "id": "627e8bdb-9997-43b6-ba7e-e05879c1c4c0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53abf538-55d3-46e0-9532-f0aded169217"
      },
      "source": [
        "sos = \"@\"\n",
        "eos = \"#\""
      ],
      "id": "53abf538-55d3-46e0-9532-f0aded169217",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7035ff0-c322-4ef6-98ac-eae0416ad3a7"
      },
      "source": [
        "class LexDataset:\n",
        "    def __init__(self, input_tensor, target_tensor, batch_size):\n",
        "        self.input_tensor = input_tensor\n",
        "        self.target_tensor = target_tensor\n",
        "        self.batch = tf.data.Dataset.from_tensor_slices((self.input_tensor, self.target_tensor)).shuffle(len(self.input_tensor)).batch(batch_size, drop_remainder=True)"
      ],
      "id": "e7035ff0-c322-4ef6-98ac-eae0416ad3a7",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce3cd7e2-7e30-44bb-8a27-77dbf944bea0"
      },
      "source": [
        "class TransliterationDatatset:\n",
        "    def __init__(self, df_list, batch_size = 64):\n",
        "        \n",
        "        self.input_tokenizer = None\n",
        "        self.target_tokenizer = None\n",
        "        self.train = None\n",
        "        self.val = None\n",
        "        self.test = None\n",
        "        self.batch_size = batch_size\n",
        "        # Load Data\n",
        "        self.load_dataset(df_list)\n",
        "        # Other parameters\n",
        "        self.num_input_tokens = len(self.input_tokenizer.index_word)+1\n",
        "        self.num_target_tokens = len(self.target_tokenizer.index_word)+1\n",
        "        self.max_input_seq_length = np.max([self.train.input_tensor.shape[1], self.val.input_tensor.shape[1], self.test.input_tensor.shape[1]])\n",
        "        self.max_target_seq_length = np.max([self.train.target_tensor.shape[1], self.val.target_tensor.shape[1], self.test.target_tensor.shape[1]])\n",
        "        \n",
        "    def preprocess_word(self, w):\n",
        "        return sos + str(w) + eos\n",
        "    \n",
        "    def print_input(self, tensor):\n",
        "        for t in tensor:\n",
        "            if t != 0:\n",
        "                print(f'{t} ----> {self.input_tokenizer.index_word[t]}')\n",
        "\n",
        "    def get_target_word(self, tensor):\n",
        "        word = []\n",
        "        for t in tensor:\n",
        "            if t != 0:\n",
        "                word.append(self.input_tokenizer.index_word[t])\n",
        "        return \"\".join([ch for ch in word])\n",
        "                \n",
        "    def print_target(self, tensor):\n",
        "        for t in tensor:\n",
        "            if t != 0:\n",
        "                print(f'{t} ----> {self.target_tokenizer.index_word[t]}')\n",
        "    \n",
        "    def create_dataset(self, data_frame):\n",
        "        input_words = []\n",
        "        target_words = []\n",
        "        # Shuffle the data_frame before creating dataset\n",
        "        df = data_frame\n",
        "        for i in range(5):\n",
        "            df = shuffle(df)\n",
        "        for x, y in zip(df[1], df[0]):\n",
        "            input_words.append(self.preprocess_word(x))\n",
        "            target_words.append(self.preprocess_word(y))\n",
        "        return (input_words, target_words)\n",
        "    \n",
        "    def load_dataset(self, df_list):\n",
        "        # df_list should have train -> val -> test in sequence\n",
        "        \n",
        "        self.input_tokenizer = Tokenizer(num_words = None, char_level = True)\n",
        "        self.target_tokenizer = Tokenizer(num_words = None, char_level = True)\n",
        "        \n",
        "        ds_list = []\n",
        "        \n",
        "        for df in df_list:\n",
        "            # Get the words list\n",
        "            (input_words, target_words) = self.create_dataset(df)\n",
        "            # Fit on the set of words\n",
        "            self.input_tokenizer.fit_on_texts(input_words)\n",
        "            self.target_tokenizer.fit_on_texts(target_words)\n",
        "            ds_list.append((input_words, target_words))\n",
        "                    \n",
        "        self.target_tokenizer.index_word.update({0:\" \"})\n",
        "        self.input_tokenizer.index_word.update({0:\" \"})\n",
        "        \n",
        "        input_word_len = []\n",
        "        target_word_len = []\n",
        "        \n",
        "        tensor_list = []\n",
        "        \n",
        "        for i, (input_words, target_words) in enumerate(ds_list):\n",
        "            input_tensor = self.input_tokenizer.texts_to_sequences(input_words)\n",
        "            target_tensor = self.target_tokenizer.texts_to_sequences(target_words)\n",
        "            tensor_list.append((input_tensor, target_tensor))\n",
        "            input_word_len.append(np.max([len(x) for x in input_tensor]))\n",
        "            target_word_len.append(np.max([len(x) for x in target_tensor]))\n",
        "        \n",
        "        for i, (input_tensor, target_tensor) in enumerate(tensor_list):\n",
        "            \n",
        "            input_tensor = pad_sequences(input_tensor, padding='post', maxlen = np.max(input_word_len))\n",
        "            target_tensor = pad_sequences(target_tensor, padding='post', maxlen = np.max(target_word_len))\n",
        "            \n",
        "            if i == 0:\n",
        "                self.train = LexDataset(input_tensor, target_tensor, self.batch_size)\n",
        "            elif i == 1:\n",
        "                self.val = LexDataset(input_tensor, target_tensor, self.batch_size)\n",
        "            else:\n",
        "                self.test = LexDataset(input_tensor, target_tensor, self.batch_size)"
      ],
      "id": "ce3cd7e2-7e30-44bb-8a27-77dbf944bea0",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63b06317-dab4-4455-812e-1b34679e27b1",
        "scrolled": true,
        "tags": []
      },
      "source": [
        "dataset = TransliterationDatatset([train_df, val_df, test_df], 128)"
      ],
      "id": "63b06317-dab4-4455-812e-1b34679e27b1",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fb817ef-9206-4341-807a-4b4db2113340"
      },
      "source": [
        "#### Training Data"
      ],
      "id": "1fb817ef-9206-4341-807a-4b4db2113340"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1da1956c-d5be-4d10-a263-f565341e5d6a",
        "tags": [],
        "outputId": "98b1562d-084c-4b8e-f96e-54d294c32a41"
      },
      "source": [
        "# Training data\n",
        "dataset.train.input_tensor.shape, dataset.train.target_tensor.shape"
      ],
      "id": "1da1956c-d5be-4d10-a263-f565341e5d6a",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((44204, 22), (44204, 21))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a008930-d6da-4be0-898c-153f49b8845a"
      },
      "source": [
        "#### Validation Data"
      ],
      "id": "3a008930-d6da-4be0-898c-153f49b8845a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3c2a7df-ec58-40b6-9ba1-6b7782fdfa0c",
        "outputId": "014bc5e2-0c8e-499e-e184-8415a73450f5"
      },
      "source": [
        "# Validation data\n",
        "dataset.val.input_tensor.shape, dataset.val.target_tensor.shape"
      ],
      "id": "a3c2a7df-ec58-40b6-9ba1-6b7782fdfa0c",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4358, 22), (4358, 21))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da416c97-6ef6-4855-8968-dfcd1fe8ff5a"
      },
      "source": [
        "#### Test Data"
      ],
      "id": "da416c97-6ef6-4855-8968-dfcd1fe8ff5a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa011970-5547-4783-a89d-8ab8c3438fa6",
        "outputId": "45ef0ad3-65fe-4b4a-ac34-f10f9bde3554"
      },
      "source": [
        "# Test data\n",
        "dataset.test.input_tensor.shape, dataset.test.target_tensor.shape"
      ],
      "id": "fa011970-5547-4783-a89d-8ab8c3438fa6",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4502, 22), (4502, 21))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a7ada4f-733d-4301-93b2-1ade66cde9a8"
      },
      "source": [
        "#### Number of Tokens"
      ],
      "id": "8a7ada4f-733d-4301-93b2-1ade66cde9a8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bba2c1c4-7fb6-454b-8210-13398ed7406b",
        "outputId": "fe4492f3-119c-4966-8110-8dbe73ed518c"
      },
      "source": [
        "# Number of tokens\n",
        "dataset.num_input_tokens, dataset.num_target_tokens"
      ],
      "id": "bba2c1c4-7fb6-454b-8210-13398ed7406b",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30, 67)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01a249a5-d5ea-46c6-ab4f-a7d0e08b05f3"
      },
      "source": [
        "#### Maximum Sequence Lengths"
      ],
      "id": "01a249a5-d5ea-46c6-ab4f-a7d0e08b05f3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c96afcb-d6ff-4b44-89f2-5d0797709ce4",
        "outputId": "42f3f20b-b51c-46a1-ba6e-c7a049f40289"
      },
      "source": [
        "# max seq length\n",
        "dataset.max_input_seq_length, dataset.max_target_seq_length"
      ],
      "id": "7c96afcb-d6ff-4b44-89f2-5d0797709ce4",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22, 21)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef24fd33-e9fd-47c7-be52-62705caf2ebc"
      },
      "source": [
        "#### Example batch - dataset"
      ],
      "id": "ef24fd33-e9fd-47c7-be52-62705caf2ebc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3c95122-8a50-482b-ae5d-994a74f31765"
      },
      "source": [
        "# example_input_batch, example_target_batch = next(iter(dataset.train.batch))\n",
        "# example_input_batch.shape, example_target_batch.shape"
      ],
      "id": "a3c95122-8a50-482b-ae5d-994a74f31765",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc25ddd3-8cd9-4f65-98de-b4f5bddf8c13"
      },
      "source": [
        "# dataset.print_input(example_input_batch[2].numpy())"
      ],
      "id": "cc25ddd3-8cd9-4f65-98de-b4f5bddf8c13",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc385626-2c47-48cf-88c6-a8bce1ab1cdb"
      },
      "source": [
        "# dataset.print_target(example_target_batch[2].numpy())"
      ],
      "id": "cc385626-2c47-48cf-88c6-a8bce1ab1cdb",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81646f07-05bf-4c75-aaf4-ce964bf9e458"
      },
      "source": [
        "## Encoder Model"
      ],
      "id": "81646f07-05bf-4c75-aaf4-ce964bf9e458"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29f2c233-4f9a-471a-8ff9-dfe5950a4963"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz, dropout=0.2, layer_type=\"GRU\", num_layers=1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.layer_type = layer_type\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.rnn_layers = []\n",
        "\n",
        "        if self.layer_type == \"LSTM\":\n",
        "            for i in range(self.num_layers):\n",
        "                self.rnn_layers.append(LSTM(self.enc_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       dropout = self.dropout,\n",
        "                                       recurrent_initializer='glorot_uniform'))\n",
        "        \n",
        "        elif self.layer_type == \"GRU\":\n",
        "            for i in range(self.num_layers):\n",
        "                self.rnn_layers.append(GRU(self.enc_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       dropout = self.dropout,\n",
        "                                       recurrent_initializer='glorot_uniform'))\n",
        "        \n",
        "        else:\n",
        "            for i in range(self.num_layers):\n",
        "                self.rnn_layers.append(SimpleRNN(self.enc_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       dropout = self.dropout,\n",
        "                                       recurrent_initializer='glorot_uniform'))        \n",
        "\n",
        "\n",
        "    def call(self, inputs, hidden):\n",
        "        inputs = self.embedding(inputs)\n",
        "        state_h, state_c = [], []\n",
        "\n",
        "        if self.layer_type == \"LSTM\":\n",
        "            output, h, c = self.rnn_layers[0](inputs, initial_state = hidden)\n",
        "            state_h.append(h)\n",
        "            state_c.append(c)\n",
        "            for i in range(1, self.num_layers):\n",
        "                output, h, c = self.rnn_layers[i](output, initial_state = hidden)\n",
        "                state_h.append(h)\n",
        "                state_c.append(c)\n",
        "            return output, state_h, state_c\n",
        "        \n",
        "        elif self.layer_type == \"GRU\":\n",
        "            output, h = self.rnn_layers[0](inputs, initial_state = hidden)\n",
        "            state_h.append(h)\n",
        "            for i in range(1, self.num_layers):\n",
        "                output, h = self.rnn_layers[i](output, initial_state = hidden)\n",
        "                state_h.append(h)\n",
        "            return output, state_h, None\n",
        "           \n",
        "        else:\n",
        "            output, h = self.rnn_layers[0](inputs, initial_state = hidden)\n",
        "            state_h.append(h)\n",
        "            for i in range(1, self.num_layers):\n",
        "                output, h = self.rnn_layers[i](output, initial_state = hidden)\n",
        "                state_h.append(h)\n",
        "            return output, state_h, None\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        if self.layer_type == \"LSTM\":\n",
        "            return [tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))]\n",
        "        else:\n",
        "            return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "id": "29f2c233-4f9a-471a-8ff9-dfe5950a4963",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei4-t_PQdEBj"
      },
      "source": [
        "### Test Encoder"
      ],
      "id": "Ei4-t_PQdEBj"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59648301-2c83-4d6e-b066-3bb2ff53650c"
      },
      "source": [
        "# vocab_inp_size = dataset.num_input_tokens\n",
        "# embedding_dim = 64\n",
        "# units = 256\n",
        "# BATCH_SIZE = dataset.batch_size"
      ],
      "id": "59648301-2c83-4d6e-b066-3bb2ff53650c",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8911625-ecf9-41b6-9234-7f241a71dd4a"
      },
      "source": [
        "# encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE, 0.2, \"GRU\", 3)\n",
        "\n",
        "# # sample input\n",
        "# sample_hidden = encoder.initialize_hidden_state()\n",
        "# sample_output, sample_hidden, sample_cell = encoder(example_input_batch, sample_hidden)\n",
        "# print('Encoder output shape: (batch size, sequence length, units)', np.shape(sample_output))\n",
        "# print('Encoder Hidden state shape: (batch size, units)', np.shape(sample_hidden))\n",
        "# if encoder.layer_type == \"LSTM\":\n",
        "#     print ('Encoder c vector shape: (batch size, units) {}'.format(np.shape(sample_cell)))"
      ],
      "id": "a8911625-ecf9-41b6-9234-7f241a71dd4a",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAiyXfzkd43y"
      },
      "source": [
        "## Decoder Model "
      ],
      "id": "pAiyXfzkd43y"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adbd8f1a-53b4-4d8d-bff6-478f2718bcd4"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, dropout=0.2, layer_type=\"GRU\", num_layers=1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.layer_type = layer_type\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.rnn_layers = []\n",
        "        \n",
        "        if self.layer_type == \"LSTM\":\n",
        "            for i in range(self.num_layers):\n",
        "                self.rnn_layers.append(LSTM(self.dec_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       dropout = self.dropout,\n",
        "                                       recurrent_initializer='glorot_uniform'))\n",
        "        \n",
        "        elif self.layer_type == \"GRU\":\n",
        "            for i in range(self.num_layers):\n",
        "                self.rnn_layers.append(GRU(self.dec_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       dropout = self.dropout,\n",
        "                                       recurrent_initializer='glorot_uniform'))\n",
        "           \n",
        "        else:\n",
        "            for i in range(self.num_layers):\n",
        "                self.rnn_layers.append(SimpleRNN(self.dec_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       dropout = self.dropout,\n",
        "                                       recurrent_initializer='glorot_uniform'))\n",
        "        \n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, state_h, state_c=None):\n",
        "        inputs = self.embedding(inputs)\n",
        "\n",
        "        if self.layer_type == \"LSTM\":\n",
        "            output, h, c = self.rnn_layers[0](inputs, initial_state = [state_h[0], state_c[0]])\n",
        "            for i in range(1, self.num_layers):\n",
        "                output, h, c = self.rnn_layers[i](output, initial_state = [state_h[i], state_c[i]])\n",
        "        \n",
        "        elif self.layer_type == \"GRU\":\n",
        "            output, h = self.rnn_layers[0](inputs, initial_state = state_h[0])\n",
        "            for i in range(1, self.num_layers):\n",
        "                output, h = self.rnn_layers[i](output, initial_state = state_h[i])\n",
        "           \n",
        "        else:\n",
        "            output, h = self.rnn_layers[0](inputs, initial_state = state_h[0])\n",
        "            for i in range(1, self.num_layers):\n",
        "                output, h = self.rnn_layers[i](output, initial_state = state_h[i])\n",
        "\n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "        # output shape == (batch_size, vocab)\n",
        "        x = self.fc(output)\n",
        "\n",
        "        # return x, state\n",
        "        if self.layer_type != \"LSTM\":\n",
        "            return x, h, None\n",
        "        else:\n",
        "            return x, h, c"
      ],
      "id": "adbd8f1a-53b4-4d8d-bff6-478f2718bcd4",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6Wr2JTSdI1L"
      },
      "source": [
        "### Test Decoder"
      ],
      "id": "N6Wr2JTSdI1L"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7fb80f6-6d51-4b88-89aa-b10d4cbcb950"
      },
      "source": [
        "# vocab_tar_size = dataset.num_target_tokens"
      ],
      "id": "f7fb80f6-6d51-4b88-89aa-b10d4cbcb950",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4dd296f-1d52-4b1a-9caa-e56ae23af6ea"
      },
      "source": [
        "# decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, 0.2, \"GRU\", 3)\n",
        "\n",
        "# if decoder.layer_type != \"LSTM\":\n",
        "#     sample_decoder_output, sample_decoder_hidden, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_hidden, sample_cell)\n",
        "# else:\n",
        "#     sample_decoder_output, sample_decoder_hidden, sample_decoder_cell = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_hidden, sample_cell)\n",
        "\n",
        "# print('Decoder output shape: (batch_size, vocab size)', sample_decoder_output.shape)\n",
        "# print('Decoder Hidden state shape: (batch size, units)', sample_decoder_hidden.shape)\n",
        "# if encoder.layer_type == \"LSTM\":\n",
        "#     print ('Encoder c vector shape: (batch size, units) {}'.format(sample_decoder_cell.shape))"
      ],
      "id": "c4dd296f-1d52-4b1a-9caa-e56ae23af6ea",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51d718ae-5b4a-472f-9d62-e3b72a468217"
      },
      "source": [
        "## loss function"
      ],
      "id": "51d718ae-5b4a-472f-9d62-e3b72a468217"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opNBWllg215N"
      },
      "source": [
        "def loss_function(real, pred, loss_object):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "id": "opNBWllg215N",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw8k1SYOYdiV"
      },
      "source": [
        "## Accuracy (Word level)"
      ],
      "id": "yw8k1SYOYdiV"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uppr-mNwYkdK"
      },
      "source": [
        "def accuracy(real, pred):\n",
        "    real = tf.cast(real, tf.int32)\n",
        "    pred = tf.cast(pred, tf.int32)\n",
        "    return tf.reduce_mean(tf.cast(tf.equal(real, pred), tf.float32))"
      ],
      "id": "uppr-mNwYkdK",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e633463-5d0c-4080-9e0d-5cc8cb88a9b9"
      },
      "source": [
        "## Training"
      ],
      "id": "7e633463-5d0c-4080-9e0d-5cc8cb88a9b9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90973b6a-e060-4e22-94f6-2400e7d700f2"
      },
      "source": [
        "def train_one_step():\n",
        "    @tf.function\n",
        "    def train_step(encoder, decoder, inp, targ, enc_hidden, optimizer, loss_object, is_val = False):\n",
        "        loss = 0\n",
        "        spc_loss = 0\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            enc_output, enc_hidden, enc_cell = encoder(inp, enc_hidden)\n",
        "            if decoder.layer_type != \"LSTM\":\n",
        "                dec_hidden = enc_hidden\n",
        "            else:\n",
        "                dec_hidden, dec_cell = enc_hidden, enc_cell\n",
        "\n",
        "            dec_input = tf.expand_dims([dataset.target_tokenizer.word_index[sos]] * dataset.batch_size, 1)\n",
        "\n",
        "            pred = None\n",
        "\n",
        "            # Teacher forcing - feeding the target as the next input\n",
        "            for t in range(1, targ.shape[1]):\n",
        "                \n",
        "                # passing enc_output to the decoder\n",
        "                if decoder.layer_type != \"LSTM\":\n",
        "                    predictions, _, _ = decoder(dec_input, dec_hidden)\n",
        "                else:\n",
        "                    predictions, _, _ = decoder(dec_input, dec_hidden, dec_cell)\n",
        "                            \n",
        "                loss += loss_function(targ[:, t], predictions, loss_object)\n",
        "\n",
        "                # using teacher forcing\n",
        "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "                if t == 1:\n",
        "                    pred = tf.expand_dims(tf.argmax(predictions, axis=-1), 1)\n",
        "                else:\n",
        "                    pred = tf.concat([pred, tf.expand_dims(tf.argmax(predictions, axis=-1), 1)], 1)\n",
        "        \n",
        "        batch_accuracy = accuracy(targ[:, 1:], pred)\n",
        "\n",
        "        batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "        if not is_val:\n",
        "            variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "            gradients = tape.gradient(loss, variables)\n",
        "\n",
        "            optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "        return batch_loss, batch_accuracy\n",
        "    return train_step"
      ],
      "id": "90973b6a-e060-4e22-94f6-2400e7d700f2",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTdsr-zAzVrI"
      },
      "source": [
        "default_config = {\n",
        "    \"layer_type\": \"LSTM\",\n",
        "    \"units\": 256,\n",
        "    \"embedding_dim\": 16,\n",
        "    \"optimiser\": \"nadam\",\n",
        "    \"epochs\": 20,\n",
        "    \"dropout\": 0.0,\n",
        "    \"batch_size\": dataset.batch_size,\n",
        "    \"num_layers\": 1\n",
        "}"
      ],
      "id": "BTdsr-zAzVrI",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJYmP35zDhZT"
      },
      "source": [
        "def log_wandb(data):\n",
        "    wandb.log(data)"
      ],
      "id": "dJYmP35zDhZT",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "attended-paradise"
      },
      "source": [
        "sweep_config = {\n",
        "    \"name\": \"Assignment 3 - Without Attention \" + str(datetime.datetime.now().replace(microsecond=0).isoformat()),\n",
        "    \"method\": \"random\",\n",
        "    \"metric\":{\n",
        "        \"name\": \"loss\",\n",
        "        \"goal\": \"minimize\"\n",
        "    },\n",
        "    \"project\": 'Assignment 3',\n",
        "    \"parameters\": {\n",
        "        \"layer_type\": {\n",
        "            \"values\": [\"GRU\", \"LSTM\", \"SimpleRNN\"]\n",
        "        },\n",
        "        \"dropout\": {\n",
        "            \"values\": [0.0, 0.2]\n",
        "        },\n",
        "        \"units\": {\n",
        "            \"values\": [64, 256]\n",
        "        },\n",
        "        \"embedding_dim\": {\n",
        "            \"values\": [16, 64]\n",
        "        },\n",
        "        \"optimiser\": {\n",
        "            \"values\": [\"nadam\"]\n",
        "        },\n",
        "        \"epochs\": {\n",
        "            \"values\": [20]\n",
        "        },\n",
        "        \"batch_size\": {\n",
        "            \"values\": [dataset.batch_size]\n",
        "        },\n",
        "        \"num_layers\": {\n",
        "            \"values\": [1, 2]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "id": "attended-paradise",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbYNYFiki7Oo"
      },
      "source": [
        "def train(dataset, config, callback=None):\n",
        "\n",
        "    run_name = \"\".join(f\"{a}:{b} \" for (a, b) in config.items())\n",
        "    print(run_name)\n",
        "    wandb.run.name = run_name\n",
        "\n",
        "    train_dataset = dataset.train\n",
        "    val_dataset = dataset.val\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Nadam()\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "    EPOCHS = config[\"epochs\"]\n",
        "    BATCH_SIZE = config[\"batch_size\"]\n",
        "    steps_per_epoch = len(train_dataset.input_tensor)//BATCH_SIZE\n",
        "    val_steps_per_epoch = len(val_dataset.input_tensor)//BATCH_SIZE\n",
        "    embedding_dim = config[\"embedding_dim\"]     \n",
        "    units = config[\"units\"]\n",
        "    layer_type = config[\"layer_type\"]\n",
        "    num_layers = config[\"num_layers\"]\n",
        "    dropout = config[\"dropout\"]\n",
        "\n",
        "    # Encoder\n",
        "    encoder = Encoder(dataset.num_input_tokens, embedding_dim, units, BATCH_SIZE, dropout, layer_type, num_layers)\n",
        "    # Decoder\n",
        "    decoder = Decoder(dataset.num_target_tokens, embedding_dim, units, BATCH_SIZE, dropout, layer_type, num_layers)\n",
        "\n",
        "    train_step = train_one_step()\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        start = time.time()\n",
        "\n",
        "        enc_hidden = encoder.initialize_hidden_state()\n",
        "        total_loss = 0\n",
        "        total_accuracy = 0\n",
        "        val_total_loss = 0\n",
        "        val_total_accuracy = 0\n",
        "\n",
        "        train_dataset.batch.shuffle(BATCH_SIZE*10)\n",
        "\n",
        "        for (batch, (inp, targ)) in enumerate(train_dataset.batch.take(steps_per_epoch)):\n",
        "            # Step Train\n",
        "            batch_loss, batch_accuracy = train_step(encoder, decoder, inp, targ, enc_hidden, optimizer, loss_object, False)\n",
        "            total_loss += batch_loss\n",
        "            total_accuracy += batch_accuracy\n",
        "            if batch % 100 == 0 or batch == steps_per_epoch-1:\n",
        "                print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f} Accuracy {batch_accuracy:.4f}')\n",
        "            \n",
        "            if callback != None:\n",
        "                callback({\"epoch\":epoch+1, \"loss\": batch_loss.numpy(), \"accuracy\":batch_accuracy})\n",
        "\n",
        "        print(f'Epoch {epoch+1} Loss {total_loss/steps_per_epoch:.4f} Acc {total_accuracy/steps_per_epoch:.4f}')\n",
        "        print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')\n",
        "        if callback != None:\n",
        "            callback({\"ep_training_loss\": total_loss/steps_per_epoch, \"ep_training_accuracy\": total_accuracy/steps_per_epoch})\n",
        "\n",
        "        val_dataset.batch.shuffle(BATCH_SIZE*10)\n",
        "\n",
        "        for (batch, (inp, targ)) in enumerate(val_dataset.batch.take(val_steps_per_epoch)):\n",
        "            val_batch_loss, val_batch_accuracy = train_step(encoder, decoder, inp, targ, enc_hidden, optimizer, loss_object, True)\n",
        "            val_total_loss += val_batch_loss\n",
        "            val_total_accuracy += val_batch_accuracy\n",
        "\n",
        "            if batch % 100 == 0 or batch == val_steps_per_epoch-1:\n",
        "                print(f'Epoch {epoch+1} Batch {batch} Val Loss {val_batch_loss.numpy():.4f} Val Accuracy {val_batch_accuracy:.4f}')\n",
        "\n",
        "            if callback != None:\n",
        "                callback({\"epoch\":epoch+1, \"val loss\": batch_loss.numpy(), \"val accuracy\":batch_accuracy})\n",
        "\n",
        "        print(f'Epoch {epoch+1} Val Loss {val_total_loss/val_steps_per_epoch:.4f} Val Acc {val_total_accuracy/val_steps_per_epoch:.4f}')\n",
        "        print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')\n",
        "\n",
        "        if callback != None:\n",
        "            callback({\"ep_val_loss\": val_total_loss/val_steps_per_epoch, \"ep_val_accuracy\": val_total_accuracy/val_steps_per_epoch})\n",
        "\n",
        "    return encoder, decoder"
      ],
      "id": "TbYNYFiki7Oo",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-KRNMA8NH9I"
      },
      "source": [
        "# enc, dec = train(dataset, default_config)"
      ],
      "id": "n-KRNMA8NH9I",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZL-pZrQgGFPJ"
      },
      "source": [
        "def sweep():\n",
        "\n",
        "    wandb.init(config=default_config, magic=True, project='Assignment 3', entity='iitm-cs6910-jan-may-2021-cs20m059-cs20m007')\n",
        "    config = wandb.config\n",
        "    \n",
        "    encoder, decoder = train(dataset, config, log_wandb)"
      ],
      "id": "ZL-pZrQgGFPJ",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L7FKLR5IUhG",
        "outputId": "28505c0e-2bc7-4fb0-a812-96c5b1552ae2"
      },
      "source": [
        "# sweep_id = wandb.sweep(sweep_config, project='Assignment 3', entity='iitm-cs6910-jan-may-2021-cs20m059-cs20m007')"
      ],
      "id": "3L7FKLR5IUhG",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create sweep with ID: ybc5yonl\n",
            "Sweep URL: https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203/sweeps/ybc5yonl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2b73578deff345d98e144de7f704ec37",
            "e2c78215a42c458b85c402be4f656bd4",
            "f598b4972d7b4f01bb32b5402cedf33a",
            "46b820e1bef54e84a4576f554f20e1aa",
            "f4a636e2521c4bc1bd6697db10cc5570",
            "6179e3ffb8a24e14b07d0988d6809ab8",
            "1b798335412649a8b6e2f35f9a1a7cec",
            "c8bb11175192415e81384715f2df6962"
          ]
        },
        "id": "6q-AX2C8IX2T",
        "outputId": "07080091-3f7c-4821-ed76-7f81025bc2a1"
      },
      "source": [
        "wandb.agent(\"ybc5yonl\", function=sweep, project='Assignment 3', entity='iitm-cs6910-jan-may-2021-cs20m059-cs20m007')"
      ],
      "id": "6q-AX2C8IX2T",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ijq9tksi with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer_type: LSTM\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimiser: nadam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tunits: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.30<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">flowing-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203\" target=\"_blank\">https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203/sweeps/ybc5yonl\" target=\"_blank\">https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203/sweeps/ybc5yonl</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203/runs/ijq9tksi\" target=\"_blank\">https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203/runs/ijq9tksi</a><br/>\n",
              "                Run data is saved locally in <code>/content/drive/My Drive/A3-checkpoints/wandb/run-20210524_131539-ijq9tksi</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "batch_size:128 dropout:0 embedding_dim:16 epochs:20 layer_type:LSTM num_layers:1 optimiser:nadam units:256 \n",
            "Epoch 1 Batch 0 Loss 1.4784 Accuracy 0.0035\n",
            "Epoch 1 Batch 100 Loss 1.0871 Accuracy 0.0594\n",
            "Epoch 1 Batch 200 Loss 0.9768 Accuracy 0.0766\n",
            "Epoch 1 Batch 300 Loss 1.0164 Accuracy 0.0836\n",
            "Epoch 1 Batch 344 Loss 1.0070 Accuracy 0.0855\n",
            "Epoch 1 Loss 1.0583 Acc 0.0700\n",
            "Time taken for 1 epoch 146.26 sec\n",
            "\n",
            "Epoch 1 Batch 0 Val Loss 0.9516 Val Accuracy 0.0750\n",
            "Epoch 1 Batch 33 Val Loss 0.9307 Val Accuracy 0.0781\n",
            "Epoch 1 Val Loss 0.9458 Val Acc 0.0786\n",
            "Time taken for 1 epoch 159.42 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.0499 Accuracy 0.0828\n",
            "Epoch 2 Batch 100 Loss 0.9834 Accuracy 0.0957\n",
            "Epoch 2 Batch 200 Loss 0.9769 Accuracy 0.0910\n",
            "Epoch 2 Batch 300 Loss 0.8613 Accuracy 0.0949\n",
            "Epoch 2 Batch 344 Loss 0.9172 Accuracy 0.0996\n",
            "Epoch 2 Loss 0.9308 Acc 0.0939\n",
            "Time taken for 1 epoch 114.87 sec\n",
            "\n",
            "Epoch 2 Batch 0 Val Loss 0.8646 Val Accuracy 0.0973\n",
            "Epoch 2 Batch 33 Val Loss 0.8663 Val Accuracy 0.0898\n",
            "Epoch 2 Val Loss 0.8634 Val Acc 0.0928\n",
            "Time taken for 1 epoch 118.58 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.8750 Accuracy 0.1039\n",
            "Epoch 3 Batch 100 Loss 0.8858 Accuracy 0.1098\n",
            "Epoch 3 Batch 200 Loss 0.8255 Accuracy 0.1070\n",
            "Epoch 3 Batch 300 Loss 0.8235 Accuracy 0.1242\n",
            "Epoch 3 Batch 344 Loss 0.7589 Accuracy 0.1203\n",
            "Epoch 3 Loss 0.8507 Acc 0.1082\n",
            "Time taken for 1 epoch 114.55 sec\n",
            "\n",
            "Epoch 3 Batch 0 Val Loss 0.7087 Val Accuracy 0.1086\n",
            "Epoch 3 Batch 33 Val Loss 0.7745 Val Accuracy 0.1121\n",
            "Epoch 3 Val Loss 0.7722 Val Acc 0.1122\n",
            "Time taken for 1 epoch 118.49 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.8113 Accuracy 0.1184\n",
            "Epoch 4 Batch 100 Loss 0.8080 Accuracy 0.1238\n",
            "Epoch 4 Batch 200 Loss 0.7717 Accuracy 0.1277\n",
            "Epoch 4 Batch 300 Loss 0.7052 Accuracy 0.1383\n",
            "Epoch 4 Batch 344 Loss 0.6617 Accuracy 0.1395\n",
            "Epoch 4 Loss 0.7553 Acc 0.1273\n",
            "Time taken for 1 epoch 113.89 sec\n",
            "\n",
            "Epoch 4 Batch 0 Val Loss 0.6109 Val Accuracy 0.1316\n",
            "Epoch 4 Batch 33 Val Loss 0.6558 Val Accuracy 0.1324\n",
            "Epoch 4 Val Loss 0.6812 Val Acc 0.1323\n",
            "Time taken for 1 epoch 118.02 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.7100 Accuracy 0.1398\n",
            "Epoch 5 Batch 100 Loss 0.6773 Accuracy 0.1363\n",
            "Epoch 5 Batch 200 Loss 0.6514 Accuracy 0.1480\n",
            "Epoch 5 Batch 300 Loss 0.6092 Accuracy 0.1461\n",
            "Epoch 5 Batch 344 Loss 0.6756 Accuracy 0.1570\n",
            "Epoch 5 Loss 0.6747 Acc 0.1458\n",
            "Time taken for 1 epoch 112.16 sec\n",
            "\n",
            "Epoch 5 Batch 0 Val Loss 0.6568 Val Accuracy 0.1523\n",
            "Epoch 5 Batch 33 Val Loss 0.5973 Val Accuracy 0.1441\n",
            "Epoch 5 Val Loss 0.6265 Val Acc 0.1437\n",
            "Time taken for 1 epoch 116.13 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.6334 Accuracy 0.1488\n",
            "Epoch 6 Batch 100 Loss 0.6186 Accuracy 0.1520\n",
            "Epoch 6 Batch 200 Loss 0.5865 Accuracy 0.1594\n",
            "Epoch 6 Batch 300 Loss 0.5742 Accuracy 0.1723\n",
            "Epoch 6 Batch 344 Loss 0.6021 Accuracy 0.1711\n",
            "Epoch 6 Loss 0.6154 Acc 0.1596\n",
            "Time taken for 1 epoch 110.59 sec\n",
            "\n",
            "Epoch 6 Batch 0 Val Loss 0.5849 Val Accuracy 0.1480\n",
            "Epoch 6 Batch 33 Val Loss 0.5630 Val Accuracy 0.1500\n",
            "Epoch 6 Val Loss 0.5816 Val Acc 0.1537\n",
            "Time taken for 1 epoch 114.41 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.6056 Accuracy 0.1711\n",
            "Epoch 7 Batch 100 Loss 0.5797 Accuracy 0.1652\n",
            "Epoch 7 Batch 200 Loss 0.5291 Accuracy 0.1836\n",
            "Epoch 7 Batch 300 Loss 0.5711 Accuracy 0.1758\n",
            "Epoch 7 Batch 344 Loss 0.5428 Accuracy 0.1867\n",
            "Epoch 7 Loss 0.5627 Acc 0.1723\n",
            "Time taken for 1 epoch 112.84 sec\n",
            "\n",
            "Epoch 7 Batch 0 Val Loss 0.5371 Val Accuracy 0.1512\n",
            "Epoch 7 Batch 33 Val Loss 0.5081 Val Accuracy 0.1637\n",
            "Epoch 7 Val Loss 0.5387 Val Acc 0.1647\n",
            "Time taken for 1 epoch 116.69 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.5694 Accuracy 0.1918\n",
            "Epoch 8 Batch 100 Loss 0.5417 Accuracy 0.1824\n",
            "Epoch 8 Batch 200 Loss 0.4977 Accuracy 0.1887\n",
            "Epoch 8 Batch 300 Loss 0.5265 Accuracy 0.1883\n",
            "Epoch 8 Batch 344 Loss 0.5148 Accuracy 0.1891\n",
            "Epoch 8 Loss 0.5173 Acc 0.1852\n",
            "Time taken for 1 epoch 110.13 sec\n",
            "\n",
            "Epoch 8 Batch 0 Val Loss 0.4984 Val Accuracy 0.1871\n",
            "Epoch 8 Batch 33 Val Loss 0.4958 Val Accuracy 0.1660\n",
            "Epoch 8 Val Loss 0.5073 Val Acc 0.1742\n",
            "Time taken for 1 epoch 114.05 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.5252 Accuracy 0.1949\n",
            "Epoch 9 Batch 100 Loss 0.5355 Accuracy 0.2043\n",
            "Epoch 9 Batch 200 Loss 0.4952 Accuracy 0.2023\n",
            "Epoch 9 Batch 300 Loss 0.5173 Accuracy 0.2102\n",
            "Epoch 9 Batch 344 Loss 0.4796 Accuracy 0.2137\n",
            "Epoch 9 Loss 0.4779 Acc 0.1981\n",
            "Time taken for 1 epoch 116.19 sec\n",
            "\n",
            "Epoch 9 Batch 0 Val Loss 0.5193 Val Accuracy 0.1809\n",
            "Epoch 9 Batch 33 Val Loss 0.4788 Val Accuracy 0.1797\n",
            "Epoch 9 Val Loss 0.4787 Val Acc 0.1841\n",
            "Time taken for 1 epoch 120.18 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.4490 Accuracy 0.2180\n",
            "Epoch 10 Batch 100 Loss 0.4057 Accuracy 0.2090\n",
            "Epoch 10 Batch 200 Loss 0.4595 Accuracy 0.2168\n",
            "Epoch 10 Batch 300 Loss 0.4434 Accuracy 0.2156\n",
            "Epoch 10 Batch 344 Loss 0.4151 Accuracy 0.2098\n",
            "Epoch 10 Loss 0.4413 Acc 0.2105\n",
            "Time taken for 1 epoch 118.31 sec\n",
            "\n",
            "Epoch 10 Batch 0 Val Loss 0.4731 Val Accuracy 0.1941\n",
            "Epoch 10 Batch 33 Val Loss 0.4762 Val Accuracy 0.1957\n",
            "Epoch 10 Val Loss 0.4547 Val Acc 0.1923\n",
            "Time taken for 1 epoch 122.21 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.4184 Accuracy 0.2227\n",
            "Epoch 11 Batch 100 Loss 0.4171 Accuracy 0.2102\n",
            "Epoch 11 Batch 200 Loss 0.4032 Accuracy 0.2188\n",
            "Epoch 11 Batch 300 Loss 0.4313 Accuracy 0.2152\n",
            "Epoch 11 Batch 344 Loss 0.3765 Accuracy 0.2145\n",
            "Epoch 11 Loss 0.4089 Acc 0.2215\n",
            "Time taken for 1 epoch 115.43 sec\n",
            "\n",
            "Epoch 11 Batch 0 Val Loss 0.4727 Val Accuracy 0.2035\n",
            "Epoch 11 Batch 33 Val Loss 0.4674 Val Accuracy 0.2043\n",
            "Epoch 11 Val Loss 0.4399 Val Acc 0.1979\n",
            "Time taken for 1 epoch 119.43 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.4098 Accuracy 0.2414\n",
            "Epoch 12 Batch 100 Loss 0.3761 Accuracy 0.2211\n",
            "Epoch 12 Batch 200 Loss 0.4394 Accuracy 0.2371\n",
            "Epoch 12 Batch 300 Loss 0.4130 Accuracy 0.2289\n",
            "Epoch 12 Batch 344 Loss 0.3866 Accuracy 0.2430\n",
            "Epoch 12 Loss 0.3816 Acc 0.2309\n",
            "Time taken for 1 epoch 115.77 sec\n",
            "\n",
            "Epoch 12 Batch 0 Val Loss 0.4598 Val Accuracy 0.1918\n",
            "Epoch 12 Batch 33 Val Loss 0.4580 Val Accuracy 0.1902\n",
            "Epoch 12 Val Loss 0.4234 Val Acc 0.2030\n",
            "Time taken for 1 epoch 119.63 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.3452 Accuracy 0.2496\n",
            "Epoch 13 Batch 100 Loss 0.3265 Accuracy 0.2430\n",
            "Epoch 13 Batch 200 Loss 0.3674 Accuracy 0.2355\n",
            "Epoch 13 Batch 300 Loss 0.3594 Accuracy 0.2418\n",
            "Epoch 13 Batch 344 Loss 0.3202 Accuracy 0.2449\n",
            "Epoch 13 Loss 0.3571 Acc 0.2393\n",
            "Time taken for 1 epoch 119.71 sec\n",
            "\n",
            "Epoch 13 Batch 0 Val Loss 0.4448 Val Accuracy 0.1961\n",
            "Epoch 13 Batch 33 Val Loss 0.4273 Val Accuracy 0.2141\n",
            "Epoch 13 Val Loss 0.4182 Val Acc 0.2056\n",
            "Time taken for 1 epoch 123.80 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.3452 Accuracy 0.2375\n",
            "Epoch 14 Batch 100 Loss 0.3395 Accuracy 0.2438\n",
            "Epoch 14 Batch 200 Loss 0.3183 Accuracy 0.2395\n",
            "Epoch 14 Batch 300 Loss 0.3165 Accuracy 0.2520\n",
            "Epoch 14 Batch 344 Loss 0.3411 Accuracy 0.2453\n",
            "Epoch 14 Loss 0.3354 Acc 0.2463\n",
            "Time taken for 1 epoch 114.79 sec\n",
            "\n",
            "Epoch 14 Batch 0 Val Loss 0.4586 Val Accuracy 0.2109\n",
            "Epoch 14 Batch 33 Val Loss 0.3739 Val Accuracy 0.2121\n",
            "Epoch 14 Val Loss 0.4119 Val Acc 0.2088\n",
            "Time taken for 1 epoch 118.72 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.2789 Accuracy 0.2520\n",
            "Epoch 15 Batch 100 Loss 0.3166 Accuracy 0.2484\n",
            "Epoch 15 Batch 200 Loss 0.3380 Accuracy 0.2535\n",
            "Epoch 15 Batch 300 Loss 0.3017 Accuracy 0.2523\n",
            "Epoch 15 Batch 344 Loss 0.3205 Accuracy 0.2488\n",
            "Epoch 15 Loss 0.3155 Acc 0.2532\n",
            "Time taken for 1 epoch 112.17 sec\n",
            "\n",
            "Epoch 15 Batch 0 Val Loss 0.3960 Val Accuracy 0.2145\n",
            "Epoch 15 Batch 33 Val Loss 0.3958 Val Accuracy 0.2027\n",
            "Epoch 15 Val Loss 0.4110 Val Acc 0.2104\n",
            "Time taken for 1 epoch 116.08 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.2882 Accuracy 0.2613\n",
            "Epoch 16 Batch 100 Loss 0.3234 Accuracy 0.2648\n",
            "Epoch 16 Batch 200 Loss 0.2808 Accuracy 0.2551\n",
            "Epoch 16 Batch 300 Loss 0.3241 Accuracy 0.2727\n",
            "Epoch 16 Batch 344 Loss 0.2955 Accuracy 0.2602\n",
            "Epoch 16 Loss 0.2974 Acc 0.2593\n",
            "Time taken for 1 epoch 116.64 sec\n",
            "\n",
            "Epoch 16 Batch 0 Val Loss 0.3850 Val Accuracy 0.2148\n",
            "Epoch 16 Batch 33 Val Loss 0.3671 Val Accuracy 0.2176\n",
            "Epoch 16 Val Loss 0.4072 Val Acc 0.2117\n",
            "Time taken for 1 epoch 120.67 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.2502 Accuracy 0.2562\n",
            "Epoch 17 Batch 100 Loss 0.2666 Accuracy 0.2715\n",
            "Epoch 17 Batch 200 Loss 0.2848 Accuracy 0.2617\n",
            "Epoch 17 Batch 300 Loss 0.2942 Accuracy 0.2750\n",
            "Epoch 17 Batch 344 Loss 0.2789 Accuracy 0.2465\n",
            "Epoch 17 Loss 0.2806 Acc 0.2648\n",
            "Time taken for 1 epoch 122.41 sec\n",
            "\n",
            "Epoch 17 Batch 0 Val Loss 0.4844 Val Accuracy 0.2152\n",
            "Epoch 17 Batch 33 Val Loss 0.4106 Val Accuracy 0.2152\n",
            "Epoch 17 Val Loss 0.4068 Val Acc 0.2132\n",
            "Time taken for 1 epoch 126.80 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.2758 Accuracy 0.2641\n",
            "Epoch 18 Batch 100 Loss 0.2527 Accuracy 0.2715\n",
            "Epoch 18 Batch 200 Loss 0.2610 Accuracy 0.2602\n",
            "Epoch 18 Batch 300 Loss 0.2521 Accuracy 0.2809\n",
            "Epoch 18 Batch 344 Loss 0.2416 Accuracy 0.2605\n",
            "Epoch 18 Loss 0.2651 Acc 0.2704\n",
            "Time taken for 1 epoch 123.32 sec\n",
            "\n",
            "Epoch 18 Batch 0 Val Loss 0.3970 Val Accuracy 0.2121\n",
            "Epoch 18 Batch 33 Val Loss 0.4045 Val Accuracy 0.2133\n",
            "Epoch 18 Val Loss 0.4039 Val Acc 0.2164\n",
            "Time taken for 1 epoch 127.37 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.2429 Accuracy 0.2633\n",
            "Epoch 19 Batch 100 Loss 0.2565 Accuracy 0.2777\n",
            "Epoch 19 Batch 200 Loss 0.2590 Accuracy 0.2773\n",
            "Epoch 19 Batch 300 Loss 0.2646 Accuracy 0.2734\n",
            "Epoch 19 Batch 344 Loss 0.2544 Accuracy 0.2641\n",
            "Epoch 19 Loss 0.2506 Acc 0.2749\n",
            "Time taken for 1 epoch 123.08 sec\n",
            "\n",
            "Epoch 19 Batch 0 Val Loss 0.3452 Val Accuracy 0.2332\n",
            "Epoch 19 Batch 33 Val Loss 0.3984 Val Accuracy 0.2160\n",
            "Epoch 19 Val Loss 0.4072 Val Acc 0.2159\n",
            "Time taken for 1 epoch 127.24 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.2348 Accuracy 0.2641\n",
            "Epoch 20 Batch 100 Loss 0.2596 Accuracy 0.2695\n",
            "Epoch 20 Batch 200 Loss 0.2678 Accuracy 0.2699\n",
            "Epoch 20 Batch 300 Loss 0.2374 Accuracy 0.2656\n",
            "Epoch 20 Batch 344 Loss 0.2468 Accuracy 0.2910\n",
            "Epoch 20 Loss 0.2371 Acc 0.2795\n",
            "Time taken for 1 epoch 125.40 sec\n",
            "\n",
            "Epoch 20 Batch 0 Val Loss 0.3405 Val Accuracy 0.2281\n",
            "Epoch 20 Batch 33 Val Loss 0.4158 Val Accuracy 0.2129\n",
            "Epoch 20 Val Loss 0.4085 Val Acc 0.2168\n",
            "Time taken for 1 epoch 129.99 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 3750<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b73578deff345d98e144de7f704ec37",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/drive/My Drive/A3-checkpoints/wandb/run-20210524_131539-ijq9tksi/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/drive/My Drive/A3-checkpoints/wandb/run-20210524_131539-ijq9tksi/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>epoch</td><td>20</td></tr><tr><td>loss</td><td>0.24678</td></tr><tr><td>accuracy</td><td>0.29102</td></tr><tr><td>_runtime</td><td>2451</td></tr><tr><td>_timestamp</td><td>1621864590</td></tr><tr><td>_step</td><td>7619</td></tr><tr><td>ep_training_loss</td><td>0.23711</td></tr><tr><td>ep_training_accuracy</td><td>0.27949</td></tr><tr><td>val loss</td><td>0.24678</td></tr><tr><td>val accuracy</td><td>0.29102</td></tr><tr><td>ep_val_loss</td><td>0.40852</td></tr><tr><td>ep_val_accuracy</td><td>0.21677</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>loss</td><td>█▇▆▆▆▅▅▅▄▄▄▄▄▄▄▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁</td></tr><tr><td>accuracy</td><td>▁▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>ep_training_loss</td><td>█▇▆▅▅▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>ep_training_accuracy</td><td>▁▂▂▃▄▄▄▅▅▆▆▆▇▇▇▇████</td></tr><tr><td>val loss</td><td>██▇▇▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val accuracy</td><td>▁▁▁▁▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▆▆▇▇▇▇██</td></tr><tr><td>ep_val_loss</td><td>█▇▆▅▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ep_val_accuracy</td><td>▁▂▃▄▄▅▅▆▆▇▇▇▇███████</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">flowing-sweep-1</strong>: <a href=\"https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203/runs/ijq9tksi\" target=\"_blank\">https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203/runs/ijq9tksi</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0kdteojo with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer_type: LSTM\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimiser: nadam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tunits: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.30<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">elated-sweep-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203\" target=\"_blank\">https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203/sweeps/ybc5yonl\" target=\"_blank\">https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203/sweeps/ybc5yonl</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203/runs/0kdteojo\" target=\"_blank\">https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203/runs/0kdteojo</a><br/>\n",
              "                Run data is saved locally in <code>/content/drive/My Drive/A3-checkpoints/wandb/run-20210524_135637-0kdteojo</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "batch_size:128 dropout:0 embedding_dim:16 epochs:20 layer_type:LSTM num_layers:1 optimiser:nadam units:64 \n",
            "Epoch 1 Batch 0 Loss 1.4374 Accuracy 0.0223\n",
            "Epoch 1 Batch 100 Loss 1.1776 Accuracy 0.0500\n",
            "Epoch 1 Batch 200 Loss 1.0650 Accuracy 0.0629\n",
            "Epoch 1 Batch 300 Loss 1.0308 Accuracy 0.0785\n",
            "Epoch 1 Batch 344 Loss 0.9947 Accuracy 0.0781\n",
            "Epoch 1 Loss 1.1034 Acc 0.0621\n",
            "Time taken for 1 epoch 66.52 sec\n",
            "\n",
            "Epoch 1 Batch 0 Val Loss 1.0062 Val Accuracy 0.0723\n",
            "Epoch 1 Batch 33 Val Loss 0.9838 Val Accuracy 0.0742\n",
            "Epoch 1 Val Loss 0.9818 Val Acc 0.0713\n",
            "Time taken for 1 epoch 79.91 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.0072 Accuracy 0.0703\n",
            "Epoch 2 Batch 100 Loss 0.9795 Accuracy 0.0910\n",
            "Epoch 2 Batch 200 Loss 0.9775 Accuracy 0.0859\n",
            "Epoch 2 Batch 300 Loss 0.9655 Accuracy 0.0848\n",
            "Epoch 2 Batch 344 Loss 0.9685 Accuracy 0.0844\n",
            "Epoch 2 Loss 0.9821 Acc 0.0835\n",
            "Time taken for 1 epoch 21.34 sec\n",
            "\n",
            "Epoch 2 Batch 0 Val Loss 0.9111 Val Accuracy 0.0812\n",
            "Epoch 2 Batch 33 Val Loss 0.9357 Val Accuracy 0.0832\n",
            "Epoch 2 Val Loss 0.9207 Val Acc 0.0837\n",
            "Time taken for 1 epoch 22.50 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.9321 Accuracy 0.0848\n",
            "Epoch 3 Batch 100 Loss 0.9301 Accuracy 0.0902\n",
            "Epoch 3 Batch 200 Loss 0.9033 Accuracy 0.0961\n",
            "Epoch 3 Batch 300 Loss 0.9551 Accuracy 0.1012\n",
            "Epoch 3 Batch 344 Loss 0.9087 Accuracy 0.0957\n",
            "Epoch 3 Loss 0.9189 Acc 0.0943\n",
            "Time taken for 1 epoch 21.96 sec\n",
            "\n",
            "Epoch 3 Batch 0 Val Loss 0.8534 Val Accuracy 0.0949\n",
            "Epoch 3 Batch 33 Val Loss 0.8877 Val Accuracy 0.0930\n",
            "Epoch 3 Val Loss 0.8507 Val Acc 0.0950\n",
            "Time taken for 1 epoch 22.86 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.9034 Accuracy 0.1000\n",
            "Epoch 4 Batch 100 Loss 0.8150 Accuracy 0.0965\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6421ac1f-81fc-4cec-9094-88b21ba143a6"
      },
      "source": [
        "## Translate"
      ],
      "id": "6421ac1f-81fc-4cec-9094-88b21ba143a6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5643c560-2f13-43ec-aba5-8fd6eb23762a"
      },
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((dataset.max_target_seq_length, dataset.max_input_seq_length))\n",
        "\n",
        "    sentence = dataset.preprocess_word(sentence)\n",
        "\n",
        "    inputs = [dataset.input_tokenizer.word_index[i] for i in sentence]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=dataset.max_input_seq_length,\n",
        "                                                         padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    if encoder.layer_type != \"LSTM\":\n",
        "        hidden = [tf.zeros((1, units))]\n",
        "    else:\n",
        "        hidden = [tf.zeros((1, units)), tf.zeros((1, units))]\n",
        "    \n",
        "    enc_out, enc_hidden, enc_cell = encoder(inputs, hidden)\n",
        "    \n",
        "    if decoder.layer_type != \"LSTM\":\n",
        "        dec_hidden = enc_hidden\n",
        "    else:\n",
        "        dec_hidden, dec_cell = enc_hidden, enc_cell\n",
        "\n",
        "    dec_input = tf.expand_dims([dataset.target_tokenizer.word_index[sos]], 0)\n",
        "\n",
        "    for t in range(dataset.max_target_seq_length):\n",
        "        # passing enc_output to the decoder\n",
        "        if decoder.layer_type != \"LSTM\":\n",
        "            predictions, _, _ = decoder(dec_input, dec_hidden)\n",
        "        else:\n",
        "            predictions, _, _ = decoder(dec_input, dec_hidden, dec_cell)\n",
        "                \n",
        "        \n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        \n",
        "        result += dataset.target_tokenizer.index_word[predicted_id]\n",
        "\n",
        "        if dataset.target_tokenizer.index_word[predicted_id] == eos:\n",
        "            return result, sentence\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence"
      ],
      "id": "5643c560-2f13-43ec-aba5-8fd6eb23762a",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9db2c059-3601-459b-b285-08cb6bda4f84"
      },
      "source": [
        "def translate(sentence):\n",
        "    result, sentence = evaluate(sentence)\n",
        "\n",
        "    print('Input:', sentence)\n",
        "    print('Predicted translation:', result)"
      ],
      "id": "9db2c059-3601-459b-b285-08cb6bda4f84",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00985e4f-7438-416c-93b0-647556ce517b",
        "outputId": "dc3a256e-78a9-4af4-9b67-6812617deb29"
      },
      "source": [
        "translate(\"false\")"
      ],
      "id": "00985e4f-7438-416c-93b0-647556ce517b",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: @false#\n",
            "Predicted translation: फालस#\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7n3LGKsiN8e",
        "outputId": "11ecf21f-7a56-4658-f57c-d689b1883459"
      },
      "source": [
        "sequence = dataset.input_tokenizer.texts_to_sequences(\"shibobrota\")\n",
        "np.reshape(sequence, len(sequence))"
      ],
      "id": "A7n3LGKsiN8e",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 9,  6,  5, 19, 11, 19,  7, 11,  8,  1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_Y_4LsajI-B",
        "outputId": "d12ff317-4c22-4cca-ab34-27b1f66ef754"
      },
      "source": [
        "text = dataset.input_tokenizer.sequences_to_texts(sequence)\n",
        "text"
      ],
      "id": "u_Y_4LsajI-B",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['s', 'h', 'i', 'b', 'o', 'b', 'r', 'o', 't', 'a']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU-lii9njfj4"
      },
      "source": [
        "def save_predictions(data_frame, name):\n",
        "    accuracy_count = 0;\n",
        "    with open(name, 'w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"INPUT\", \"PREDICTION\", \"TRUE\"])\n",
        "        for i, (inp, trg) in enumerate(zip(data_frame[1], data_frame[0])): \n",
        "            result, sentence, attention_plot = evaluate(inp)\n",
        "            writer.writerow([inp, result[:-1], trg])\n",
        "            print(inp, result[:-1], trg)\n",
        "            if result[:-1] == trg:\n",
        "                accuracy_count += 1\n",
        "            if (i+1) % 100 == 0 or i+1 == data_frame.size:\n",
        "                print(\"Accuracy\", (accuracy_count / (i+1)))\n",
        "\n",
        "    return accuracy_count/data_frame.size"
      ],
      "id": "zU-lii9njfj4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkzktH4_qvaB"
      },
      "source": [
        "save_predictions(test_df, \"new_without_attn_predictions.csv\")"
      ],
      "id": "kkzktH4_qvaB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTI16VpofDOy",
        "outputId": "80b0aef7-3e18-45f1-cb71-7ad530d061e7"
      },
      "source": [
        "dataset.target_tokenizer.word_index[sos], dataset.target_tokenizer.word_index[eos]"
      ],
      "id": "kTI16VpofDOy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWdbkwZhfEWF"
      },
      "source": [
        ""
      ],
      "id": "kWdbkwZhfEWF",
      "execution_count": null,
      "outputs": []
    }
  ]
}