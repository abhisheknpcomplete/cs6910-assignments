{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "multi layered without attention - configurable.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33f85e9d-215f-45c6-95ad-d56e6e3d3759"
      },
      "source": [
        "#### CS20M059 Shibobrota Das | CS20M007 Abhishek Kumar"
      ],
      "id": "33f85e9d-215f-45c6-95ad-d56e6e3d3759"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6700a5ec-7c66-45f5-9539-a593bb36ddc6"
      },
      "source": [
        "## Setup"
      ],
      "id": "6700a5ec-7c66-45f5-9539-a593bb36ddc6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b51cb32-fdbd-4f91-96ca-91ee41122b6b",
        "outputId": "0eba8c36-a79c-45fd-92a6-1b1b3919713d"
      },
      "source": [
        "!pip install tensorflow-addons -qqq"
      ],
      "id": "6b51cb32-fdbd-4f91-96ca-91ee41122b6b",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▌                               | 10kB 28.3MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 32.4MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 35.4MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 36.1MB/s eta 0:00:01\r\u001b[K     |██▍                             | 51kB 36.2MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 38.2MB/s eta 0:00:01\r\u001b[K     |███▍                            | 71kB 33.3MB/s eta 0:00:01\r\u001b[K     |███▉                            | 81kB 34.0MB/s eta 0:00:01\r\u001b[K     |████▍                           | 92kB 32.1MB/s eta 0:00:01\r\u001b[K     |████▉                           | 102kB 33.3MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 112kB 33.3MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 122kB 33.3MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 133kB 33.3MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 143kB 33.3MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 153kB 33.3MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 163kB 33.3MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 174kB 33.3MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 184kB 33.3MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 194kB 33.3MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 204kB 33.3MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 215kB 33.3MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 225kB 33.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 235kB 33.3MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 245kB 33.3MB/s eta 0:00:01\r\u001b[K     |████████████                    | 256kB 33.3MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 266kB 33.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 276kB 33.3MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 286kB 33.3MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 296kB 33.3MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 307kB 33.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 317kB 33.3MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 327kB 33.3MB/s eta 0:00:01\r\u001b[K     |████████████████                | 337kB 33.3MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 348kB 33.3MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 358kB 33.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 368kB 33.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 378kB 33.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 389kB 33.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 399kB 33.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 409kB 33.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 419kB 33.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 430kB 33.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 440kB 33.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 450kB 33.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 460kB 33.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 471kB 33.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 481kB 33.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 491kB 33.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 501kB 33.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 512kB 33.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 522kB 33.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 532kB 33.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 542kB 33.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 552kB 33.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 563kB 33.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 573kB 33.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 583kB 33.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 593kB 33.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 604kB 33.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 614kB 33.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 624kB 33.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 634kB 33.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 645kB 33.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 655kB 33.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 665kB 33.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 675kB 33.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 686kB 33.3MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a08fb4b5-e93b-4c82-bc45-e2a22a7dee95",
        "outputId": "9aa26258-94d8-46ab-887d-4c417ccff72b"
      },
      "source": [
        "!pip install wandb -qqq"
      ],
      "id": "a08fb4b5-e93b-4c82-bc45-e2a22a7dee95",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.8MB 28.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 55.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 56.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 14.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.5MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c70406c-fe4d-43f1-93a4-9dc617b301da",
        "outputId": "b3ca001e-317d-4141-b250-7172506554b4"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow import GradientTape\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import datetime\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Embedding, LSTM, GRU, SimpleRNN, SimpleRNNCell, LSTMCell, GRUCell\n",
        "from keras.models import Sequential\n",
        "from keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy\n",
        "import time\n",
        "import sys\n",
        "import datetime\n",
        "from sklearn.utils import shuffle\n",
        "import wandb\n",
        "# import nltk\n",
        "import csv\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from matplotlib.font_manager import FontProperties\n",
        "\n",
        "print(\"Using numpy:\",np.__version__)\n",
        "print(\"Using tensorflow:\",tf.__version__)\n",
        "print(\"Using tensorflow Addons:\",tfa.__version__)\n",
        "print(\"Using keras:\",keras.__version__)\n",
        "print(\"Using pandas:\",pd.__version__)"
      ],
      "id": "5c70406c-fe4d-43f1-93a4-9dc617b301da",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using numpy: 1.19.5\n",
            "Using tensorflow: 2.4.1\n",
            "Using tensorflow Addons: 0.13.0\n",
            "Using keras: 2.4.0\n",
            "Using pandas: 1.1.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddf43393-e89f-46f4-a711-f2e7078e9e85"
      },
      "source": [
        "# wandb.init(project='Assignment 3', entity='iitm-cs6910-jan-may-2021-cs20m059-cs20m007')"
      ],
      "id": "ddf43393-e89f-46f4-a711-f2e7078e9e85",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cd226e1-96b2-4440-a310-15a8cf2f0035",
        "outputId": "d6cfdc79-acc5-425a-8724-eab1907a8dcd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/My Drive/DL-A3 Dataset/dakshina_dataset_v1.0/hi/'"
      ],
      "id": "4cd226e1-96b2-4440-a310-15a8cf2f0035",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/DL-A3 Dataset/dakshina_dataset_v1.0/hi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "559c92d9-374e-4e0f-b2d1-7f17d895cc04"
      },
      "source": [
        "#### Load Data"
      ],
      "id": "559c92d9-374e-4e0f-b2d1-7f17d895cc04"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee85d467-1fac-40d1-a7ca-b00faf2d3d4b",
        "outputId": "ebf4aeed-9224-41a2-b6e0-bd04f5484417"
      },
      "source": [
        "val_df = pd.read_csv(\"./lexicons/hi.translit.sampled.dev.tsv\", sep='\\t', header=None)\n",
        "train_df = pd.read_csv(\"./lexicons/hi.translit.sampled.train.tsv\", sep='\\t', header=None)\n",
        "test_df = pd.read_csv(\"./lexicons/hi.translit.sampled.test.tsv\", sep='\\t', header=None)\n",
        "print(\"Data Loaded to Dataframes!\")"
      ],
      "id": "ee85d467-1fac-40d1-a7ca-b00faf2d3d4b",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Loaded to Dataframes!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13ae78d1-de5a-435a-8b39-088bd63f8e2c",
        "outputId": "25aad203-6ae0-43a8-f957-21814d75ebbb"
      },
      "source": [
        "%cd '/content/drive/My Drive/A3-checkpoints/'"
      ],
      "id": "13ae78d1-de5a-435a-8b39-088bd63f8e2c",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/A3-checkpoints\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caa6b5e6-844e-4fb9-9eeb-bcedd0cf6fd1"
      },
      "source": [
        "#### Dataset Samples"
      ],
      "id": "caa6b5e6-844e-4fb9-9eeb-bcedd0cf6fd1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "fefa85bd-2299-4503-8d29-458451185e8f",
        "outputId": "a992a677-cebb-4e44-e59b-5c34e1a649ca"
      },
      "source": [
        "train_df.sample(n=3)"
      ],
      "id": "fefa85bd-2299-4503-8d29-458451185e8f",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>40451</th>\n",
              "      <td>सांकृत्यायन</td>\n",
              "      <td>sankrityayan</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24505</th>\n",
              "      <td>प्रभावोत्पादक</td>\n",
              "      <td>prabhaavotpaadak</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21299</th>\n",
              "      <td>न्यायापालिका</td>\n",
              "      <td>nyayapalika</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   0                 1  2\n",
              "40451    सांकृत्यायन      sankrityayan  1\n",
              "24505  प्रभावोत्पादक  prabhaavotpaadak  1\n",
              "21299   न्यायापालिका       nyayapalika  2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "627e8bdb-9997-43b6-ba7e-e05879c1c4c0"
      },
      "source": [
        "## Preparing Dataset"
      ],
      "id": "627e8bdb-9997-43b6-ba7e-e05879c1c4c0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53abf538-55d3-46e0-9532-f0aded169217"
      },
      "source": [
        "sos = \"@\"\n",
        "eos = \"#\""
      ],
      "id": "53abf538-55d3-46e0-9532-f0aded169217",
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7035ff0-c322-4ef6-98ac-eae0416ad3a7"
      },
      "source": [
        "class LexDataset:\n",
        "    def __init__(self, input_tensor, target_tensor, batch_size):\n",
        "        self.input_tensor = input_tensor\n",
        "        self.target_tensor = target_tensor\n",
        "        self.batch = tf.data.Dataset.from_tensor_slices((self.input_tensor, self.target_tensor)).shuffle(len(self.input_tensor)).batch(batch_size, drop_remainder=True)"
      ],
      "id": "e7035ff0-c322-4ef6-98ac-eae0416ad3a7",
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce3cd7e2-7e30-44bb-8a27-77dbf944bea0"
      },
      "source": [
        "class TransliterationDatatset:\n",
        "    def __init__(self, df_list, batch_size = 64):\n",
        "        \n",
        "        self.input_tokenizer = None\n",
        "        self.target_tokenizer = None\n",
        "        self.train = None\n",
        "        self.val = None\n",
        "        self.test = None\n",
        "        self.batch_size = batch_size\n",
        "        # Load Data\n",
        "        self.load_dataset(df_list)\n",
        "        # Other parameters\n",
        "        self.num_input_tokens = len(self.input_tokenizer.index_word)+1\n",
        "        self.num_target_tokens = len(self.target_tokenizer.index_word)+1\n",
        "        self.max_input_seq_length = np.max([self.train.input_tensor.shape[1], self.val.input_tensor.shape[1], self.test.input_tensor.shape[1]])\n",
        "        self.max_target_seq_length = np.max([self.train.target_tensor.shape[1], self.val.target_tensor.shape[1], self.test.target_tensor.shape[1]])\n",
        "        \n",
        "    def preprocess_word(self, w):\n",
        "        return sos + str(w) + eos\n",
        "    \n",
        "    def print_input(self, tensor):\n",
        "        for t in tensor:\n",
        "            if t != 0:\n",
        "                print(f'{t} ----> {self.input_tokenizer.index_word[t]}')\n",
        "\n",
        "    def get_target_word(self, tensor):\n",
        "        word = []\n",
        "        for t in tensor:\n",
        "            if t != 0:\n",
        "                word.append(self.input_tokenizer.index_word[t])\n",
        "        return \"\".join([ch for ch in word])\n",
        "                \n",
        "    def print_target(self, tensor):\n",
        "        for t in tensor:\n",
        "            if t != 0:\n",
        "                print(f'{t} ----> {self.target_tokenizer.index_word[t]}')\n",
        "    \n",
        "    def create_dataset(self, data_frame):\n",
        "        input_words = []\n",
        "        target_words = []\n",
        "        # Shuffle the data_frame before creating dataset\n",
        "        df = data_frame\n",
        "        for i in range(5):\n",
        "            df = shuffle(df)\n",
        "        for x, y in zip(df[1], df[0]):\n",
        "            input_words.append(self.preprocess_word(x))\n",
        "            target_words.append(self.preprocess_word(y))\n",
        "        return (input_words, target_words)\n",
        "    \n",
        "    def load_dataset(self, df_list):\n",
        "        # df_list should have train -> val -> test in sequence\n",
        "        \n",
        "        self.input_tokenizer = Tokenizer(num_words = None, char_level = True)\n",
        "        self.target_tokenizer = Tokenizer(num_words = None, char_level = True)\n",
        "        \n",
        "        ds_list = []\n",
        "        \n",
        "        for df in df_list:\n",
        "            # Get the words list\n",
        "            (input_words, target_words) = self.create_dataset(df)\n",
        "            # Fit on the set of words\n",
        "            self.input_tokenizer.fit_on_texts(input_words)\n",
        "            self.target_tokenizer.fit_on_texts(target_words)\n",
        "            ds_list.append((input_words, target_words))\n",
        "                    \n",
        "        self.target_tokenizer.index_word.update({0:\" \"})\n",
        "        self.input_tokenizer.index_word.update({0:\" \"})\n",
        "        \n",
        "        input_word_len = []\n",
        "        target_word_len = []\n",
        "        \n",
        "        tensor_list = []\n",
        "        \n",
        "        for i, (input_words, target_words) in enumerate(ds_list):\n",
        "            input_tensor = self.input_tokenizer.texts_to_sequences(input_words)\n",
        "            target_tensor = self.target_tokenizer.texts_to_sequences(target_words)\n",
        "            tensor_list.append((input_tensor, target_tensor))\n",
        "            input_word_len.append(np.max([len(x) for x in input_tensor]))\n",
        "            target_word_len.append(np.max([len(x) for x in target_tensor]))\n",
        "        \n",
        "        for i, (input_tensor, target_tensor) in enumerate(tensor_list):\n",
        "            \n",
        "            input_tensor = pad_sequences(input_tensor, padding='post', maxlen = np.max(input_word_len))\n",
        "            target_tensor = pad_sequences(target_tensor, padding='post', maxlen = np.max(target_word_len))\n",
        "            \n",
        "            if i == 0:\n",
        "                self.train = LexDataset(input_tensor, target_tensor, self.batch_size)\n",
        "            elif i == 1:\n",
        "                self.val = LexDataset(input_tensor, target_tensor, self.batch_size)\n",
        "            else:\n",
        "                self.test = LexDataset(input_tensor, target_tensor, self.batch_size)"
      ],
      "id": "ce3cd7e2-7e30-44bb-8a27-77dbf944bea0",
      "execution_count": 278,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63b06317-dab4-4455-812e-1b34679e27b1",
        "scrolled": true,
        "tags": []
      },
      "source": [
        "dataset = TransliterationDatatset([train_df, val_df, test_df], 128)"
      ],
      "id": "63b06317-dab4-4455-812e-1b34679e27b1",
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fb817ef-9206-4341-807a-4b4db2113340"
      },
      "source": [
        "#### Training Data"
      ],
      "id": "1fb817ef-9206-4341-807a-4b4db2113340"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1da1956c-d5be-4d10-a263-f565341e5d6a",
        "tags": [],
        "outputId": "78e7cb45-cf61-4f4d-da44-362078c6508a"
      },
      "source": [
        "# Training data\n",
        "dataset.train.input_tensor.shape, dataset.train.target_tensor.shape"
      ],
      "id": "1da1956c-d5be-4d10-a263-f565341e5d6a",
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((44204, 22), (44204, 21))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a008930-d6da-4be0-898c-153f49b8845a"
      },
      "source": [
        "#### Validation Data"
      ],
      "id": "3a008930-d6da-4be0-898c-153f49b8845a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3c2a7df-ec58-40b6-9ba1-6b7782fdfa0c",
        "outputId": "7d0bfbae-0ab6-4e1a-c048-71e3c6d17577"
      },
      "source": [
        "# Validation data\n",
        "dataset.val.input_tensor.shape, dataset.val.target_tensor.shape"
      ],
      "id": "a3c2a7df-ec58-40b6-9ba1-6b7782fdfa0c",
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4358, 22), (4358, 21))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da416c97-6ef6-4855-8968-dfcd1fe8ff5a"
      },
      "source": [
        "#### Test Data"
      ],
      "id": "da416c97-6ef6-4855-8968-dfcd1fe8ff5a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa011970-5547-4783-a89d-8ab8c3438fa6",
        "outputId": "8d7822d1-bab5-4737-dde7-ed865c776a0b"
      },
      "source": [
        "# Test data\n",
        "dataset.test.input_tensor.shape, dataset.test.target_tensor.shape"
      ],
      "id": "fa011970-5547-4783-a89d-8ab8c3438fa6",
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4502, 22), (4502, 21))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a7ada4f-733d-4301-93b2-1ade66cde9a8"
      },
      "source": [
        "#### Number of Tokens"
      ],
      "id": "8a7ada4f-733d-4301-93b2-1ade66cde9a8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bba2c1c4-7fb6-454b-8210-13398ed7406b",
        "outputId": "ec8dd882-38b2-40c9-a4fb-8e96426d6a98"
      },
      "source": [
        "# Number of tokens\n",
        "dataset.num_input_tokens, dataset.num_target_tokens"
      ],
      "id": "bba2c1c4-7fb6-454b-8210-13398ed7406b",
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30, 67)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01a249a5-d5ea-46c6-ab4f-a7d0e08b05f3"
      },
      "source": [
        "#### Maximum Sequence Lengths"
      ],
      "id": "01a249a5-d5ea-46c6-ab4f-a7d0e08b05f3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c96afcb-d6ff-4b44-89f2-5d0797709ce4",
        "outputId": "fea50e2c-a1f4-48b3-b9da-4ab8d4661d08"
      },
      "source": [
        "# max seq length\n",
        "dataset.max_input_seq_length, dataset.max_target_seq_length"
      ],
      "id": "7c96afcb-d6ff-4b44-89f2-5d0797709ce4",
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22, 21)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef24fd33-e9fd-47c7-be52-62705caf2ebc"
      },
      "source": [
        "#### Example batch - dataset"
      ],
      "id": "ef24fd33-e9fd-47c7-be52-62705caf2ebc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3c95122-8a50-482b-ae5d-994a74f31765",
        "outputId": "8a4bb762-3384-4db8-c285-b0390c62ecef"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset.train.batch))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "id": "a3c95122-8a50-482b-ae5d-994a74f31765",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([128, 22]), TensorShape([128, 21]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc25ddd3-8cd9-4f65-98de-b4f5bddf8c13",
        "outputId": "4b4d128c-e3c2-4ee0-9748-258568e1cf76"
      },
      "source": [
        "dataset.print_input(example_input_batch[2].numpy())"
      ],
      "id": "cc25ddd3-8cd9-4f65-98de-b4f5bddf8c13",
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2 ----> @\n",
            "22 ----> c\n",
            "6 ----> h\n",
            "6 ----> h\n",
            "11 ----> o\n",
            "7 ----> r\n",
            "3 ----> #\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc385626-2c47-48cf-88c6-a8bce1ab1cdb",
        "outputId": "867dcdc2-b69b-432b-c11e-935e7dabba59"
      },
      "source": [
        "dataset.print_target(example_target_batch[2].numpy())"
      ],
      "id": "cc385626-2c47-48cf-88c6-a8bce1ab1cdb",
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 ----> @\n",
            "47 ----> छ\n",
            "19 ----> ो\n",
            "4 ----> र\n",
            "2 ----> #\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81646f07-05bf-4c75-aaf4-ce964bf9e458"
      },
      "source": [
        "## Encoder Model"
      ],
      "id": "81646f07-05bf-4c75-aaf4-ce964bf9e458"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29f2c233-4f9a-471a-8ff9-dfe5950a4963"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz, dropout=0.2, layer_type=\"GRU\", num_layers=1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.layer_type = layer_type\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.rnn_layers = []\n",
        "\n",
        "        if self.layer_type == \"LSTM\":\n",
        "            for i in range(self.num_layers):\n",
        "                self.rnn_layers.append(LSTM(self.enc_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       dropout = self.dropout,\n",
        "                                       recurrent_initializer='glorot_uniform'))\n",
        "        \n",
        "        elif self.layer_type == \"GRU\":\n",
        "            for i in range(self.num_layers):\n",
        "                self.rnn_layers.append(GRU(self.enc_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       dropout = self.dropout,\n",
        "                                       recurrent_initializer='glorot_uniform'))\n",
        "        \n",
        "        else:\n",
        "            for i in range(self.num_layers):\n",
        "                self.rnn_layers.append(SimpleRNN(self.enc_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       dropout = self.dropout,\n",
        "                                       recurrent_initializer='glorot_uniform'))        \n",
        "\n",
        "\n",
        "    def call(self, inputs, hidden):\n",
        "        inputs = self.embedding(inputs)\n",
        "        state_h, state_c = [], []\n",
        "\n",
        "        if self.layer_type == \"LSTM\":\n",
        "            output, h, c = self.rnn_layers[0](inputs, initial_state = hidden)\n",
        "            state_h.append(h)\n",
        "            state_c.append(c)\n",
        "            for i in range(1, self.num_layers):\n",
        "                output, h, c = self.rnn_layers[i](output, initial_state = hidden)\n",
        "                state_h.append(h)\n",
        "                state_c.append(c)\n",
        "            return output, state_h, state_c\n",
        "        \n",
        "        elif self.layer_type == \"GRU\":\n",
        "            output, h = self.rnn_layers[0](inputs, initial_state = hidden)\n",
        "            state_h.append(h)\n",
        "            for i in range(1, self.num_layers):\n",
        "                output, h = self.rnn_layers[i](output, initial_state = hidden)\n",
        "                state_h.append(h)\n",
        "            return output, state_h, None\n",
        "           \n",
        "        else:\n",
        "            output, h = self.rnn_layers[0](inputs, initial_state = hidden)\n",
        "            state_h.append(h)\n",
        "            for i in range(1, self.num_layers):\n",
        "                output, h = self.rnn_layers[i](output, initial_state = hidden)\n",
        "                state_h.append(h)\n",
        "            return output, state_h, None\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        if self.layer_type == \"LSTM\":\n",
        "            return [tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))]\n",
        "        else:\n",
        "            return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "id": "29f2c233-4f9a-471a-8ff9-dfe5950a4963",
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei4-t_PQdEBj"
      },
      "source": [
        "### Test Encoder"
      ],
      "id": "Ei4-t_PQdEBj"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59648301-2c83-4d6e-b066-3bb2ff53650c"
      },
      "source": [
        "vocab_inp_size = dataset.num_input_tokens\n",
        "embedding_dim = 64\n",
        "units = 256\n",
        "BATCH_SIZE = dataset.batch_size"
      ],
      "id": "59648301-2c83-4d6e-b066-3bb2ff53650c",
      "execution_count": 333,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8911625-ecf9-41b6-9234-7f241a71dd4a",
        "outputId": "d4a125d6-6013-4625-b94a-39b576c86b60"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE, 0.2, \"GRU\", 3)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden, sample_cell = encoder(example_input_batch, sample_hidden)\n",
        "print('Encoder output shape: (batch size, sequence length, units)', np.shape(sample_output))\n",
        "print('Encoder Hidden state shape: (batch size, units)', np.shape(sample_hidden))\n",
        "if encoder.layer_type == \"LSTM\":\n",
        "    print ('Encoder c vector shape: (batch size, units) {}'.format(np.shape(sample_cell)))"
      ],
      "id": "a8911625-ecf9-41b6-9234-7f241a71dd4a",
      "execution_count": 334,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (128, 22, 256)\n",
            "Encoder Hidden state shape: (batch size, units) (3, 128, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAiyXfzkd43y"
      },
      "source": [
        "## Decoder Model "
      ],
      "id": "pAiyXfzkd43y"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adbd8f1a-53b4-4d8d-bff6-478f2718bcd4"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, dropout=0.2, layer_type=\"GRU\", num_layers=1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.layer_type = layer_type\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.rnn_layers = []\n",
        "        \n",
        "        if self.layer_type == \"LSTM\":\n",
        "            for i in range(self.num_layers):\n",
        "                self.rnn_layers.append(LSTM(self.dec_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       dropout = self.dropout,\n",
        "                                       recurrent_initializer='glorot_uniform'))\n",
        "        \n",
        "        elif self.layer_type == \"GRU\":\n",
        "            for i in range(self.num_layers):\n",
        "                self.rnn_layers.append(GRU(self.dec_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       dropout = self.dropout,\n",
        "                                       recurrent_initializer='glorot_uniform'))\n",
        "           \n",
        "        else:\n",
        "            for i in range(self.num_layers):\n",
        "                self.rnn_layers.append(SimpleRNN(self.dec_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       dropout = self.dropout,\n",
        "                                       recurrent_initializer='glorot_uniform'))\n",
        "        \n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, state_h, state_c=None):\n",
        "        inputs = self.embedding(inputs)\n",
        "\n",
        "        if self.layer_type == \"LSTM\":\n",
        "            output, h, c = self.rnn_layers[0](inputs, initial_state = [state_h[0], state_c[0]])\n",
        "            for i in range(1, self.num_layers):\n",
        "                output, h, c = self.rnn_layers[i](output, initial_state = [state_h[i], state_c[i]])\n",
        "        \n",
        "        elif self.layer_type == \"GRU\":\n",
        "            output, h = self.rnn_layers[0](inputs, initial_state = state_h[0])\n",
        "            for i in range(1, self.num_layers):\n",
        "                output, h = self.rnn_layers[i](output, initial_state = state_h[i])\n",
        "           \n",
        "        else:\n",
        "            output, h = self.rnn_layers[0](inputs, initial_state = state_h[0])\n",
        "            for i in range(1, self.num_layers):\n",
        "                output, h = self.rnn_layers[i](output, initial_state = state_h[i])\n",
        "\n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "        # output shape == (batch_size, vocab)\n",
        "        x = self.fc(output)\n",
        "\n",
        "        # return x, state\n",
        "        if self.layer_type != \"LSTM\":\n",
        "            return x, h, None\n",
        "        else:\n",
        "            return x, h, c"
      ],
      "id": "adbd8f1a-53b4-4d8d-bff6-478f2718bcd4",
      "execution_count": 335,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6Wr2JTSdI1L"
      },
      "source": [
        "### Test Decoder"
      ],
      "id": "N6Wr2JTSdI1L"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7fb80f6-6d51-4b88-89aa-b10d4cbcb950"
      },
      "source": [
        "vocab_tar_size = dataset.num_target_tokens"
      ],
      "id": "f7fb80f6-6d51-4b88-89aa-b10d4cbcb950",
      "execution_count": 336,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4dd296f-1d52-4b1a-9caa-e56ae23af6ea",
        "outputId": "3396d230-8415-46ed-f6cb-39d1ce0d1be7"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, 0.2, \"GRU\", 3)\n",
        "\n",
        "if decoder.layer_type != \"LSTM\":\n",
        "    sample_decoder_output, sample_decoder_hidden, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_hidden, sample_cell)\n",
        "else:\n",
        "    sample_decoder_output, sample_decoder_hidden, sample_decoder_cell = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_hidden, sample_cell)\n",
        "\n",
        "print('Decoder output shape: (batch_size, vocab size)', sample_decoder_output.shape)\n",
        "print('Decoder Hidden state shape: (batch size, units)', sample_decoder_hidden.shape)\n",
        "if encoder.layer_type == \"LSTM\":\n",
        "    print ('Encoder c vector shape: (batch size, units) {}'.format(sample_decoder_cell.shape))"
      ],
      "id": "c4dd296f-1d52-4b1a-9caa-e56ae23af6ea",
      "execution_count": 337,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (128, 67)\n",
            "Decoder Hidden state shape: (batch size, units) (128, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51d718ae-5b4a-472f-9d62-e3b72a468217"
      },
      "source": [
        "## Optimizer and the loss function"
      ],
      "id": "51d718ae-5b4a-472f-9d62-e3b72a468217"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d21003ab-70ee-455d-a0bd-4e16a87f6c70"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Nadam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
      ],
      "id": "d21003ab-70ee-455d-a0bd-4e16a87f6c70",
      "execution_count": 365,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opNBWllg215N"
      },
      "source": [
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "id": "opNBWllg215N",
      "execution_count": 366,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw8k1SYOYdiV"
      },
      "source": [
        "## Accuracy (Word level)"
      ],
      "id": "yw8k1SYOYdiV"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uppr-mNwYkdK"
      },
      "source": [
        "def accuracy(real, pred):\n",
        "    real = tf.cast(real, tf.int32)\n",
        "    pred = tf.cast(pred, tf.int32)\n",
        "    return tf.reduce_mean(tf.cast(tf.equal(real, pred), tf.float32))"
      ],
      "id": "uppr-mNwYkdK",
      "execution_count": 375,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWAIHS3VUilM"
      },
      "source": [
        "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()"
      ],
      "id": "UWAIHS3VUilM",
      "execution_count": 376,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e633463-5d0c-4080-9e0d-5cc8cb88a9b9"
      },
      "source": [
        "## Training"
      ],
      "id": "7e633463-5d0c-4080-9e0d-5cc8cb88a9b9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90973b6a-e060-4e22-94f6-2400e7d700f2"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden, is_val = False):\n",
        "    loss = 0\n",
        "    spc_loss = 0\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden, enc_cell = encoder(inp, enc_hidden)\n",
        "\n",
        "        if decoder.layer_type != \"LSTM\":\n",
        "            dec_hidden = enc_hidden\n",
        "        else:\n",
        "            dec_hidden, dec_cell = enc_hidden, enc_cell\n",
        "\n",
        "        dec_input = tf.expand_dims([dataset.target_tokenizer.word_index[sos]] * BATCH_SIZE, 1)\n",
        "\n",
        "        pred = None\n",
        "\n",
        "        # Teacher forcing - feeding the target as the next input\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            \n",
        "            # passing enc_output to the decoder\n",
        "            if decoder.layer_type != \"LSTM\":\n",
        "                predictions, _, _ = decoder(dec_input, dec_hidden)\n",
        "            else:\n",
        "                predictions, _, _ = decoder(dec_input, dec_hidden, dec_cell)\n",
        "                        \n",
        "            loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "            # Update training metric.\n",
        "            train_acc_metric.update_state(targ[:, t], predictions)\n",
        "\n",
        "            # using teacher forcing\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "            # # without teacher forcing\n",
        "            # dec_input = tf.expand_dims(tf.argmax(predictions, axis=-1), 1)\n",
        "            if t == 1:\n",
        "                pred = tf.expand_dims(tf.argmax(predictions, axis=-1), 1)\n",
        "            else:\n",
        "                pred = tf.concat([pred, tf.expand_dims(tf.argmax(predictions, axis=-1), 1)], 1)\n",
        "    \n",
        "    batch_accuracy = accuracy(targ[:, 1:], pred)\n",
        "\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "    if not is_val:\n",
        "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "        gradients = tape.gradient(loss, variables)\n",
        "\n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss, batch_accuracy"
      ],
      "id": "90973b6a-e060-4e22-94f6-2400e7d700f2",
      "execution_count": 379,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df721112-cd36-4e6b-8608-3a1e06fcab84",
        "outputId": "84f8a7c6-0aa2-4d92-890f-0f4f26e7776e"
      },
      "source": [
        "EPOCHS = 1\n",
        "\n",
        "train_dataset = dataset.train\n",
        "val_dataset = dataset.val\n",
        "steps_per_epoch = len(train_dataset.input_tensor)//BATCH_SIZE\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "    val_total_loss = 0\n",
        "    val_total_accuracy = 0\n",
        "\n",
        "    train_dataset.batch.shuffle(1)\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(train_dataset.batch.take(steps_per_epoch)):\n",
        "        batch_loss, batch_accuracy = train_step(inp, targ, enc_hidden)\n",
        "        total_loss += batch_loss\n",
        "        total_accuracy += batch_accuracy\n",
        "\n",
        "        if batch % 100 == 0 or batch == steps_per_epoch-1:\n",
        "            print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f} Accuracy {batch_accuracy:.4f}')\n",
        "\n",
        "    print(f'Epoch {epoch+1} Loss {total_loss/steps_per_epoch:.4f} Acc {total_accuracy/steps_per_epoch:.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')\n",
        "\n",
        "    val_dataset.batch.shuffle(1)\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(val_dataset.batch.take(steps_per_epoch)):\n",
        "        val_batch_loss, val_batch_accuracy = train_step(inp, targ, enc_hidden)\n",
        "        val_total_loss += val_batch_loss\n",
        "        val_total_accuracy += val_batch_accuracy\n",
        "\n",
        "        if batch % 100 == 0 or batch == steps_per_epoch-1:\n",
        "            print(f'Epoch {epoch+1} Batch {batch} Val Loss {val_batch_loss.numpy():.4f} Val Accuracy {val_batch_accuracy:.4f}')\n",
        "\n",
        "    print(f'Epoch {epoch+1} Val Loss {val_total_loss/steps_per_epoch:.4f} Val Acc {val_total_accuracy/steps_per_epoch:.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
      ],
      "id": "df721112-cd36-4e6b-8608-3a1e06fcab84",
      "execution_count": 380,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"Identity:0\", shape=(), dtype=float32)\n",
            "Epoch 1 Batch 0 Loss 0.0742 Accuracy 0.3309\n",
            "Epoch 1 Batch 100 Loss 0.0512 Accuracy 0.3332\n",
            "Epoch 1 Batch 200 Loss 0.0491 Accuracy 0.3355\n",
            "Epoch 1 Batch 300 Loss 0.0550 Accuracy 0.3313\n",
            "Epoch 1 Batch 344 Loss 0.0524 Accuracy 0.3367\n",
            "Epoch 1 Loss 0.0583 Acc 0.3332\n",
            "Time taken for 1 epoch 53.40 sec\n",
            "\n",
            "Epoch 1 Batch 0 Val Loss 0.1490 Val Accuracy 0.2902\n",
            "Epoch 1 Val Loss 0.0101 Val Acc 0.0305\n",
            "Time taken for 1 epoch 55.67 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6421ac1f-81fc-4cec-9094-88b21ba143a6"
      },
      "source": [
        "## Translate"
      ],
      "id": "6421ac1f-81fc-4cec-9094-88b21ba143a6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5643c560-2f13-43ec-aba5-8fd6eb23762a"
      },
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((dataset.max_target_seq_length, dataset.max_input_seq_length))\n",
        "\n",
        "    sentence = dataset.preprocess_word(sentence)\n",
        "\n",
        "    inputs = [dataset.input_tokenizer.word_index[i] for i in sentence]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=dataset.max_input_seq_length,\n",
        "                                                         padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    if encoder.layer_type != \"LSTM\":\n",
        "        hidden = [tf.zeros((1, units))]\n",
        "    else:\n",
        "        hidden = [tf.zeros((1, units)), tf.zeros((1, units))]\n",
        "    \n",
        "    enc_out, enc_hidden, enc_cell = encoder(inputs, hidden)\n",
        "    \n",
        "    if decoder.layer_type != \"LSTM\":\n",
        "        dec_hidden = enc_hidden\n",
        "    else:\n",
        "        dec_hidden, dec_cell = enc_hidden, enc_cell\n",
        "\n",
        "    dec_input = tf.expand_dims([dataset.target_tokenizer.word_index[sos]], 0)\n",
        "\n",
        "    for t in range(dataset.max_target_seq_length):\n",
        "        # passing enc_output to the decoder\n",
        "        if decoder.layer_type != \"LSTM\":\n",
        "            predictions, _, _ = decoder(dec_input, dec_hidden)\n",
        "        else:\n",
        "            predictions, _, _ = decoder(dec_input, dec_hidden, dec_cell)\n",
        "                \n",
        "        \n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        \n",
        "        result += dataset.target_tokenizer.index_word[predicted_id]\n",
        "\n",
        "        if dataset.target_tokenizer.index_word[predicted_id] == eos:\n",
        "            return result, sentence\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence"
      ],
      "id": "5643c560-2f13-43ec-aba5-8fd6eb23762a",
      "execution_count": 381,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9db2c059-3601-459b-b285-08cb6bda4f84"
      },
      "source": [
        "def translate(sentence):\n",
        "    result, sentence = evaluate(sentence)\n",
        "\n",
        "    print('Input:', sentence)\n",
        "    print('Predicted translation:', result)"
      ],
      "id": "9db2c059-3601-459b-b285-08cb6bda4f84",
      "execution_count": 382,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00985e4f-7438-416c-93b0-647556ce517b",
        "outputId": "77728ab4-1779-4cee-f7a8-14bf0062118d"
      },
      "source": [
        "translate(\"angad\")"
      ],
      "id": "00985e4f-7438-416c-93b0-647556ce517b",
      "execution_count": 386,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: @angad#\n",
            "Predicted translation: अंगद#\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7n3LGKsiN8e",
        "outputId": "11ecf21f-7a56-4658-f57c-d689b1883459"
      },
      "source": [
        "sequence = dataset.input_tokenizer.texts_to_sequences(\"shibobrota\")\n",
        "np.reshape(sequence, len(sequence))"
      ],
      "id": "A7n3LGKsiN8e",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 9,  6,  5, 19, 11, 19,  7, 11,  8,  1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_Y_4LsajI-B",
        "outputId": "d12ff317-4c22-4cca-ab34-27b1f66ef754"
      },
      "source": [
        "text = dataset.input_tokenizer.sequences_to_texts(sequence)\n",
        "text"
      ],
      "id": "u_Y_4LsajI-B",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['s', 'h', 'i', 'b', 'o', 'b', 'r', 'o', 't', 'a']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU-lii9njfj4"
      },
      "source": [
        "def save_predictions(data_frame, name):\n",
        "    accuracy_count = 0;\n",
        "    with open(name, 'w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"INPUT\", \"PREDICTION\", \"TRUE\"])\n",
        "        for i, (inp, trg) in enumerate(zip(data_frame[1], data_frame[0])): \n",
        "            result, sentence, attention_plot = evaluate(inp)\n",
        "            writer.writerow([inp, result[:-1], trg])\n",
        "            print(inp, result[:-1], trg)\n",
        "            if result[:-1] == trg:\n",
        "                accuracy_count += 1\n",
        "            if (i+1) % 100 == 0 or i+1 == data_frame.size:\n",
        "                print(\"Accuracy\", (accuracy_count / (i+1)))\n",
        "\n",
        "    return accuracy_count/data_frame.size"
      ],
      "id": "zU-lii9njfj4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkzktH4_qvaB"
      },
      "source": [
        "save_predictions(test_df, \"new_without_attn_predictions.csv\")"
      ],
      "id": "kkzktH4_qvaB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTI16VpofDOy",
        "outputId": "80b0aef7-3e18-45f1-cb71-7ad530d061e7"
      },
      "source": [
        "dataset.target_tokenizer.word_index[sos], dataset.target_tokenizer.word_index[eos]"
      ],
      "id": "kTI16VpofDOy",
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWdbkwZhfEWF"
      },
      "source": [
        ""
      ],
      "id": "kWdbkwZhfEWF",
      "execution_count": null,
      "outputs": []
    }
  ]
}