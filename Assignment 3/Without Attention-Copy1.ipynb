{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33f85e9d-215f-45c6-95ad-d56e6e3d3759",
   "metadata": {
    "id": "auburn-draft"
   },
   "source": [
    "#### CS20M059 Shibobrota Das | CS20M007 Abhishek Kumar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6700a5ec-7c66-45f5-9539-a593bb36ddc6",
   "metadata": {
    "id": "great-tiffany"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6b51cb32-fdbd-4f91-96ca-91ee41122b6b",
   "metadata": {
    "id": "gaKxXHmTkdRL"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow-addons -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a08fb4b5-e93b-4c82-bc45-e2a22a7dee95",
   "metadata": {
    "id": "xrxO62AQkoWM"
   },
   "outputs": [],
   "source": [
    "!pip install wandb -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c70406c-fe4d-43f1-93a4-9dc617b301da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "skilled-effort",
    "outputId": "b0039f46-48e2-4357-ff63-84e31e10ce49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using numpy: 1.19.5\n",
      "Using tensorflow: 2.5.0\n",
      "Using tensorflow Addons: 0.13.0\n",
      "Using keras: 2.5.0\n",
      "Using pandas: 1.2.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import GradientTape\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU, SimpleRNN, SimpleRNNCell, LSTMCell, GRUCell\n",
    "from keras.models import Sequential\n",
    "from keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy\n",
    "import time\n",
    "import sys\n",
    "import datetime\n",
    "from sklearn.utils import shuffle\n",
    "import wandb\n",
    "# import nltk\n",
    "import csv\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Using numpy:\",np.__version__)\n",
    "print(\"Using tensorflow:\",tf.__version__)\n",
    "print(\"Using tensorflow Addons:\",tfa.__version__)\n",
    "print(\"Using keras:\",keras.__version__)\n",
    "print(\"Using pandas:\",pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ddf43393-e89f-46f4-a711-f2e7078e9e85",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 631
    },
    "id": "split-cardiff",
    "outputId": "928563a1-ca11-4647-e59b-2ecd1757b9ff"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2ieo5mk6) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:2ieo5mk6). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">crisp-water-211</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203\" target=\"_blank\">https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203/runs/6g6t6l2c\" target=\"_blank\">https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203/runs/6g6t6l2c</a><br/>\n",
       "                Run data is saved locally in <code>/content/drive/My Drive/A3-checkpoints/wandb/run-20210520_185916-6g6t6l2c</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(6g6t6l2c)</h1><iframe src=\"https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203/runs/6g6t6l2c\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fa6c5de0bd0>"
      ]
     },
     "execution_count": 119,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wandb.init(project='Assignment 3', entity='iitm-cs6910-jan-may-2021-cs20m059-cs20m007')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cd226e1-96b2-4440-a310-15a8cf2f0035",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z_G5-UqDjYwf",
    "outputId": "e9af9df0-72ef-4334-b168-df94d0f8720e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "/content/drive/My Drive/DL-A3 Dataset/dakshina_dataset_v1.0/hi\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd '/content/drive/My Drive/DL-A3 Dataset/dakshina_dataset_v1.0/hi/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559c92d9-374e-4e0f-b2d1-7f17d895cc04",
   "metadata": {
    "id": "sweet-citizenship"
   },
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee85d467-1fac-40d1-a7ca-b00faf2d3d4b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "boolean-knock",
    "outputId": "e3bcd07b-b14f-4ad3-9479-964b98ca4997"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded to Dataframes!\n"
     ]
    }
   ],
   "source": [
    "val_df = pd.read_csv(\"./lexicons/hi.translit.sampled.dev.tsv\", sep='\\t', header=None)\n",
    "train_df = pd.read_csv(\"./lexicons/hi.translit.sampled.train.tsv\", sep='\\t', header=None)\n",
    "test_df = pd.read_csv(\"./lexicons/hi.translit.sampled.test.tsv\", sep='\\t', header=None)\n",
    "print(\"Data Loaded to Dataframes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13ae78d1-de5a-435a-8b39-088bd63f8e2c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "awJtOAhGkDQc",
    "outputId": "6d5a2121-6e37-49a5-a43b-ca4eb46d5bf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/A3-checkpoints\n"
     ]
    }
   ],
   "source": [
    "%cd '/content/drive/My Drive/A3-checkpoints/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa6b5e6-844e-4fb9-9eeb-bcedd0cf6fd1",
   "metadata": {
    "id": "given-prediction"
   },
   "source": [
    "#### Dataset Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fefa85bd-2299-4503-8d29-458451185e8f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "measured-restriction",
    "outputId": "fb1b28c1-df14-49c9-d94e-313d2b75d6af"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1556</th>\n",
       "      <td>टेस्टोस्टेरॉन</td>\n",
       "      <td>testosteron</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4059</th>\n",
       "      <td>सुपरनोवा</td>\n",
       "      <td>supernova</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>धारकों</td>\n",
       "      <td>dharakon</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0            1  2\n",
       "1556  टेस्टोस्टेरॉन  testosteron  1\n",
       "4059       सुपरनोवा    supernova  3\n",
       "1980         धारकों     dharakon  1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.sample(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627e8bdb-9997-43b6-ba7e-e05879c1c4c0",
   "metadata": {
    "id": "furnished-check"
   },
   "source": [
    "## Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53abf538-55d3-46e0-9532-f0aded169217",
   "metadata": {
    "id": "standing-reducing"
   },
   "outputs": [],
   "source": [
    "sos = \"@\"\n",
    "eos = \"#\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7035ff0-c322-4ef6-98ac-eae0416ad3a7",
   "metadata": {
    "id": "common-matter"
   },
   "outputs": [],
   "source": [
    "class LexDataset:\n",
    "    def __init__(self, input_tensor, target_tensor, batch_size):\n",
    "        self.input_tensor = input_tensor\n",
    "        self.target_tensor = target_tensor\n",
    "        self.batch = tf.data.Dataset.from_tensor_slices((self.input_tensor, self.target_tensor)).shuffle(len(self.input_tensor)).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce3cd7e2-7e30-44bb-8a27-77dbf944bea0",
   "metadata": {
    "id": "continental-transport"
   },
   "outputs": [],
   "source": [
    "class TransliterationDatatset:\n",
    "    def __init__(self, df_list, batch_size = 64):\n",
    "        \n",
    "        self.input_tokenizer = None\n",
    "        self.target_tokenizer = None\n",
    "        self.train = None\n",
    "        self.val = None\n",
    "        self.test = None\n",
    "        self.batch_size = batch_size\n",
    "        # Load Data\n",
    "        self.load_dataset(df_list)\n",
    "        # Other parameters\n",
    "        self.num_input_tokens = len(self.input_tokenizer.index_word)+1\n",
    "        self.num_target_tokens = len(self.target_tokenizer.index_word)+1\n",
    "        self.max_input_seq_length = np.max([self.train.input_tensor.shape[1], self.val.input_tensor.shape[1], self.test.input_tensor.shape[1]])\n",
    "        self.max_target_seq_length = np.max([self.train.target_tensor.shape[1], self.val.target_tensor.shape[1], self.test.target_tensor.shape[1]])\n",
    "        \n",
    "    def preprocess_word(self, w):\n",
    "        return sos + str(w) + eos\n",
    "    \n",
    "    def print_input(self, tensor):\n",
    "        for t in tensor:\n",
    "            if t != 0:\n",
    "                print(f'{t} ----> {self.input_tokenizer.index_word[t]}')\n",
    "                \n",
    "    def print_target(self, tensor):\n",
    "        for t in tensor:\n",
    "            if t != 0:\n",
    "                print(f'{t} ----> {self.target_tokenizer.index_word[t]}')\n",
    "    \n",
    "    def create_dataset(self, data_frame):\n",
    "        input_words = []\n",
    "        target_words = []\n",
    "        # Shuffle the data_frame before creating dataset\n",
    "        df = data_frame\n",
    "        for i in range(5):\n",
    "            df = shuffle(df)\n",
    "        for x, y in zip(df[1], df[0]):\n",
    "            input_words.append(self.preprocess_word(x))\n",
    "            target_words.append(self.preprocess_word(y))\n",
    "        return (input_words, target_words)\n",
    "    \n",
    "    def load_dataset(self, df_list):\n",
    "        # df_list should have train -> val -> test in sequence\n",
    "        \n",
    "        self.input_tokenizer = Tokenizer(num_words = None, char_level = True)\n",
    "        self.target_tokenizer = Tokenizer(num_words = None, char_level = True)\n",
    "        \n",
    "        ds_list = []\n",
    "        \n",
    "        for df in df_list:\n",
    "            # Get the words list\n",
    "            (input_words, target_words) = self.create_dataset(df)\n",
    "            # Fit on the set of words\n",
    "            self.input_tokenizer.fit_on_texts(input_words)\n",
    "            self.target_tokenizer.fit_on_texts(target_words)\n",
    "            ds_list.append((input_words, target_words))\n",
    "                    \n",
    "        self.target_tokenizer.index_word.update({0:\" \"})\n",
    "        self.input_tokenizer.index_word.update({0:\" \"})\n",
    "        \n",
    "        input_word_len = []\n",
    "        target_word_len = []\n",
    "        \n",
    "        tensor_list = []\n",
    "        \n",
    "        for i, (input_words, target_words) in enumerate(ds_list):\n",
    "            input_tensor = self.input_tokenizer.texts_to_sequences(input_words)\n",
    "            target_tensor = self.target_tokenizer.texts_to_sequences(target_words)\n",
    "            tensor_list.append((input_tensor, target_tensor))\n",
    "            input_word_len.append(np.max([len(x) for x in input_tensor]))\n",
    "            target_word_len.append(np.max([len(x) for x in target_tensor]))\n",
    "        \n",
    "        for i, (input_tensor, target_tensor) in enumerate(tensor_list):\n",
    "            \n",
    "            input_tensor = pad_sequences(input_tensor, padding='post', maxlen = np.max(input_word_len))\n",
    "            target_tensor = pad_sequences(target_tensor, padding='post', maxlen = np.max(target_word_len))\n",
    "            \n",
    "            if i == 0:\n",
    "                self.train = LexDataset(input_tensor, target_tensor, self.batch_size)\n",
    "            elif i == 1:\n",
    "                self.val = LexDataset(input_tensor, target_tensor, self.batch_size)\n",
    "            else:\n",
    "                self.test = LexDataset(input_tensor, target_tensor, self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63b06317-dab4-4455-812e-1b34679e27b1",
   "metadata": {
    "id": "offensive-islam",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = TransliterationDatatset([train_df, val_df, test_df], 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb817ef-9206-4341-807a-4b4db2113340",
   "metadata": {
    "id": "traditional-oxygen"
   },
   "source": [
    "#### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1da1956c-d5be-4d10-a263-f565341e5d6a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "soviet-speed",
    "outputId": "2c158b50-cf42-4f17-ac18-0cd7cb296d33",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((44204, 20), (44204, 21))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training data\n",
    "dataset.train.input_tensor.shape, dataset.train.target_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a008930-d6da-4be0-898c-153f49b8845a",
   "metadata": {
    "id": "sorted-geneva"
   },
   "source": [
    "#### Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3c2a7df-ec58-40b6-9ba1-6b7782fdfa0c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "great-neutral",
    "outputId": "d6053b61-47cb-414d-94ee-f6ac0296af74"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4358, 20), (4358, 21))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation data\n",
    "dataset.val.input_tensor.shape, dataset.val.target_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da416c97-6ef6-4855-8968-dfcd1fe8ff5a",
   "metadata": {
    "id": "wanted-pharmacy"
   },
   "source": [
    "#### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa011970-5547-4783-a89d-8ab8c3438fa6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "intermediate-survey",
    "outputId": "da8f3d32-f387-4087-bd94-d28b2c1bfed4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4502, 20), (4502, 21))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test data\n",
    "dataset.test.input_tensor.shape, dataset.test.target_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7ada4f-733d-4301-93b2-1ade66cde9a8",
   "metadata": {
    "id": "great-print"
   },
   "source": [
    "#### Number of Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bba2c1c4-7fb6-454b-8210-13398ed7406b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "affecting-toner",
    "outputId": "5d75818e-099c-460c-e598-a0176e19a5f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 67)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of tokens\n",
    "dataset.num_input_tokens, dataset.num_target_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a249a5-d5ea-46c6-ab4f-a7d0e08b05f3",
   "metadata": {
    "id": "french-jason"
   },
   "source": [
    "#### Maximum Sequence Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c96afcb-d6ff-4b44-89f2-5d0797709ce4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "regional-friday",
    "outputId": "916fb584-b8f1-4ea9-c98d-3bc1505194cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 21)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max seq length\n",
    "dataset.max_input_seq_length, dataset.max_target_seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef24fd33-e9fd-47c7-be52-62705caf2ebc",
   "metadata": {
    "id": "incredible-gallery"
   },
   "source": [
    "#### Example batch - dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a3c95122-8a50-482b-ae5d-994a74f31765",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "academic-exploration",
    "outputId": "f16577f2-d3f5-4e7f-81c9-c3666f256f1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([128, 22]), TensorShape([128, 21]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset.train.batch))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc25ddd3-8cd9-4f65-98de-b4f5bddf8c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 ----> @\n",
      "8 ----> t\n",
      "10 ----> e\n",
      "23 ----> j\n",
      "5 ----> i\n",
      "3 ----> #\n"
     ]
    }
   ],
   "source": [
    "dataset.print_input(example_input_batch[2].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc385626-2c47-48cf-88c6-a8bce1ab1cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ----> @\n",
      "11 ----> त\n",
      "14 ----> े\n",
      "21 ----> ज\n",
      "33 ----> ़\n",
      "12 ----> ी\n",
      "2 ----> #\n"
     ]
    }
   ],
   "source": [
    "dataset.print_target(example_target_batch[2].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81646f07-05bf-4c75-aaf4-ce964bf9e458",
   "metadata": {},
   "source": [
    "## Encoder and Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29f2c233-4f9a-471a-8ff9-dfe5950a4963",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59648301-2c83-4d6e-b066-3bb2ff53650c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_inp_size = dataset.num_input_tokens\n",
    "embedding_dim = 64\n",
    "units = 256\n",
    "BATCH_SIZE = dataset.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8911625-ecf9-41b6-9234-7f241a71dd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (128, 22, 256)\n",
      "Encoder Hidden state shape: (batch size, units) (128, 256)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print('Encoder output shape: (batch size, sequence length, units)', sample_output.shape)\n",
    "print('Encoder Hidden state shape: (batch size, units)', sample_hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4cd28da3-6384-4280-9c67-9d542884f55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "955fd636-70c1-4208-b59b-1b19d3090e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (128, 256)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (128, 22, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units)\", attention_result.shape)\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1)\", attention_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "adbd8f1a-53b4-4d8d-bff6-478f2718bcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f7fb80f6-6d51-4b88-89aa-b10d4cbcb950",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_tar_size = dataset.num_target_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c4dd296f-1d52-4b1a-9caa-e56ae23af6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (128, 67)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print('Decoder output shape: (batch_size, vocab size)', sample_decoder_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d718ae-5b4a-472f-9d62-e3b72a468217",
   "metadata": {},
   "source": [
    "## Optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d21003ab-70ee-455d-a0bd-4e16a87f6c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                            reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e633463-5d0c-4080-9e0d-5cc8cb88a9b9",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "90973b6a-e060-4e22-94f6-2400e7d700f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([dataset.target_tokenizer.word_index[sos]] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df721112-cd36-4e6b-8608-3a1e06fcab84",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "steps_per_epoch = len(dataset.train.input_tensor)//BATCH_SIZE\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.train.batch.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "        print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss/steps_per_epoch:.4f}')\n",
    "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6421ac1f-81fc-4cec-9094-88b21ba143a6",
   "metadata": {},
   "source": [
    "## Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5643c560-2f13-43ec-aba5-8fd6eb23762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([dataset.target_tokenizer.word_index[sos]], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                         dec_hidden,\n",
    "                                                         enc_out)\n",
    "\n",
    "    # storing the attention weights to plot later on\n",
    "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "    attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "    result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "    if targ_lang.index_word[predicted_id] == eos:\n",
    "        return result, sentence, attention_plot\n",
    "\n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba6377a-ec7d-4c48-8b40-e6e75bd64de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db2c059-3601-459b-b285-08cb6bda4f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "    print('Input:', sentence)\n",
    "    print('Predicted translation:', result)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')),\n",
    "                                  :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00985e4f-7438-416c-93b0-647556ce517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('hello')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
