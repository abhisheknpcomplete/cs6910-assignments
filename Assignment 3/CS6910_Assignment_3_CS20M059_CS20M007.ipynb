{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "auburn-draft",
   "metadata": {
    "id": "auburn-draft"
   },
   "source": [
    "#### CS20M059 Shibobrota Das | CS20M007 Abhishek Kumar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-tiffany",
   "metadata": {
    "id": "great-tiffany"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "gaKxXHmTkdRL",
   "metadata": {
    "id": "gaKxXHmTkdRL"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow-addons -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "xrxO62AQkoWM",
   "metadata": {
    "id": "xrxO62AQkoWM"
   },
   "outputs": [],
   "source": [
    "!pip install wandb -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "skilled-effort",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "skilled-effort",
    "outputId": "b0039f46-48e2-4357-ff63-84e31e10ce49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using numpy: 1.19.5\n",
      "Using tensorflow: 2.5.0\n",
      "Using tensorflow Addons: 0.13.0\n",
      "Using keras: 2.5.0\n",
      "Using pandas: 1.2.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import GradientTape\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU, SimpleRNN, SimpleRNNCell, LSTMCell, GRUCell\n",
    "from keras.models import Sequential\n",
    "from keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy\n",
    "import time\n",
    "import sys\n",
    "import datetime\n",
    "from sklearn.utils import shuffle\n",
    "import wandb\n",
    "# import nltk\n",
    "import csv\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Using numpy:\",np.__version__)\n",
    "print(\"Using tensorflow:\",tf.__version__)\n",
    "print(\"Using tensorflow Addons:\",tfa.__version__)\n",
    "print(\"Using keras:\",keras.__version__)\n",
    "print(\"Using pandas:\",pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "split-cardiff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 631
    },
    "id": "split-cardiff",
    "outputId": "928563a1-ca11-4647-e59b-2ecd1757b9ff"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2ieo5mk6) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:2ieo5mk6). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">crisp-water-211</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203\" target=\"_blank\">https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203/runs/6g6t6l2c\" target=\"_blank\">https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203/runs/6g6t6l2c</a><br/>\n",
       "                Run data is saved locally in <code>/content/drive/My Drive/A3-checkpoints/wandb/run-20210520_185916-6g6t6l2c</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(6g6t6l2c)</h1><iframe src=\"https://wandb.ai/iitm-cs6910-jan-may-2021-cs20m059-cs20m007/Assignment%203/runs/6g6t6l2c\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fa6c5de0bd0>"
      ]
     },
     "execution_count": 119,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project='Assignment 3', entity='iitm-cs6910-jan-may-2021-cs20m059-cs20m007')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "Z_G5-UqDjYwf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z_G5-UqDjYwf",
    "outputId": "e9af9df0-72ef-4334-b168-df94d0f8720e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "/content/drive/My Drive/DL-A3 Dataset/dakshina_dataset_v1.0/hi\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd '/content/drive/My Drive/DL-A3 Dataset/dakshina_dataset_v1.0/hi/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-citizenship",
   "metadata": {
    "id": "sweet-citizenship"
   },
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "boolean-knock",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "boolean-knock",
    "outputId": "e3bcd07b-b14f-4ad3-9479-964b98ca4997"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded to Dataframes!\n"
     ]
    }
   ],
   "source": [
    "val_df = pd.read_csv(\"./lexicons/hi.translit.sampled.dev.tsv\", sep='\\t', header=None)\n",
    "train_df = pd.read_csv(\"./lexicons/hi.translit.sampled.train.tsv\", sep='\\t', header=None)\n",
    "test_df = pd.read_csv(\"./lexicons/hi.translit.sampled.test.tsv\", sep='\\t', header=None)\n",
    "print(\"Data Loaded to Dataframes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "awJtOAhGkDQc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "awJtOAhGkDQc",
    "outputId": "6d5a2121-6e37-49a5-a43b-ca4eb46d5bf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/A3-checkpoints\n"
     ]
    }
   ],
   "source": [
    "%cd '/content/drive/My Drive/A3-checkpoints/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-prediction",
   "metadata": {
    "id": "given-prediction"
   },
   "source": [
    "#### Dataset Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "measured-restriction",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "measured-restriction",
    "outputId": "fb1b28c1-df14-49c9-d94e-313d2b75d6af"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1134</th>\n",
       "      <td>चिंताजनक</td>\n",
       "      <td>chintajank</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>ऋतुराज</td>\n",
       "      <td>ruturaj</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2739</th>\n",
       "      <td>बिताते</td>\n",
       "      <td>bitaate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0           1  2\n",
       "1134  चिंताजनक  chintajank  1\n",
       "472     ऋतुराज     ruturaj  1\n",
       "2739    बिताते     bitaate  1"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.sample(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-check",
   "metadata": {
    "id": "furnished-check"
   },
   "source": [
    "## Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "standing-reducing",
   "metadata": {
    "id": "standing-reducing"
   },
   "outputs": [],
   "source": [
    "sos = \"@\"\n",
    "eos = \"#\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "common-matter",
   "metadata": {
    "id": "common-matter"
   },
   "outputs": [],
   "source": [
    "class LexDataset:\n",
    "    def __init__(self, input_tensor, target_tensor, batch_size):\n",
    "        self.input_tensor = input_tensor\n",
    "        self.target_tensor = target_tensor\n",
    "        self.batch = tf.data.Dataset.from_tensor_slices((self.input_tensor, self.target_tensor)).shuffle(len(self.input_tensor)).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "continental-transport",
   "metadata": {
    "id": "continental-transport"
   },
   "outputs": [],
   "source": [
    "class TransliterationDatatset:\n",
    "    def __init__(self, df_list, problem_type = \"en-hi\", batch_size = 128):\n",
    "        self.problem_type = problem_type\n",
    "        self.input_tokenizer = None\n",
    "        self.target_tokenizer = None\n",
    "        self.train = None\n",
    "        self.val = None\n",
    "        self.test = None\n",
    "        self.batch_size = batch_size\n",
    "        # Load Data\n",
    "        self.load_dataset(df_list)\n",
    "        \n",
    "    def preprocess_word(self, w):\n",
    "        return sos + str(w) + eos\n",
    "    \n",
    "    def create_dataset(self, data_frame):\n",
    "        input_words = []\n",
    "        target_words = []\n",
    "        # Shuffle the data_frame before creating dataset\n",
    "        df_shuffled = shuffle(data_frame)\n",
    "        for x, y in zip(df_shuffled[1], df_shuffled[0]):\n",
    "            input_words.append(self.preprocess_word(x))\n",
    "            target_words.append(self.preprocess_word(y))\n",
    "        return (input_words, target_words)\n",
    "    \n",
    "    def load_dataset(self, df_list):\n",
    "        # df_list should have train -> val -> test in sequence\n",
    "        \n",
    "        self.input_tokenizer = Tokenizer(num_words = None, char_level = True)\n",
    "        self.target_tokenizer = Tokenizer(num_words = None, char_level = True)\n",
    "        \n",
    "        ds_list = []\n",
    "        \n",
    "        for df in df_list:\n",
    "            # Get the words list\n",
    "            (input_words, target_words) = self.create_dataset(df)\n",
    "            # Fit on the set of words\n",
    "            self.input_tokenizer.fit_on_texts(input_words)\n",
    "            self.target_tokenizer.fit_on_texts(target_words)\n",
    "            ds_list.append((input_words, target_words))\n",
    "                    \n",
    "        self.target_tokenizer.index_word.update({0:\" \"})\n",
    "        self.input_tokenizer.index_word.update({0:\" \"})\n",
    "        \n",
    "        input_word_len = []\n",
    "        target_word_len = []\n",
    "        \n",
    "        tensor_list = []\n",
    "        \n",
    "        for i, (input_words, target_words) in enumerate(ds_list):\n",
    "            input_tensor = self.input_tokenizer.texts_to_sequences(input_words)\n",
    "            target_tensor = self.target_tokenizer.texts_to_sequences(target_words)\n",
    "            tensor_list.append((input_tensor, target_tensor))\n",
    "            input_word_len.append(np.max([len(x) for x in input_tensor]))\n",
    "            target_word_len.append(np.max([len(x) for x in target_tensor]))\n",
    "        \n",
    "        for i, (input_tensor, target_tensor) in enumerate(tensor_list):\n",
    "            \n",
    "            input_tensor = pad_sequences(input_tensor, padding='post', maxlen = np.max(input_word_len))\n",
    "            target_tensor = pad_sequences(target_tensor, padding='post', maxlen = np.max(target_word_len))\n",
    "            \n",
    "            if i == 0:\n",
    "                self.train = LexDataset(input_tensor, target_tensor, self.batch_size)\n",
    "            elif i == 1:\n",
    "                self.val = LexDataset(input_tensor, target_tensor, self.batch_size)\n",
    "            else:\n",
    "                self.test = LexDataset(input_tensor, target_tensor, self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "offensive-islam",
   "metadata": {
    "id": "offensive-islam",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = TransliterationDatatset([train_df, val_df, test_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-oxygen",
   "metadata": {
    "id": "traditional-oxygen"
   },
   "source": [
    "#### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "soviet-speed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "soviet-speed",
    "outputId": "2c158b50-cf42-4f17-ac18-0cd7cb296d33",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((44204, 22), (44204, 21))"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training data\n",
    "dataset.train.input_tensor.shape, dataset.train.target_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-geneva",
   "metadata": {
    "id": "sorted-geneva"
   },
   "source": [
    "#### Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "great-neutral",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "great-neutral",
    "outputId": "d6053b61-47cb-414d-94ee-f6ac0296af74"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4358, 22), (4358, 21))"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation data\n",
    "dataset.val.input_tensor.shape, dataset.val.target_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-pharmacy",
   "metadata": {
    "id": "wanted-pharmacy"
   },
   "source": [
    "#### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "intermediate-survey",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "intermediate-survey",
    "outputId": "da8f3d32-f387-4087-bd94-d28b2c1bfed4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4502, 22), (4502, 21))"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test data\n",
    "dataset.test.input_tensor.shape, dataset.test.target_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-print",
   "metadata": {
    "id": "great-print"
   },
   "source": [
    "#### Number of Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "affecting-toner",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "affecting-toner",
    "outputId": "5d75818e-099c-460c-e598-a0176e19a5f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 67)"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of tokens\n",
    "num_encoder_tokens = len(dataset.input_tokenizer.index_word)+1\n",
    "num_decoder_tokens = len(dataset.target_tokenizer.index_word)+1\n",
    "num_encoder_tokens, num_decoder_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-jason",
   "metadata": {
    "id": "french-jason"
   },
   "source": [
    "#### Maximum Sequence Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "regional-friday",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "regional-friday",
    "outputId": "916fb584-b8f1-4ea9-c98d-3bc1505194cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 21)"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max seq length\n",
    "max_encoder_seq_length = np.max([dataset.train.input_tensor.shape[1], dataset.val.input_tensor.shape[1], dataset.test.input_tensor.shape[1]])\n",
    "max_decoder_seq_length = np.max([dataset.train.target_tensor.shape[1], dataset.val.target_tensor.shape[1], dataset.test.target_tensor.shape[1]])\n",
    "max_encoder_seq_length, max_decoder_seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-gallery",
   "metadata": {
    "id": "incredible-gallery"
   },
   "source": [
    "#### Example batch - dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "academic-exploration",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "academic-exploration",
    "outputId": "f16577f2-d3f5-4e7f-81c9-c3666f256f1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([128, 22]), TensorShape([128, 21]))"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset.train.batch))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qQxuDnxod96f",
   "metadata": {
    "id": "qQxuDnxod96f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dried-sherman",
   "metadata": {
    "id": "dried-sherman"
   },
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "chicken-oxford",
   "metadata": {
    "id": "chicken-oxford"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, encoder_units, batch_size, layer_type = \"SimpleRNN\", dropout = 0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.encoder_units = encoder_units\n",
    "        self.layer_type = layer_type\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        ##-------- RNN layer in Encoder ------- ##\n",
    "        if self.layer_type == \"LSTM\":\n",
    "            self.layer = LSTM(self.encoder_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       dropout = dropout,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "            \n",
    "        elif self.layer_type == \"GRU\":\n",
    "            self.layer = GRU(self.encoder_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       dropout = dropout,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "           \n",
    "        else:\n",
    "            self.layer = SimpleRNN(self.encoder_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       dropout = dropout,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "            \n",
    "            \n",
    "    def call(self, inputs, hidden):\n",
    "        inputs = self.embedding(inputs)\n",
    "        if self.layer_type == \"LSTM\":\n",
    "            output, h, c = self.layer(inputs, initial_state = hidden)\n",
    "            return output, h, c\n",
    "        else:\n",
    "            output, h = self.layer(inputs, initial_state = hidden)\n",
    "            return output, h, None\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        if self.layer_type == \"LSTM\":\n",
    "            return [tf.zeros((self.batch_size, self.encoder_units)), tf.zeros((self.batch_size, self.encoder_units))]\n",
    "        else:\n",
    "            return tf.zeros((self.batch_size, self.encoder_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-hartford",
   "metadata": {
    "id": "guided-hartford"
   },
   "source": [
    "#### Test Encoder Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "atomic-archives",
   "metadata": {
    "id": "atomic-archives"
   },
   "outputs": [],
   "source": [
    "# ## Test Encoder Stack\n",
    "\n",
    "# encoder = Encoder(num_encoder_tokens, 16, 128, dataset.batch_size, \"SimpleRNN\", 3)\n",
    "\n",
    "\n",
    "# # sample input\n",
    "# sample_hidden = encoder.initialize_hidden_state()\n",
    "# sample_output, sample_h, sample_c = encoder(example_input_batch, sample_hidden)\n",
    "# print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "# print ('Encoder h vecotr shape: (batch size, units) {}'.format(sample_h.shape))\n",
    "# if encoder.layer_type == \"LSTM\":\n",
    "#     print ('Encoder c vector shape: (batch size, units) {}'.format(sample_c.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-newsletter",
   "metadata": {
    "id": "reduced-newsletter"
   },
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "involved-syria",
   "metadata": {
    "id": "involved-syria"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, decoder_units, batch_size, layer_type, attention_type='luong'):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.decoder_units = decoder_units\n",
    "        self.layer_type = layer_type\n",
    "        self.attention_type = attention_type\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Embedding Layer\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Final Dense layer on which softmax will be applied\n",
    "        self.fc = Dense(vocab_size, activation='softmax')\n",
    "        \n",
    "        # fundamental cell for decoder recurrent structure\n",
    "        if self.layer_type == \"LSTM\":\n",
    "            self.decoder_rnn_cell = LSTMCell(self.decoder_units)\n",
    "        elif self.layer_type == \"GRU\":\n",
    "            self.decoder_rnn_cell = GRUCell(self.decoder_units)\n",
    "        else:\n",
    "            self.decoder_rnn_cell = SimpleRNNCell(self.decoder_units)\n",
    "\n",
    "        # Sampler\n",
    "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "        if self.attention_type != \"none\":\n",
    "            # Create attention mechanism with memory = None\n",
    "            self.attention = self.build_attention_mechanism(self.decoder_units, \n",
    "                                                            None, self.batch_size*[max_decoder_seq_length], \n",
    "                                                            self.attention_type)\n",
    "            # Wrap attention mechanism with the fundamental rnn cell of decoder\n",
    "            self.rnn_cell = self.build_rnn_cell()\n",
    "        else:\n",
    "            # Without attention mechanism\n",
    "            self.rnn_cell = self.decoder_rnn_cell\n",
    "            \n",
    "        # Define the decoder with respect to fundamental rnn cell\n",
    "        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler=self.sampler, output_layer=self.fc)\n",
    "\n",
    "        \n",
    "    def build_rnn_cell(self):\n",
    "        rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnn_cell,\n",
    "                                               self.attention,\n",
    "                                               attention_layer_size = self.decoder_units)\n",
    "        return rnn_cell\n",
    "    \n",
    "    def build_attention_mechanism(self, decoder_units, memory, \n",
    "                                  memory_sequence_length, attention_type='luong'):\n",
    "        if(attention_type=='bahdanau'):\n",
    "            return tfa.seq2seq.BahdanauAttention(units=decoder_units, memory = memory, memory_sequence_length = memory_sequence_length)\n",
    "        else:\n",
    "            return tfa.seq2seq.LuongAttention(units=decoder_units, memory = memory, memory_sequence_length = memory_sequence_length)\n",
    "\n",
    "    def build_initial_state(self, batch_size, encoder_state, Dtype):\n",
    "        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size = batch_size, dtype = Dtype)\n",
    "        if self.attention_type != \"none\":\n",
    "            decoder_initial_state = decoder_initial_state.clone(cell_state = encoder_state)\n",
    "        return decoder_initial_state\n",
    "    \n",
    "    def call(self, inputs, initial_state):\n",
    "        inputs = self.embedding(inputs)\n",
    "        outputs, _, _ = self.decoder(inputs, initial_state = initial_state, sequence_length = self.batch_size*[max_decoder_seq_length-1])\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-holly",
   "metadata": {
    "id": "cooperative-holly"
   },
   "source": [
    "#### Test decoder stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "extended-vancouver",
   "metadata": {
    "id": "extended-vancouver"
   },
   "outputs": [],
   "source": [
    "# # Test decoder stack\n",
    "\n",
    "# decoder = Decoder(num_decoder_tokens, 16, 128, dataset.batch_size, \"LSTM\", 'none')\n",
    "# sample_x = tf.random.uniform((dataset.batch_size, max_decoder_seq_length))\n",
    "# if decoder.attention_type != \"none\":\n",
    "#     decoder.attention.setup_memory(sample_output)\n",
    "# if decoder.layer_type == \"LSTM\":\n",
    "#     initial_state = decoder.build_initial_state(dataset.batch_size, [sample_h, sample_c], tf.float32)\n",
    "# else:\n",
    "#     initial_state = decoder.build_initial_state(dataset.batch_size, sample_h, tf.float32)\n",
    "\n",
    "\n",
    "# sample_decoder_outputs = decoder(sample_x, initial_state)\n",
    "\n",
    "# print(\"Decoder Outputs Shape: \", sample_decoder_outputs.rnn_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-gentleman",
   "metadata": {
    "id": "particular-gentleman"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "duplicate-performer",
   "metadata": {
    "id": "duplicate-performer"
   },
   "outputs": [],
   "source": [
    "default_config = {\n",
    "    \"layer_type\": \"LSTM\",\n",
    "    \"units\": 1024,\n",
    "    \"embedding_dim\": 32,\n",
    "    \"optimiser\": \"nadam\",\n",
    "    \"num_encoders\": 1,\n",
    "    \"num_decoders\": 1,\n",
    "    \"epochs\": 20,\n",
    "    \"dropout\": 0.002,\n",
    "    \"batch_size\": dataset.batch_size,\n",
    "    \"attention\": \"none\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-superior",
   "metadata": {
    "id": "pleasant-superior"
   },
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "joint-richards",
   "metadata": {
    "id": "joint-richards"
   },
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    # real shape = (BATCH_SIZE, max_length_output)\n",
    "    # pred shape = (BATCH_SIZE, max_length_output, tar_vocab_size )\n",
    "    cross_entropy = SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    loss = cross_entropy(y_true=real, y_pred=pred)\n",
    "    mask = tf.logical_not(tf.math.equal(real,0))   #output 0 for y=0 else output 1\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)  \n",
    "    loss = mask* loss\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-nicholas",
   "metadata": {
    "id": "employed-nicholas"
   },
   "source": [
    "#### Accuracy Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "elegant-egypt",
   "metadata": {
    "id": "elegant-egypt"
   },
   "outputs": [],
   "source": [
    "def calc_accuracy(real, pred):\n",
    "    # real shape = (BATCH_SIZE, max_length_output)\n",
    "    # pred shape = (BATCH_SIZE, max_length_output, tar_vocab_size )\n",
    "    predictions = tf.cast(tf.argmax(pred, axis=2), tf.int32)\n",
    "    correct_preds = tf.equal(predictions, real)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_preds, tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-plant",
   "metadata": {
    "id": "dressed-plant"
   },
   "source": [
    "#### Checkpoints (Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "timely-victory",
   "metadata": {
    "id": "timely-victory"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './checkpoints'+datetime.datetime.now().replace(microsecond=0).isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "endless-ministry",
   "metadata": {
    "id": "endless-ministry"
   },
   "outputs": [],
   "source": [
    "def printf(data):\n",
    "    sys.stdout.write(\"\\r\")\n",
    "    sys.stdout.write(data)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-remark",
   "metadata": {
    "id": "warming-remark"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "micro-patrick",
   "metadata": {
    "id": "micro-patrick"
   },
   "outputs": [],
   "source": [
    "def fit(config, train_dataset, val_dataset, encoder = None, decoder = None):\n",
    "    \n",
    "    run_name = \"\".join(f\"{a}:{b} \" for (a, b) in config.items())\n",
    "    print(run_name)\n",
    "    wandb.run.name = run_name\n",
    "    \n",
    "    ## Encoder\n",
    "    if encoder == None:\n",
    "        encoder = Encoder(num_encoder_tokens, config[\"embedding_dim\"], \n",
    "                        config[\"units\"], config[\"batch_size\"], \n",
    "                        config[\"layer_type\"], config[\"dropout\"])\n",
    "\n",
    "    ## Decoder\n",
    "    if decoder == None:\n",
    "        decoder = Decoder(num_decoder_tokens, config[\"embedding_dim\"], \n",
    "                        config[\"units\"], config[\"batch_size\"], \n",
    "                        config[\"layer_type\"], config[\"attention\"])\n",
    "    \n",
    "    ## Optimizer\n",
    "    optimizer = keras.optimizers.Nadam()\n",
    "\n",
    "    # Checkpoint\n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                     encoder=encoder,\n",
    "                                     decoder=decoder)\n",
    "\n",
    "    EPOCHS = config[\"epochs\"]\n",
    "    BATCH_SIZE = config[\"batch_size\"]\n",
    "    steps_per_epoch = np.shape(dataset.train.input_tensor)[0]//dataset.batch_size\n",
    "    val_steps_per_epoch = np.shape(val_dataset.input_tensor)[0]//dataset.batch_size\n",
    "\n",
    "    # init matrics\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    val_total_loss = 0\n",
    "    val_total_accuracy = 0\n",
    "    steps_count = 0\n",
    "    val_steps_count = 0\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        start = time.time()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}')\n",
    "\n",
    "        train_dataset.batch.shuffle(BATCH_SIZE)\n",
    "        val_dataset.batch.shuffle(BATCH_SIZE)\n",
    "        \n",
    "        # Training Loop\n",
    "        for (batch, (inp, targ)) in enumerate(train_dataset.batch.take(steps_per_epoch)):        \n",
    "            batch_loss = 0\n",
    "            accuracy = 0\n",
    "            with GradientTape() as tape:\n",
    "                enc_output, enc_h, enc_c = encoder(inp, enc_hidden)\n",
    "\n",
    "                dec_input = targ[ : , :-1 ]       # Ignore '#' token\n",
    "                real = targ[ : , 1: ]             # ignore '@' token\n",
    "\n",
    "                if decoder.attention_type != \"none\":\n",
    "                    # Set the AttentionMechanism object with encoder_outputs\n",
    "                    decoder.attention.setup_memory(enc_output)\n",
    "\n",
    "                # Create AttentionWrapperState as initial_state for decoder\n",
    "                if decoder.layer_type == \"LSTM\":\n",
    "                    decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, [enc_h, enc_c], tf.float32)\n",
    "                else:\n",
    "                    decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, enc_h, tf.float32)\n",
    "                \n",
    "                pred = decoder(dec_input, decoder_initial_state)\n",
    "                logits = pred.rnn_output\n",
    "                batch_loss = loss_function(real, logits)\n",
    "\n",
    "                # Experiment for Accuracy\n",
    "                accuracy = calc_accuracy(real, logits)\n",
    "\n",
    "\n",
    "            variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "            gradients = tape.gradient(batch_loss, variables)\n",
    "            optimizer.apply_gradients([\n",
    "                (grad, var) \n",
    "                for (grad, var) in zip(gradients, variables) \n",
    "                if grad is not None\n",
    "            ])\n",
    "            # optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "            total_loss += batch_loss\n",
    "            total_accuracy += accuracy\n",
    "            steps_count += 1\n",
    "\n",
    "            printf(f\"Training: {(100 * batch / len(dataset.train.batch.take(steps_per_epoch))):.2f}% Accuracy: {(total_accuracy / steps_count):.4f} Loss: {(total_loss / steps_count):.4f}\")\n",
    "            wandb.log({\"accuracy\":(total_accuracy / steps_count), \"loss\":(total_loss / steps_count), \"epochs\": epoch})\n",
    "\n",
    "        printf(\"\\n\")\n",
    "        \n",
    "        # Validation Loop\n",
    "        for (val_batch, (inp, targ)) in enumerate(val_dataset.batch.take(steps_per_epoch)):        \n",
    "            val_batch_loss = 0\n",
    "            val_accuracy = 0\n",
    "            \n",
    "            enc_output, enc_h, enc_c = encoder(inp, enc_hidden)\n",
    "\n",
    "            dec_input = targ[ : , :-1 ]       # Ignore '#' token\n",
    "            real = targ[ : , 1: ]             # ignore '@' token\n",
    "\n",
    "            if decoder.attention_type != \"none\":\n",
    "                # Set the AttentionMechanism object with encoder_outputs\n",
    "                decoder.attention.setup_memory(enc_output)\n",
    "\n",
    "            # Create AttentionWrapperState as initial_state for decoder\n",
    "            if decoder.layer_type == \"LSTM\":\n",
    "                decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, [enc_h, enc_c], tf.float32)\n",
    "            else:\n",
    "                decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, enc_h, tf.float32)\n",
    "\n",
    "            pred = decoder(dec_input, decoder_initial_state)\n",
    "            logits = pred.rnn_output\n",
    "            \n",
    "            val_batch_loss = loss_function(real, logits)\n",
    "            val_accuracy = calc_accuracy(real, logits)\n",
    "            \n",
    "            val_total_loss += val_batch_loss\n",
    "            val_total_accuracy += val_accuracy\n",
    "            val_steps_count += 1\n",
    "            \n",
    "            printf(f\"Validating: {(100 * val_batch / len(val_dataset.batch.take(val_steps_per_epoch))):.2f}% Accuracy: {(val_total_accuracy / val_steps_count):.4f} Loss: {(val_total_loss / val_steps_count):.4f}\")\n",
    "            wandb.log({\"val accuracy\":(val_total_accuracy / val_steps_count), \"val loss\":(val_total_loss / val_steps_count), \"epochs\": epoch})\n",
    "\n",
    "        printf(\"\\n\")\n",
    "        # saving (checkpoint) the model every epochs\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"\".join(f\"{a}-{b}_\" for (a, b) in config.items()))\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "        print(f'Loss {(total_loss / (steps_per_epoch * (epoch+1))):.4f} Accuracy {(total_accuracy / (steps_per_epoch * (epoch+1))):.4f}')\n",
    "        print(f'Val Loss {(val_total_loss / (val_steps_per_epoch * (epoch+1))):.4f} Val Accuracy {(val_total_accuracy / (val_steps_per_epoch * (epoch+1))):.4f}')\n",
    "        print(f'Time taken for this epoch {(time.time() - start):.4f} sec\\n')\n",
    "        \n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-granny",
   "metadata": {
    "id": "mental-granny"
   },
   "source": [
    "#### Test Training method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "robust-hours",
   "metadata": {
    "id": "robust-hours",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# encoder, decoder = fit(default_config, dataset.train, dataset.val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-calendar",
   "metadata": {
    "id": "eastern-calendar"
   },
   "source": [
    "### BeamSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "swiss-species",
   "metadata": {
    "id": "swiss-species"
   },
   "outputs": [],
   "source": [
    "def beam_evaluate_word(word, encoder, decoder, config, beam_width=3):\n",
    "    word = dataset.preprocess_word(word)\n",
    "\n",
    "    inputs = [dataset.input_tokenizer.word_index[i] for i in word]\n",
    "    inputs = keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                          maxlen=max_encoder_seq_length,\n",
    "                                                          padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    inference_batch_size = inputs.shape[0]\n",
    "    result = ''\n",
    "\n",
    "    if encoder.layer_type == \"LSTM\":\n",
    "        enc_start_state = [tf.zeros((inference_batch_size, config[\"units\"])), tf.zeros((inference_batch_size, config[\"units\"]))]\n",
    "    else:\n",
    "        enc_start_state = tf.zeros((inference_batch_size, config[\"units\"]))\n",
    "    \n",
    "    enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\n",
    "\n",
    "    dec_h = enc_h\n",
    "    dec_c = enc_c\n",
    "\n",
    "    start_tokens = tf.fill([inference_batch_size], dataset.target_tokenizer.word_index['@'])\n",
    "    end_token = dataset.target_tokenizer.word_index['#']\n",
    "\n",
    "    enc_out = tfa.seq2seq.tile_batch(enc_out, multiplier=beam_width)\n",
    "    if decoder.attention_type != \"none\":\n",
    "        decoder.attention.setup_memory(enc_out)\n",
    "\n",
    "    # set decoder_inital_state which is an AttentionWrapperState considering beam_width\n",
    "    if decoder.layer_type == \"LSTM\":\n",
    "        hidden_state = tfa.seq2seq.tile_batch([enc_h, enc_c], multiplier=beam_width)\n",
    "    else:\n",
    "        hidden_state = tfa.seq2seq.tile_batch(enc_h, multiplier=beam_width)\n",
    "\n",
    "    decoder_initial_state = decoder.rnn_cell.get_initial_state(batch_size=beam_width*inference_batch_size, dtype=tf.float32)\n",
    "    decoder_initial_state = decoder_initial_state.clone(cell_state=hidden_state)\n",
    "\n",
    "    # Instantiate BeamSearchDecoder\n",
    "    decoder_instance = tfa.seq2seq.BeamSearchDecoder(decoder.rnn_cell,beam_width=beam_width, output_layer=decoder.fc)\n",
    "    decoder_embedding_matrix = decoder.embedding.variables[0]\n",
    "\n",
    "    # The BeamSearchDecoder object's call() function takes care of everything.\n",
    "    outputs, final_state, sequence_lengths = decoder_instance(decoder_embedding_matrix, start_tokens=start_tokens, end_token=end_token, initial_state=decoder_initial_state)\n",
    "\n",
    "    final_outputs = tf.transpose(outputs.predicted_ids, perm=(0,2,1))\n",
    "    beam_scores = tf.transpose(outputs.beam_search_decoder_output.scores, perm=(0,2,1))\n",
    "\n",
    "    return final_outputs.numpy(), beam_scores.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "laughing-interface",
   "metadata": {
    "id": "laughing-interface"
   },
   "outputs": [],
   "source": [
    "def beam_transliterate(word, encoder, decoder, config):\n",
    "  result, beam_scores = beam_evaluate_word(word, encoder, decoder, config, 5)\n",
    "  print(result.shape, beam_scores.shape)\n",
    "  for beam, score in zip(result, beam_scores):\n",
    "    print(beam.shape, score.shape)\n",
    "    output = dataset.target_tokenizer.sequences_to_texts(beam)\n",
    "    output = [a[:a.index('#')] for a in output]\n",
    "    beam_score = [a.sum() for a in score]\n",
    "    print('Input: %s' % (word))\n",
    "    for i in range(len(output)):\n",
    "      print('{} Predicted translation: {}  {}'\n",
    "            .format(i+1, \"\".join(output[i].split(\" \")).replace(\"#\", \"\"), \n",
    "                    beam_score[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "continuing-rapid",
   "metadata": {
    "id": "continuing-rapid"
   },
   "outputs": [],
   "source": [
    "# beam_transliterate('shibobrota', encoder, decoder, test_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-waste",
   "metadata": {
    "id": "built-waste"
   },
   "source": [
    "### Hyper Parameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "attended-paradise",
   "metadata": {
    "id": "attended-paradise"
   },
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"name\": \"Assignment 3 - Without Attention \" + datetime.datetime.now().replace(microsecond=0).isoformat() ,\n",
    "    \"metric\": \"categorical_accuracy\",\n",
    "    \"method\": \"random\",\n",
    "    \"project\": 'Assignment 3',\n",
    "    \"parameters\": {\n",
    "        \"layer_type\": {\n",
    "            \"values\": [\"GRU\", \"LSTM\", \"SimpleRNN\"]\n",
    "        },\n",
    "        \"dropout\": {\n",
    "            \"values\": [0.002, 0.2]\n",
    "        },\n",
    "        \"units\": {\n",
    "            \"values\": [64, 256]\n",
    "        },\n",
    "        \"embedding_dim\": {\n",
    "            \"values\": [8, 32, 128]\n",
    "        },\n",
    "        \"optimiser\": {\n",
    "            \"values\": [\"nadam\"]\n",
    "        },\n",
    "        \"epochs\": {\n",
    "            \"values\": [20]\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"values\": [dataset.batch_size]\n",
    "        },\n",
    "        \"attention\": {\n",
    "            \"values\": [\"none\"]\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "continental-prediction",
   "metadata": {
    "id": "continental-prediction"
   },
   "outputs": [],
   "source": [
    "def sweep():\n",
    "\n",
    "    config = wandb.config\n",
    "    print(str(config))\n",
    "\n",
    "    wandb.init(config=default_config, magic=True, project='Assignment 3', entity='iitm-cs6910-jan-may-2021-cs20m059-cs20m007')\n",
    "    config = wandb.config\n",
    "    \n",
    "    encoder, decoder = fit(config, dataset.train, dataset.val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-certification",
   "metadata": {
    "id": "native-certification"
   },
   "outputs": [],
   "source": [
    "# sweep_id = wandb.sweep(sweep_config, project='Assignment 3', entity='iitm-cs6910-jan-may-2021-cs20m059-cs20m007')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-shadow",
   "metadata": {
    "id": "dominican-shadow"
   },
   "outputs": [],
   "source": [
    "# wandb.agent(\"2ie0acfv\", function=sweep, project='Assignment 3', entity='iitm-cs6910-jan-may-2021-cs20m059-cs20m007')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-expert",
   "metadata": {
    "id": "small-expert"
   },
   "source": [
    "### Evaluate Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "applied-ontario",
   "metadata": {
    "id": "applied-ontario"
   },
   "outputs": [],
   "source": [
    "def evaluate_word(word, encoder, decoder, config):\n",
    "    word = dataset.preprocess_word(word)\n",
    "\n",
    "    inputs = [dataset.input_tokenizer.word_index[i] for i in word]\n",
    "    inputs = keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                          maxlen=max_encoder_seq_length,\n",
    "                                                          padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    inference_batch_size = inputs.shape[0]\n",
    "    result = ''\n",
    "\n",
    "    if encoder.layer_type == \"LSTM\":\n",
    "        enc_start_state = [tf.zeros((inference_batch_size, config[\"units\"])), tf.zeros((inference_batch_size, config[\"units\"]))]\n",
    "    else:\n",
    "        enc_start_state = tf.zeros((inference_batch_size, config[\"units\"]))\n",
    "    \n",
    "    enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\n",
    "\n",
    "    dec_h = enc_h\n",
    "    dec_c = enc_c\n",
    "\n",
    "    start_tokens = tf.fill([inference_batch_size], dataset.target_tokenizer.word_index['@'])\n",
    "    end_token = dataset.target_tokenizer.word_index['#']\n",
    "\n",
    "    greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
    "\n",
    "    # Instantiate BasicDecoder object\n",
    "    decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder.rnn_cell, sampler=greedy_sampler, output_layer=decoder.fc)\n",
    "    \n",
    "    # Setup Memory in decoder stack\n",
    "    if decoder.attention_type != \"none\":\n",
    "        decoder.attention.setup_memory(enc_out)\n",
    "\n",
    "    # set decoder_initial_state\n",
    "    if decoder.layer_type == \"LSTM\":\n",
    "        decoder_initial_state = decoder.build_initial_state(inference_batch_size, [enc_h, enc_c], tf.float32)\n",
    "    else:\n",
    "        decoder_initial_state = decoder.build_initial_state(inference_batch_size, enc_h, tf.float32)\n",
    "\n",
    "    decoder_embedding_matrix = decoder.embedding.variables[0]\n",
    "\n",
    "    outputs, _, _ = decoder_instance(decoder_embedding_matrix, start_tokens = start_tokens, end_token= end_token, initial_state = decoder_initial_state)\n",
    "    return outputs.sample_id.numpy()\n",
    "\n",
    "\n",
    "def transliterate(word, encoder, decoder, config):\n",
    "    result = evaluate_word(word, encoder, decoder, config)\n",
    "    print(result)\n",
    "    result = dataset.target_tokenizer.sequences_to_texts(result)\n",
    "    pred = \"\".join(result[0].split(\" \")).replace(\"#\", \"\")\n",
    "    print(f'Input: {word}')  \n",
    "    print(f'Predicted translation: {pred}')\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-british",
   "metadata": {
    "id": "polyphonic-british"
   },
   "outputs": [],
   "source": [
    "# transliterate(\"hello\", encoder, decoder, default_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4CyOUJSi8slY",
   "metadata": {
    "id": "4CyOUJSi8slY"
   },
   "source": [
    "# Accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "LtQpJFSB6qYB",
   "metadata": {
    "id": "LtQpJFSB6qYB"
   },
   "outputs": [],
   "source": [
    "def save_predictions(data_frame, encoder, decoder, config, name):\n",
    "    accuracy_count = 0;\n",
    "    with open(name, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"INPUT\", \"PREDICTION\", \"TRUE\", \"DISTANCE\"])\n",
    "        for i, (inp, trg) in enumerate(zip(data_frame[1], data_frame[0])): \n",
    "            pred = transliterate(inp, encoder, decoder, config)\n",
    "            distance = nltk.jaccard_distance(set(trg), set(pred))\n",
    "            writer.writerow([inp, pred, trg, distance])\n",
    "            if pred == trg:\n",
    "                accuracy_count += 1\n",
    "            if (i+1) % 10 == 0 or i+1 == data_frame.size:\n",
    "                wandb.log({\"test accuracy\":(accuracy_count / (i+1))})\n",
    "\n",
    "    return accuracy_count/data_frame.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "IGt83ai6MvrK",
   "metadata": {
    "id": "IGt83ai6MvrK"
   },
   "outputs": [],
   "source": [
    "test_config = {\n",
    "    \"layer_type\": \"LSTM\",\n",
    "    \"units\": 128,\n",
    "    \"embedding_dim\": 128,\n",
    "    \"optimiser\": \"adam\",\n",
    "    \"num_encoders\": 1,\n",
    "    \"num_decoders\": 1,\n",
    "    \"epochs\": 5,\n",
    "    \"dropout\": 0.002,\n",
    "    \"batch_size\": dataset.batch_size,\n",
    "    \"attention\": \"luong\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "eyehBJafOY8N",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyehBJafOY8N",
    "outputId": "bff08156-6fee-4bb2-8e06-d7dff630bebe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_type:LSTM units:128 embedding_dim:128 optimiser:adam num_encoders:1 num_decoders:1 epochs:5 dropout:0.002 batch_size:128 attention:luong \n",
      "Epoch 1\n",
      "Training: 9.57% Accuracy: 0.2641 Loss: 1.2072\n",
      "Validating: 97.06% Accuracy: 0.2662 Loss: 1.2055\n",
      "Loss 0.1190 Accuracy 0.0260\n",
      "Val Loss 1.2055 Val Accuracy 0.2662\n",
      "Time taken for this epoch 27.8995 sec\n",
      "\n",
      "Epoch 2\n",
      "Training: 9.57% Accuracy: 0.2651 Loss: 1.2062\n",
      "Validating: 97.06% Accuracy: 0.2664 Loss: 1.2053\n",
      "Loss 0.1189 Accuracy 0.0261\n",
      "Val Loss 1.2053 Val Accuracy 0.2664\n",
      "Time taken for this epoch 27.1254 sec\n",
      "\n",
      "Epoch 3\n",
      "Training: 9.57% Accuracy: 0.2659 Loss: 1.2056\n",
      "Validating: 97.06% Accuracy: 0.2673 Loss: 1.2044\n",
      "Loss 0.1188 Accuracy 0.0262\n",
      "Val Loss 1.2044 Val Accuracy 0.2673\n",
      "Time taken for this epoch 31.4241 sec\n",
      "\n",
      "Epoch 4\n",
      "Training: 9.57% Accuracy: 0.2666 Loss: 1.2047\n",
      "Validating: 97.06% Accuracy: 0.2679 Loss: 1.2037\n",
      "Loss 0.1187 Accuracy 0.0263\n",
      "Val Loss 1.2037 Val Accuracy 0.2679\n",
      "Time taken for this epoch 29.2467 sec\n",
      "\n",
      "Epoch 5\n",
      "Training: 9.57% Accuracy: 0.2672 Loss: 1.2041\n",
      "Validating: 97.06% Accuracy: 0.2684 Loss: 1.2032\n",
      "Loss 0.1187 Accuracy 0.0263\n",
      "Val Loss 1.2032 Val Accuracy 0.2684\n",
      "Time taken for this epoch 29.2191 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# encoder, decoder = fit(test_config, dataset.val, dataset.val, encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "wXX3I2GVXG1e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "wXX3I2GVXG1e",
    "outputId": "d8b12b9a-6232-433f-b187-ca22571840de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20  7 17  2]]\n",
      "Input: dip\n",
      "Predicted translation: दिप\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'दिप'"
      ]
     },
     "execution_count": 135,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transliterate(\"dip\", encoder, decoder, test_config)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CS6910: Assignment 3 - CS20M059 - CS20M007",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
