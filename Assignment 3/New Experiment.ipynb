{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "spare-reaction",
   "metadata": {},
   "source": [
    "#### CS20M059 Shibobrota Das | CS20M007 Abhishek Kumar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-yukon",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "floppy-clearing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using numpy: 1.19.5\n",
      "Using tensorflow: 2.4.1\n",
      "Using tensorflow Addons: 0.12.1\n",
      "Using keras: 2.4.0\n",
      "Using pandas: 1.2.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import GradientTape\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU, SimpleRNN, SimpleRNNCell, LSTMCell, GRUCell\n",
    "from keras.models import Sequential\n",
    "from keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "print(\"Using numpy:\",np.__version__)\n",
    "print(\"Using tensorflow:\",tf.__version__)\n",
    "print(\"Using tensorflow Addons:\",tfa.__version__)\n",
    "print(\"Using keras:\",keras.__version__)\n",
    "print(\"Using pandas:\",pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-onion",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "protected-parent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded to Dataframes!\n"
     ]
    }
   ],
   "source": [
    "val_df = pd.read_csv(\"./lexicons/hi.translit.sampled.dev.tsv\", sep='\\t', header=None)\n",
    "train_df = pd.read_csv(\"./lexicons/hi.translit.sampled.train.tsv\", sep='\\t', header=None)\n",
    "test_df = pd.read_csv(\"./lexicons/hi.translit.sampled.test.tsv\", sep='\\t', header=None)\n",
    "print(\"Data Loaded to Dataframes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-sunday",
   "metadata": {},
   "source": [
    "#### Dataset Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faced-grill",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2382</th>\n",
       "      <td>प्रक्रियाओं</td>\n",
       "      <td>prakriyaon</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1287</th>\n",
       "      <td>जनजीवन</td>\n",
       "      <td>janjivn</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>अनादरण</td>\n",
       "      <td>anadaran</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0           1  2\n",
       "2382  प्रक्रियाओं  prakriyaon  3\n",
       "1287       जनजीवन     janjivn  1\n",
       "81         अनादरण    anadaran  2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.sample(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-shelter",
   "metadata": {},
   "source": [
    "## Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "phantom-pharmacology",
   "metadata": {},
   "outputs": [],
   "source": [
    "sos = \"@\"\n",
    "eos = \"#\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "increased-reception",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LexDataset:\n",
    "    def __init__(self, input_tensor, target_tensor, batch_size):\n",
    "        self.input_tensor = input_tensor\n",
    "        self.target_tensor = target_tensor\n",
    "        self.batch = tf.data.Dataset.from_tensor_slices((self.input_tensor, self.target_tensor)).shuffle(len(self.input_tensor)).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "serious-argentina",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransliterationDatatset:\n",
    "    def __init__(self, df_list, problem_type = \"en-hi\", batch_size = 32):\n",
    "        self.problem_type = problem_type\n",
    "        self.input_tokenizer = None\n",
    "        self.target_tokenizer = None\n",
    "        self.train = None\n",
    "        self.val = None\n",
    "        self.test = None\n",
    "        self.batch_size = batch_size\n",
    "        # Load Data\n",
    "        self.load_dataset(df_list)\n",
    "        \n",
    "    def preprocess_word(self, w):\n",
    "        return sos + str(w) + eos\n",
    "    \n",
    "    def create_dataset(self, data_frame):\n",
    "        input_words = []\n",
    "        target_words = []\n",
    "        # Shuffle the data_frame before creating dataset\n",
    "        df_shuffled = shuffle(data_frame)\n",
    "        for x, y in zip(df_shuffled[1], df_shuffled[0]):\n",
    "            input_words.append(self.preprocess_word(x))\n",
    "            target_words.append(self.preprocess_word(y))\n",
    "        return (input_words, target_words)\n",
    "    \n",
    "    def load_dataset(self, df_list):\n",
    "        # df_list should have train -> val -> test in sequence\n",
    "        \n",
    "        self.input_tokenizer = Tokenizer(num_words = None, char_level = True)\n",
    "        self.target_tokenizer = Tokenizer(num_words = None, char_level = True)\n",
    "        \n",
    "        ds_list = []\n",
    "        \n",
    "        for df in df_list:\n",
    "            # Get the words list\n",
    "            (input_words, target_words) = self.create_dataset(df)\n",
    "            # Fit on the set of words\n",
    "            self.input_tokenizer.fit_on_texts(input_words)\n",
    "            self.target_tokenizer.fit_on_texts(target_words)\n",
    "            ds_list.append((input_words, target_words))\n",
    "                    \n",
    "        self.target_tokenizer.index_word.update({0:\" \"})\n",
    "        self.input_tokenizer.index_word.update({0:\" \"})\n",
    "        \n",
    "        for i, (input_words, target_words) in enumerate(ds_list):\n",
    "            \n",
    "            input_tensor = self.input_tokenizer.texts_to_sequences(input_words)\n",
    "            input_tensor = pad_sequences(input_tensor, padding='post')\n",
    "            \n",
    "            target_tensor = self.target_tokenizer.texts_to_sequences(target_words)\n",
    "            target_tensor = pad_sequences(target_tensor, padding='post')\n",
    "            \n",
    "            if i == 0:\n",
    "                self.train = LexDataset(input_tensor, target_tensor, self.batch_size)\n",
    "            elif i == 1:\n",
    "                self.val = LexDataset(input_tensor, target_tensor, self.batch_size)\n",
    "            else:\n",
    "                self.test = LexDataset(input_tensor, target_tensor, self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "vocational-james",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TransliterationDatatset([train_df, val_df, test_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-senator",
   "metadata": {},
   "source": [
    "#### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "congressional-master",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((44204, 22), (44204, 21))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training data\n",
    "dataset.train.input_tensor.shape, dataset.train.target_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-england",
   "metadata": {},
   "source": [
    "#### Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "written-campus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4358, 20), (4358, 16))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation data\n",
    "dataset.val.input_tensor.shape, dataset.val.target_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-involvement",
   "metadata": {},
   "source": [
    "#### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "presidential-stream",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4502, 18), (4502, 17))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test data\n",
    "dataset.test.input_tensor.shape, dataset.test.target_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-sweet",
   "metadata": {},
   "source": [
    "#### Number of Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "meaningful-developer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 67)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of tokens\n",
    "num_encoder_tokens = len(dataset.input_tokenizer.index_word)+1\n",
    "num_decoder_tokens = len(dataset.target_tokenizer.index_word)+1\n",
    "num_encoder_tokens, num_decoder_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-arrest",
   "metadata": {},
   "source": [
    "#### Maximum Sequence Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "personal-blond",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 21)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max seq length\n",
    "max_encoder_seq_length = np.max([dataset.train.input_tensor.shape[1], dataset.val.input_tensor.shape[1], dataset.test.input_tensor.shape[1]])\n",
    "max_decoder_seq_length = np.max([dataset.train.target_tensor.shape[1], dataset.val.target_tensor.shape[1], dataset.test.target_tensor.shape[1]])\n",
    "max_encoder_seq_length, max_decoder_seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-investor",
   "metadata": {},
   "source": [
    "#### Example batch - dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "premium-color",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([32, 22]), TensorShape([32, 21]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset.train.batch))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-skill",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "exact-feedback",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, encoder_units, batch_size, layer_type = \"SimpleRNN\"):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.encoder_units = encoder_units\n",
    "        self.layer_type = layer_type\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        ##-------- RNN layer in Encoder ------- ##\n",
    "        if self.layer_type == \"LSTM\":\n",
    "            self.layer = LSTM(self.encoder_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        elif self.layer_type == \"GRU\":\n",
    "            self.layer = GRU(self.encoder_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        else:\n",
    "            self.layer = SimpleRNN(self.encoder_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "\n",
    "    def call(self, inputs, hidden):\n",
    "        inputs = self.embedding(inputs)\n",
    "        if self.layer_type == \"LSTM\":\n",
    "            output, h, c = self.layer(inputs, initial_state = hidden)\n",
    "            return output, h, c\n",
    "        else:\n",
    "            output, h = self.layer(inputs, initial_state = hidden)\n",
    "            return output, h, None\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        if self.layer_type == \"LSTM\":\n",
    "            return [tf.zeros((self.batch_size, self.encoder_units)), tf.zeros((self.batch_size, self.encoder_units))]\n",
    "        else:\n",
    "            return tf.zeros((self.batch_size, self.encoder_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-childhood",
   "metadata": {},
   "source": [
    "#### Test Encoder Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "assured-fishing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Test Encoder Stack\n",
    "\n",
    "# encoder = Encoder(num_encoder_tokens, embedding_dim, units, dataset.batch_size, \"SimpleRNN\")\n",
    "\n",
    "\n",
    "# # sample input\n",
    "# sample_hidden = encoder.initialize_hidden_state()\n",
    "# sample_output, sample_h, sample_c = encoder(example_input_batch, sample_hidden)\n",
    "# print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "# print ('Encoder h vecotr shape: (batch size, units) {}'.format(sample_h.shape))\n",
    "# if encoder.layer_type == \"LSTM\":\n",
    "#     print ('Encoder c vector shape: (batch size, units) {}'.format(sample_c.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-burlington",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "suited-sunset",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, decoder_units, batch_size, layer_type, attention_type='luong'):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.decoder_units = decoder_units\n",
    "        self.layer_type = layer_type\n",
    "        self.attention_type = attention_type\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Embedding Layer\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Final Dense layer on which softmax will be applied\n",
    "        self.fc = Dense(vocab_size)\n",
    "        \n",
    "        # fundamental cell for decoder recurrent structure\n",
    "        if self.layer_type == \"LSTM\":\n",
    "            self.decoder_rnn_cell = LSTMCell(self.decoder_units)\n",
    "        elif self.layer_type == \"GRU\":\n",
    "            self.decoder_rnn_cell = GRUCell(self.decoder_units)\n",
    "        else:\n",
    "            self.decoder_rnn_cell = SimpleRNNCell(self.decoder_units)\n",
    "\n",
    "        # Sampler\n",
    "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "        # Create attention mechanism with memory = None\n",
    "        self.attention = self.build_attention_mechanism(self.decoder_units, \n",
    "                                                        None, self.batch_size*[max_decoder_seq_length], \n",
    "                                                        self.attention_type)\n",
    "\n",
    "        # Wrap attention mechanism with the fundamental rnn cell of decoder\n",
    "        self.rnn_cell = self.build_rnn_cell()\n",
    "\n",
    "        # Define the decoder with respect to fundamental rnn cell\n",
    "        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler=self.sampler, output_layer=self.fc)\n",
    "\n",
    "        \n",
    "    def build_rnn_cell(self):\n",
    "        rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnn_cell,\n",
    "                                               self.attention,\n",
    "                                               attention_layer_size = self.decoder_units)\n",
    "        return rnn_cell\n",
    "    \n",
    "    def build_attention_mechanism(self, decoder_units, memory, \n",
    "                                  memory_sequence_length, attention_type='luong'):\n",
    "        if(attention_type=='bahdanau'):\n",
    "            return tfa.seq2seq.BahdanauAttention(units=decoder_units, memory = memory, memory_sequence_length = memory_sequence_length)\n",
    "        else:\n",
    "            return tfa.seq2seq.LuongAttention(units=decoder_units, memory = memory, memory_sequence_length = memory_sequence_length)\n",
    "\n",
    "    def build_initial_state(self, batch_size, encoder_state, Dtype):\n",
    "        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size = self.batch_size, dtype = Dtype)\n",
    "        decoder_initial_state = decoder_initial_state.clone(cell_state = encoder_state)\n",
    "        return decoder_initial_state\n",
    "    \n",
    "    def call(self, inputs, initial_state):\n",
    "        inputs = self.embedding(inputs)\n",
    "        outputs, _, _ = self.decoder(inputs, initial_state = initial_state, sequence_length = self.batch_size*[max_decoder_seq_length-1])\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-basis",
   "metadata": {},
   "source": [
    "#### Test decoder stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "prepared-greenhouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test decoder stack\n",
    "\n",
    "# decoder = Decoder(num_decoder_tokens, embedding_dim, units, dataset.batch_size, \"SimpleRNN\", 'luong')\n",
    "# sample_x = tf.random.uniform((dataset.batch_size, max_decoder_seq_length))\n",
    "# decoder.attention.setup_memory(sample_output)\n",
    "# if decoder.layer_type == \"LSTM\":\n",
    "#     initial_state = decoder.build_initial_state(dataset.batch_size, [sample_h, sample_c], tf.float32)\n",
    "# else:\n",
    "#     initial_state = decoder.build_initial_state(dataset.batch_size, sample_h, tf.float32)\n",
    "\n",
    "\n",
    "# sample_decoder_outputs = decoder(sample_x, initial_state)\n",
    "\n",
    "# print(\"Decoder Outputs Shape: \", sample_decoder_outputs.rnn_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-sender",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "concerned-independence",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"layer_type\": \"LSTM\",\n",
    "    \"units\": 128,\n",
    "    \"embedding_dim\": 16,\n",
    "    \"optimiser\": \"nadam\",\n",
    "    \"num_encoders\": 1,\n",
    "    \"num_decoders\": 1,\n",
    "    \"dropout\": 0.2,\n",
    "    \"epochs\": 1,\n",
    "    \"batch_size\": dataset.batch_size,\n",
    "    \"attention\": \"luong\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-alberta",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adapted-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Nadam()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-vault",
   "metadata": {},
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "familiar-activity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    # real shape = (BATCH_SIZE, max_length_output)\n",
    "    # pred shape = (BATCH_SIZE, max_length_output, tar_vocab_size )\n",
    "    cross_entropy = SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    loss = cross_entropy(y_true=real, y_pred=pred)\n",
    "    mask = tf.logical_not(tf.math.equal(real,0))   #output 0 for y=0 else output 1\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)  \n",
    "    loss = mask* loss\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-guinea",
   "metadata": {},
   "source": [
    "#### Accuracy Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "detailed-bottom",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(real, pred):\n",
    "    # real shape = (BATCH_SIZE, max_length_output)\n",
    "    # pred shape = (BATCH_SIZE, max_length_output, tar_vocab_size )\n",
    "    predictions = tf.cast(tf.argmax(pred, axis=2), tf.int32)\n",
    "    correct_preds = tf.equal(predictions, real)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_preds, tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-ready",
   "metadata": {},
   "source": [
    "#### Checkpoints (Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "green-holder",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-durham",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-rochester",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoder\n",
    "encoder = Encoder(num_encoder_tokens, config[\"embedding_dim\"], \n",
    "                  config[\"units\"], config[\"batch_size\"], \n",
    "                  config[\"layer_type\"])\n",
    "\n",
    "## Decoder\n",
    "decoder = Decoder(num_decoder_tokens, config[\"embedding_dim\"], \n",
    "                  config[\"units\"], config[\"batch_size\"], \n",
    "                  config[\"layer_type\"], config[\"attention\"])\n",
    "\n",
    "# Checkpoint\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n",
    "\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = config[\"batch_size\"]\n",
    "steps_per_epoch = int(np.shape(dataset.train.input_tensor)[0]//dataset.batch_size)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}')\n",
    "    \n",
    "    dataset.train.batch.shuffle(BATCH_SIZE)\n",
    "    \n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.train.batch.take(steps_per_epoch)):        \n",
    "        batch_loss = 0\n",
    "        accuracy = 0\n",
    "        with GradientTape() as tape:\n",
    "            enc_output, enc_h, enc_c = encoder(inp, enc_hidden)\n",
    "\n",
    "\n",
    "            dec_input = targ[ : , :-1 ]       # Ignore '#' token\n",
    "            real = targ[ : , 1: ]             # ignore '@' token\n",
    "\n",
    "            # Set the AttentionMechanism object with encoder_outputs\n",
    "            decoder.attention.setup_memory(enc_output)\n",
    "\n",
    "            # Create AttentionWrapperState as initial_state for decoder\n",
    "            if decoder.layer_type == \"LSTM\":\n",
    "                decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, [enc_h, enc_c], tf.float32)\n",
    "            else:\n",
    "                decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, enc_h, tf.float32)\n",
    "            pred = decoder(dec_input, decoder_initial_state)\n",
    "            logits = pred.rnn_output\n",
    "            batch_loss = loss_function(real, logits)\n",
    "            \n",
    "            # Experiment for Accuracy\n",
    "            accuracy = calc_accuracy(real, logits)\n",
    "            \n",
    "\n",
    "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "        gradients = tape.gradient(batch_loss, variables)\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        total_accuracy += accuracy\n",
    "        \n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(f\"\\rProcessing: {(100 * batch / len(dataset.train.batch.take(steps_per_epoch))):.2f}% Accuracy: {(total_accuracy / batch):.4f} Loss: {(total_loss / batch):.4f}\")\n",
    "            \n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print(f'Loss {(total_loss / steps_per_epoch):.4f} Accuracy {(total_accuracy / steps_per_epoch):.4f}')\n",
    "    print(f'Time taken for 1 epoch {time.time() - start} sec\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "clinical-significance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'attention-luong__batch_size-32__dropout-0.2__embedding_dim-16__epochs-1__layer_type-LSTM__num_decoders-1__num_encoders-1__optimiser-nadam__units-128__'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(f'{key}-{val}__' for key, val in sorted(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-blink",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-quest",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
