{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "organized-ebony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using numpy: 1.19.5\n",
      "Using tensorflow: 2.4.1\n",
      "Using tensorflow Addons: 0.12.1\n",
      "Using keras: 2.4.0\n",
      "Using pandas: 1.2.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU, SimpleRNN, SimpleRNNCell, LSTMCell, GRUCell\n",
    "from keras.models import Sequential\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(\"Using numpy:\",np.__version__)\n",
    "print(\"Using tensorflow:\",tf.__version__)\n",
    "print(\"Using tensorflow Addons:\",tfa.__version__)\n",
    "print(\"Using keras:\",keras.__version__)\n",
    "print(\"Using pandas:\",pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aging-pizza",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded to Dataframes!\n"
     ]
    }
   ],
   "source": [
    "val_df = pd.read_csv(\"./lexicons/hi.translit.sampled.dev.tsv\", sep='\\t', header=None)\n",
    "train_df = pd.read_csv(\"./lexicons/hi.translit.sampled.train.tsv\", sep='\\t', header=None)\n",
    "test_df = pd.read_csv(\"./lexicons/hi.translit.sampled.test.tsv\", sep='\\t', header=None)\n",
    "print(\"Data Loaded to Dataframes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "political-pleasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LexDataset:\n",
    "    def __init__(self, input_tensor, target_tensor, inp_word_tokenizer, targ_word_tokenizer):\n",
    "        self.input_tensor = input_tensor\n",
    "        self.target_tensor = target_tensor\n",
    "        self.inp_word_tokenizer = inp_word_tokenizer\n",
    "        self.targ_word_tokenizer = targ_word_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "demonstrated-balloon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(words, tokenizer):\n",
    "    tensor = tokenizer.texts_to_sequences(words)\n",
    "    \n",
    "    #Pad the smaller words\n",
    "    tensor = pad_sequences(tensor, padding='post')\n",
    "    \n",
    "    # Return the tensor and the tokenizer\n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "prime-expert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the dataframe to \n",
    "def create_dataset(data_frame):\n",
    "    input_words = []\n",
    "    target_words = []\n",
    "    for x, y in zip(data_frame[1], data_frame[0]):\n",
    "        # Add words to respective lists\n",
    "        input_words.append(\"@\"+str(x)+\"#\")\n",
    "        target_words.append(\"@\"+str(y)+\"#\")\n",
    "    return input_words, target_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "italian-blake",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(data_frame_list):\n",
    "    # Initialize the tokenizer\n",
    "    input_tokenizer = Tokenizer(num_words = None, char_level = True)\n",
    "    target_tokenizer = Tokenizer(num_words = None, char_level = True)\n",
    "    \n",
    "    dataset_list = []\n",
    "    \n",
    "    for df in data_frame_list:\n",
    "        # Get the words list\n",
    "        input_words, target_words = create_dataset(df)\n",
    "        # Fit on the set of words\n",
    "        input_tokenizer.fit_on_texts(input_words)\n",
    "        target_tokenizer.fit_on_texts(target_words)\n",
    "        dataset_list.append((input_words, target_words))\n",
    "    \n",
    "    words_data = []\n",
    "    \n",
    "    target_tokenizer.index_word.update({0:\" \"})\n",
    "    input_tokenizer.index_word.update({0:\" \"})\n",
    "    \n",
    "    for (input_words, target_words) in dataset_list:\n",
    "        # Tokenize the words\n",
    "        input_tensor, inp_word_tokenizer = tokenize(input_words, input_tokenizer)\n",
    "        target_tensor, targ_word_tokenizer = tokenize(target_words, target_tokenizer)\n",
    "        words_data.append(LexDataset(input_tensor, target_tensor, inp_word_tokenizer, targ_word_tokenizer))\n",
    "\n",
    "    return words_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "perceived-montreal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Val input tensor: (4358, 20) | Shape of Val target tensor: (4358, 16)\n",
      "Shape of Train input tensor: (44204, 22) | Shape of Train target tensor: (44204, 21)\n",
      "Shape of Test input tensor: (4502, 18) | Shape of Test target tensor: (4502, 17)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset([val_df, train_df, test_df])\n",
    "\n",
    "print(f'Shape of Val input tensor: {np.shape(dataset[0].input_tensor)} | Shape of Val target tensor: {np.shape(dataset[0].target_tensor)}')\n",
    "print(f'Shape of Train input tensor: {np.shape(dataset[1].input_tensor)} | Shape of Train target tensor: {np.shape(dataset[1].target_tensor)}')\n",
    "print(f'Shape of Test input tensor: {np.shape(dataset[2].input_tensor)} | Shape of Test target tensor: {np.shape(dataset[2].target_tensor)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "continental-raleigh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(tk, tensor):\n",
    "    for t in tensor:\n",
    "        if t != 0:\n",
    "            print(f'{t} ----> {tk.index_word[t]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cathedral-glossary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Input Word; index to character mapping\n",
      "2 ----> @\n",
      "1 ----> a\n",
      "4 ----> n\n",
      "13 ----> k\n",
      "1 ----> a\n",
      "4 ----> n\n",
      "3 ----> #\n",
      "\n",
      "Val Target Word; index to character mapping\n",
      "1 ----> @\n",
      "31 ----> अ\n",
      "10 ----> ं\n",
      "8 ----> क\n",
      "6 ----> न\n",
      "2 ----> #\n"
     ]
    }
   ],
   "source": [
    "print(\"Val Input Word; index to character mapping\")\n",
    "convert(dataset[0].inp_word_tokenizer, dataset[0].input_tensor[0])\n",
    "print()\n",
    "print(\"Val Target Word; index to character mapping\")\n",
    "convert(dataset[0].targ_word_tokenizer, dataset[0].target_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "entertaining-vision",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 67)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_encoder_tokens = len(dataset[0].inp_word_tokenizer.index_word)+1\n",
    "num_decoder_tokens = len(dataset[0].targ_word_tokenizer.index_word)+1\n",
    "num_encoder_tokens, num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "private-acquisition",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_seq_length = max([np.shape(dataset[i].input_tensor)[1] for i in range(len(dataset))])\n",
    "max_decoder_seq_length = max([np.shape(dataset[i].target_tensor)[1] for i in range(len(dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "educated-resolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[0].targ_word_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-oklahoma",
   "metadata": {},
   "source": [
    "## Tensorflow Dataset from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "loved-africa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1381"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "embedding_dim = 16\n",
    "units = 128\n",
    "steps_per_epoch = np.shape(dataset[1].input_tensor)[0]//BATCH_SIZE\n",
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subjective-difficulty",
   "metadata": {},
   "source": [
    "#### Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "central-devil",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((dataset[1].input_tensor, dataset[1].target_tensor)).shuffle(len(dataset[1].input_tensor))\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-sterling",
   "metadata": {},
   "source": [
    "#### Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "quick-branch",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = tf.data.Dataset.from_tensor_slices((dataset[0].input_tensor, dataset[0].target_tensor)).shuffle(len(dataset[0].input_tensor))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-computer",
   "metadata": {},
   "source": [
    "#### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "double-organizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices((dataset[2].input_tensor, dataset[2].target_tensor)).shuffle(len(dataset[2].input_tensor))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "architectural-summary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([32, 22]), TensorShape([32, 21]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(train_dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "christian-venezuela",
   "metadata": {
    "id": "nZ2rI24i3jFg"
   },
   "outputs": [],
   "source": [
    "##### \n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    ##-------- LSTM layer in Encoder ------- ##\n",
    "    self.lstm_layer = tf.keras.layers.LSTM(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    \n",
    "\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, h, c = self.lstm_layer(x, initial_state = hidden)\n",
    "    return output, h, c\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return [tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "weird-collapse",
   "metadata": {
    "id": "60gSVh05Jl6l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (32, 22, 128)\n",
      "Encoder h vecotr shape: (batch size, units) (32, 128)\n",
      "Encoder c vector shape: (batch size, units) (32, 128)\n"
     ]
    }
   ],
   "source": [
    "## Test Encoder Stack\n",
    "\n",
    "encoder = Encoder(num_encoder_tokens, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_h, sample_c = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder h vecotr shape: (batch size, units) {}'.format(sample_h.shape))\n",
    "print ('Encoder c vector shape: (batch size, units) {}'.format(sample_c.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "loving-attention",
   "metadata": {
    "id": "yJ_B3mhW3jFk"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, attention_type='luong'):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.attention_type = attention_type\n",
    "    \n",
    "    # Embedding Layer\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    \n",
    "    #Final Dense layer on which softmax will be applied\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # Define the fundamental cell for decoder recurrent structure\n",
    "    self.decoder_rnn_cell = tf.keras.layers.LSTMCell(self.dec_units)\n",
    "   \n",
    "\n",
    "\n",
    "    # Sampler\n",
    "    self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "    # Create attention mechanism with memory = None\n",
    "    self.attention_mechanism = self.build_attention_mechanism(self.dec_units, \n",
    "                                                              None, self.batch_sz*[max_decoder_seq_length], self.attention_type)\n",
    "\n",
    "    # Wrap attention mechanism with the fundamental rnn cell of decoder\n",
    "    self.rnn_cell = self.build_rnn_cell(batch_sz)\n",
    "\n",
    "    # Define the decoder with respect to fundamental rnn cell\n",
    "    self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler=self.sampler, output_layer=self.fc)\n",
    "\n",
    "    \n",
    "  def build_rnn_cell(self, batch_sz):\n",
    "    rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnn_cell, \n",
    "                                  self.attention_mechanism, attention_layer_size=self.dec_units)\n",
    "    return rnn_cell\n",
    "\n",
    "  def build_attention_mechanism(self, dec_units, memory, memory_sequence_length, attention_type='luong'):\n",
    "    # ------------- #\n",
    "    # typ: Which sort of attention (Bahdanau, Luong)\n",
    "    # dec_units: final dimension of attention outputs \n",
    "    # memory: encoder hidden states of shape (batch_size, max_length_input, enc_units)\n",
    "    # memory_sequence_length: 1d array of shape (batch_size) with every element set to max_length_input (for masking purpose)\n",
    "\n",
    "    if(attention_type=='bahdanau'):\n",
    "      return tfa.seq2seq.BahdanauAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
    "    else:\n",
    "      return tfa.seq2seq.LuongAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
    "\n",
    "  def build_initial_state(self, batch_sz, encoder_state, Dtype):\n",
    "    decoder_initial_state = self.rnn_cell.get_initial_state(batch_size=batch_sz, dtype=Dtype)\n",
    "    decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n",
    "    return decoder_initial_state\n",
    "\n",
    "\n",
    "  def call(self, inputs, initial_state):\n",
    "    x = self.embedding(inputs)\n",
    "    outputs, _, _ = self.decoder(x, initial_state=initial_state, sequence_length=self.batch_sz*[max_decoder_seq_length-1])\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "positive-unknown",
   "metadata": {
    "id": "DaiO0Z6_Ml1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder Outputs Shape:  (32, 20, 67)\n"
     ]
    }
   ],
   "source": [
    "# Test decoder stack\n",
    "\n",
    "decoder = Decoder(num_decoder_tokens, embedding_dim, units, BATCH_SIZE, 'luong')\n",
    "sample_x = tf.random.uniform((BATCH_SIZE, max_decoder_seq_length))\n",
    "decoder.attention_mechanism.setup_memory(sample_output)\n",
    "initial_state = decoder.build_initial_state(BATCH_SIZE, [sample_h, sample_c], tf.float32)\n",
    "\n",
    "\n",
    "sample_decoder_outputs = decoder(sample_x, initial_state)\n",
    "\n",
    "print(\"Decoder Outputs Shape: \", sample_decoder_outputs.rnn_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-tourism",
   "metadata": {
    "id": "_ch_71VbIRfK"
   },
   "source": [
    "## Define the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "laden-provider",
   "metadata": {
    "id": "WmTHr5iV3jFr"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  # real shape = (BATCH_SIZE, max_length_output)\n",
    "  # pred shape = (BATCH_SIZE, max_length_output, tar_vocab_size )\n",
    "  cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "  loss = cross_entropy(y_true=real, y_pred=pred)\n",
    "  mask = tf.logical_not(tf.math.equal(real,0))   #output 0 for y=0 else output 1\n",
    "  mask = tf.cast(mask, dtype=loss.dtype)  \n",
    "  loss = mask* loss\n",
    "  loss = tf.reduce_mean(loss)\n",
    "  return loss  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-liberal",
   "metadata": {
    "id": "DMVWzzsfNl4e"
   },
   "source": [
    "## Checkpoints (Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "suspected-specific",
   "metadata": {
    "id": "Zj8bXQTgNwrF"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-character",
   "metadata": {
    "id": "8Bw95utNiFHa"
   },
   "source": [
    "## One train_step operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "reasonable-second",
   "metadata": {
    "id": "sC9ArXSsVfqn"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_h, enc_c = encoder(inp, enc_hidden)\n",
    "\n",
    "\n",
    "    dec_input = targ[ : , :-1 ] # Ignore <end> token\n",
    "    real = targ[ : , 1: ]         # ignore <start> token\n",
    "\n",
    "    # Set the AttentionMechanism object with encoder_outputs\n",
    "    decoder.attention_mechanism.setup_memory(enc_output)\n",
    "\n",
    "    # Create AttentionWrapperState as initial_state for decoder\n",
    "    decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, [enc_h, enc_c], tf.float32)\n",
    "    pred = decoder(dec_input, decoder_initial_state)\n",
    "    logits = pred.rnn_output\n",
    "    loss = loss_function(real, logits)\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-panel",
   "metadata": {
    "id": "pey8eb9piMMg"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "corrected-polls",
   "metadata": {
    "id": "ddefjBMa3jF0",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.2976\n",
      "Epoch 1 Batch 100 Loss 0.3363\n",
      "Epoch 1 Batch 200 Loss 0.2510\n",
      "Epoch 1 Batch 300 Loss 0.1651\n",
      "Epoch 1 Batch 400 Loss 0.2381\n",
      "Epoch 1 Batch 500 Loss 0.3404\n",
      "Epoch 1 Batch 600 Loss 0.3438\n",
      "Epoch 1 Batch 700 Loss 0.3347\n",
      "Epoch 1 Batch 800 Loss 0.1841\n",
      "Epoch 1 Batch 900 Loss 0.2376\n",
      "Epoch 1 Batch 1000 Loss 0.2337\n",
      "Epoch 1 Batch 1100 Loss 0.1909\n",
      "Epoch 1 Batch 1200 Loss 0.2058\n",
      "Epoch 1 Batch 1300 Loss 0.2162\n",
      "Epoch 1 Loss 0.2544\n",
      "Time taken for 1 epoch 161.25463128089905 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.2556\n",
      "Epoch 2 Batch 100 Loss 0.2288\n",
      "Epoch 2 Batch 200 Loss 0.1630\n",
      "Epoch 2 Batch 300 Loss 0.2067\n",
      "Epoch 2 Batch 400 Loss 0.1683\n",
      "Epoch 2 Batch 500 Loss 0.1762\n",
      "Epoch 2 Batch 600 Loss 0.2006\n",
      "Epoch 2 Batch 700 Loss 0.2299\n",
      "Epoch 2 Batch 800 Loss 0.1873\n",
      "Epoch 2 Batch 900 Loss 0.2371\n",
      "Epoch 2 Batch 1000 Loss 0.1944\n",
      "Epoch 2 Batch 1100 Loss 0.1934\n",
      "Epoch 2 Batch 1200 Loss 0.1628\n",
      "Epoch 2 Batch 1300 Loss 0.1325\n",
      "Epoch 2 Loss 0.2082\n",
      "Time taken for 1 epoch 161.5080006122589 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.2204\n",
      "Epoch 3 Batch 100 Loss 0.1798\n",
      "Epoch 3 Batch 200 Loss 0.1657\n",
      "Epoch 3 Batch 300 Loss 0.2169\n",
      "Epoch 3 Batch 400 Loss 0.1247\n",
      "Epoch 3 Batch 500 Loss 0.1496\n",
      "Epoch 3 Batch 600 Loss 0.2167\n",
      "Epoch 3 Batch 700 Loss 0.2142\n",
      "Epoch 3 Batch 800 Loss 0.1404\n",
      "Epoch 3 Batch 900 Loss 0.1663\n",
      "Epoch 3 Batch 1000 Loss 0.1441\n",
      "Epoch 3 Batch 1100 Loss 0.2281\n",
      "Epoch 3 Batch 1200 Loss 0.1877\n",
      "Epoch 3 Batch 1300 Loss 0.1940\n",
      "Epoch 3 Loss 0.1833\n",
      "Time taken for 1 epoch 162.43969106674194 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.1426\n",
      "Epoch 4 Batch 100 Loss 0.1894\n",
      "Epoch 4 Batch 200 Loss 0.1546\n",
      "Epoch 4 Batch 300 Loss 0.1749\n",
      "Epoch 4 Batch 400 Loss 0.1536\n",
      "Epoch 4 Batch 500 Loss 0.1926\n",
      "Epoch 4 Batch 600 Loss 0.1429\n",
      "Epoch 4 Batch 700 Loss 0.2154\n",
      "Epoch 4 Batch 800 Loss 0.2223\n",
      "Epoch 4 Batch 900 Loss 0.1687\n",
      "Epoch 4 Batch 1000 Loss 0.1763\n",
      "Epoch 4 Batch 1100 Loss 0.1670\n",
      "Epoch 4 Batch 1200 Loss 0.1421\n",
      "Epoch 4 Batch 1300 Loss 0.1384\n",
      "Epoch 4 Loss 0.1658\n",
      "Time taken for 1 epoch 163.73834705352783 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.1433\n",
      "Epoch 5 Batch 100 Loss 0.1496\n",
      "Epoch 5 Batch 200 Loss 0.1722\n",
      "Epoch 5 Batch 300 Loss 0.1488\n",
      "Epoch 5 Batch 400 Loss 0.1521\n",
      "Epoch 5 Batch 500 Loss 0.1388\n",
      "Epoch 5 Batch 600 Loss 0.1565\n",
      "Epoch 5 Batch 700 Loss 0.1322\n",
      "Epoch 5 Batch 800 Loss 0.1524\n",
      "Epoch 5 Batch 900 Loss 0.1295\n",
      "Epoch 5 Batch 1000 Loss 0.2191\n",
      "Epoch 5 Batch 1100 Loss 0.1249\n",
      "Epoch 5 Batch 1200 Loss 0.1231\n",
      "Epoch 5 Batch 1300 Loss 0.1286\n",
      "Epoch 5 Loss 0.1522\n",
      "Time taken for 1 epoch 161.05319166183472 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.1010\n",
      "Epoch 6 Batch 100 Loss 0.1042\n",
      "Epoch 6 Batch 200 Loss 0.1228\n",
      "Epoch 6 Batch 300 Loss 0.1477\n",
      "Epoch 6 Batch 400 Loss 0.1396\n",
      "Epoch 6 Batch 500 Loss 0.1296\n",
      "Epoch 6 Batch 600 Loss 0.1319\n",
      "Epoch 6 Batch 700 Loss 0.1566\n",
      "Epoch 6 Batch 800 Loss 0.1120\n",
      "Epoch 6 Batch 900 Loss 0.1319\n",
      "Epoch 6 Batch 1000 Loss 0.1248\n",
      "Epoch 6 Batch 1100 Loss 0.1114\n",
      "Epoch 6 Batch 1200 Loss 0.1247\n",
      "Epoch 6 Batch 1300 Loss 0.1875\n",
      "Epoch 6 Loss 0.1410\n",
      "Time taken for 1 epoch 160.8000030517578 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.0952\n",
      "Epoch 7 Batch 100 Loss 0.1517\n",
      "Epoch 7 Batch 200 Loss 0.1236\n",
      "Epoch 7 Batch 300 Loss 0.1270\n",
      "Epoch 7 Batch 400 Loss 0.1699\n",
      "Epoch 7 Batch 500 Loss 0.1668\n",
      "Epoch 7 Batch 600 Loss 0.1502\n",
      "Epoch 7 Batch 700 Loss 0.1665\n",
      "Epoch 7 Batch 800 Loss 0.1257\n",
      "Epoch 7 Batch 900 Loss 0.1642\n",
      "Epoch 7 Batch 1000 Loss 0.1418\n",
      "Epoch 7 Batch 1100 Loss 0.1241\n",
      "Epoch 7 Batch 1200 Loss 0.1236\n",
      "Epoch 7 Batch 1300 Loss 0.1275\n",
      "Epoch 7 Loss 0.1325\n",
      "Time taken for 1 epoch 159.72399830818176 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.1042\n",
      "Epoch 8 Batch 100 Loss 0.1377\n",
      "Epoch 8 Batch 200 Loss 0.0960\n",
      "Epoch 8 Batch 300 Loss 0.1449\n",
      "Epoch 8 Batch 400 Loss 0.1337\n",
      "Epoch 8 Batch 500 Loss 0.1408\n",
      "Epoch 8 Batch 600 Loss 0.1113\n",
      "Epoch 8 Batch 700 Loss 0.1729\n",
      "Epoch 8 Batch 800 Loss 0.1156\n",
      "Epoch 8 Batch 900 Loss 0.0965\n",
      "Epoch 8 Batch 1000 Loss 0.1298\n",
      "Epoch 8 Batch 1100 Loss 0.1083\n",
      "Epoch 8 Batch 1200 Loss 0.1206\n",
      "Epoch 8 Batch 1300 Loss 0.0954\n",
      "Epoch 8 Loss 0.1243\n",
      "Time taken for 1 epoch 144.53257250785828 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.1093\n",
      "Epoch 9 Batch 100 Loss 0.1471\n",
      "Epoch 9 Batch 200 Loss 0.1138\n",
      "Epoch 9 Batch 300 Loss 0.1254\n",
      "Epoch 9 Batch 400 Loss 0.1039\n",
      "Epoch 9 Batch 500 Loss 0.1499\n",
      "Epoch 9 Batch 600 Loss 0.0777\n",
      "Epoch 9 Batch 700 Loss 0.1206\n",
      "Epoch 9 Batch 800 Loss 0.0822\n",
      "Epoch 9 Batch 900 Loss 0.1102\n",
      "Epoch 9 Batch 1000 Loss 0.0913\n",
      "Epoch 9 Batch 1100 Loss 0.1473\n",
      "Epoch 9 Batch 1200 Loss 0.1089\n",
      "Epoch 9 Batch 1300 Loss 0.0664\n",
      "Epoch 9 Loss 0.1178\n",
      "Time taken for 1 epoch 118.97199892997742 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.2145\n",
      "Epoch 10 Batch 100 Loss 0.1026\n",
      "Epoch 10 Batch 200 Loss 0.1364\n",
      "Epoch 10 Batch 300 Loss 0.1471\n",
      "Epoch 10 Batch 400 Loss 0.1193\n",
      "Epoch 10 Batch 500 Loss 0.1202\n",
      "Epoch 10 Batch 600 Loss 0.1136\n",
      "Epoch 10 Batch 700 Loss 0.0974\n",
      "Epoch 10 Batch 800 Loss 0.0642\n",
      "Epoch 10 Batch 900 Loss 0.1170\n",
      "Epoch 10 Batch 1000 Loss 0.1131\n",
      "Epoch 10 Batch 1100 Loss 0.0975\n",
      "Epoch 10 Batch 1200 Loss 0.1055\n",
      "Epoch 10 Batch 1300 Loss 0.1015\n",
      "Epoch 10 Loss 0.1118\n",
      "Time taken for 1 epoch 115.6496651172638 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    # print(enc_hidden[0].shape, enc_hidden[1].shape)\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch/100,\n",
    "                                                   batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "engaging-kansas",
   "metadata": {
    "id": "EbQpyYs13jF_"
   },
   "outputs": [],
   "source": [
    "def evaluate_sentence(sentence):\n",
    "    sentence = \"@\"+sentence+\"#\"\n",
    "\n",
    "    inputs = [dataset[0].inp_word_tokenizer.word_index[i] for i in sentence]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                          maxlen=max_encoder_seq_length,\n",
    "                                                          padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    inference_batch_size = inputs.shape[0]\n",
    "    result = ''\n",
    "\n",
    "    enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size,units))]\n",
    "    enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\n",
    "\n",
    "    dec_h = enc_h\n",
    "    dec_c = enc_c\n",
    "\n",
    "    start_tokens = tf.fill([inference_batch_size], dataset[0].targ_word_tokenizer.word_index['@'])\n",
    "    end_token = dataset[0].targ_word_tokenizer.word_index['#']\n",
    "\n",
    "    greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
    "\n",
    "    # Instantiate BasicDecoder object\n",
    "    decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder.rnn_cell, sampler=greedy_sampler, output_layer=decoder.fc)\n",
    "    # Setup Memory in decoder stack\n",
    "    decoder.attention_mechanism.setup_memory(enc_out)\n",
    "\n",
    "    # set decoder_initial_state\n",
    "    decoder_initial_state = decoder.build_initial_state(inference_batch_size, [enc_h, enc_c], tf.float32)\n",
    "\n",
    "\n",
    "    ### Since the BasicDecoder wraps around Decoder's rnn cell only, you have to ensure that the inputs to BasicDecoder \n",
    "    ### decoding step is output of embedding layer. tfa.seq2seq.GreedyEmbeddingSampler() takes care of this. \n",
    "    ### You only need to get the weights of embedding layer, which can be done by decoder.embedding.variables[0] and pass this callabble to BasicDecoder's call() function\n",
    "\n",
    "    decoder_embedding_matrix = decoder.embedding.variables[0]\n",
    "\n",
    "    outputs, _, _ = decoder_instance(decoder_embedding_matrix, start_tokens = start_tokens, end_token= end_token, initial_state=decoder_initial_state)\n",
    "    return outputs.sample_id.numpy()\n",
    "\n",
    "def translate(sentence):\n",
    "    result = evaluate_sentence(sentence)\n",
    "    print(result)\n",
    "    result = dataset[1].targ_word_tokenizer.sequences_to_texts(result)\n",
    "    print(f'Input: {sentence}')  \n",
    "    print(f'Predicted translation: {\"\".join(result[0].split(\" \")).replace(\"#\", \"\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "persistent-transaction",
   "metadata": {
    "id": "UJpT9D5_OgP6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x13c86eec850>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "centered-clock",
   "metadata": {
    "id": "WYmYhNN_faR5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[40 25 14  2]]\n",
      "Input: abe\n",
      "Predicted translation: आबे\n"
     ]
    }
   ],
   "source": [
    "translate(u'abe')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-disney",
   "metadata": {
    "id": "IRUuNDeY0HiC"
   },
   "source": [
    "## Use tf-addons BeamSearchDecoder \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "happy-slovenia",
   "metadata": {
    "id": "AJ-RTQ0hsJNL"
   },
   "outputs": [],
   "source": [
    "def beam_evaluate_sentence(sentence, beam_width=3):\n",
    "    sentence = \"@\"+sentence+\"#\"\n",
    "\n",
    "    inputs = [dataset[0].inp_word_tokenizer.word_index[i] for i in sentence]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                          maxlen=max_encoder_seq_length,\n",
    "                                                          padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    inference_batch_size = inputs.shape[0]\n",
    "    result = ''\n",
    "\n",
    "    enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size,units))]\n",
    "    enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\n",
    "\n",
    "    dec_h = enc_h\n",
    "    dec_c = enc_c\n",
    "\n",
    "    start_tokens = tf.fill([inference_batch_size], dataset[0].targ_word_tokenizer.word_index['@'])\n",
    "    end_token = dataset[0].targ_word_tokenizer.word_index['#']\n",
    "\n",
    "    # From official documentation\n",
    "    # NOTE If you are using the BeamSearchDecoder with a cell wrapped in AttentionWrapper, then you must ensure that:\n",
    "    # The encoder output has been tiled to beam_width via tfa.seq2seq.tile_batch (NOT tf.tile).\n",
    "    # The batch_size argument passed to the get_initial_state method of this wrapper is equal to true_batch_size * beam_width.\n",
    "    # The initial state created with get_initial_state above contains a cell_state value containing properly tiled final state from the encoder.\n",
    "\n",
    "    enc_out = tfa.seq2seq.tile_batch(enc_out, multiplier=beam_width)\n",
    "    decoder.attention_mechanism.setup_memory(enc_out)\n",
    "    print(\"beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 16, 1024]] :\", enc_out.shape)\n",
    "\n",
    "    # set decoder_inital_state which is an AttentionWrapperState considering beam_width\n",
    "    hidden_state = tfa.seq2seq.tile_batch([enc_h, enc_c], multiplier=beam_width)\n",
    "    decoder_initial_state = decoder.rnn_cell.get_initial_state(batch_size=beam_width*inference_batch_size, dtype=tf.float32)\n",
    "    decoder_initial_state = decoder_initial_state.clone(cell_state=hidden_state)\n",
    "\n",
    "    # Instantiate BeamSearchDecoder\n",
    "    decoder_instance = tfa.seq2seq.BeamSearchDecoder(decoder.rnn_cell,beam_width=beam_width, output_layer=decoder.fc)\n",
    "    decoder_embedding_matrix = decoder.embedding.variables[0]\n",
    "\n",
    "    # The BeamSearchDecoder object's call() function takes care of everything.\n",
    "    outputs, final_state, sequence_lengths = decoder_instance(decoder_embedding_matrix, start_tokens=start_tokens, end_token=end_token, initial_state=decoder_initial_state)\n",
    "    # outputs is tfa.seq2seq.FinalBeamSearchDecoderOutput object. \n",
    "    # The final beam predictions are stored in outputs.predicted_id\n",
    "    # outputs.beam_search_decoder_output is a tfa.seq2seq.BeamSearchDecoderOutput object which keep tracks of beam_scores and parent_ids while performing a beam decoding step\n",
    "    # final_state = tfa.seq2seq.BeamSearchDecoderState object.\n",
    "    # Sequence Length = [inference_batch_size, beam_width] details the maximum length of the beams that are generated\n",
    "\n",
    "\n",
    "    # outputs.predicted_id.shape = (inference_batch_size, time_step_outputs, beam_width)\n",
    "    # outputs.beam_search_decoder_output.scores.shape = (inference_batch_size, time_step_outputs, beam_width)\n",
    "    # Convert the shape of outputs and beam_scores to (inference_batch_size, beam_width, time_step_outputs)\n",
    "    final_outputs = tf.transpose(outputs.predicted_ids, perm=(0,2,1))\n",
    "    beam_scores = tf.transpose(outputs.beam_search_decoder_output.scores, perm=(0,2,1))\n",
    "\n",
    "    return final_outputs.numpy(), beam_scores.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "british-passport",
   "metadata": {
    "id": "g_LvXGvX8X-O"
   },
   "outputs": [],
   "source": [
    "def beam_translate(sentence):\n",
    "  result, beam_scores = beam_evaluate_sentence(sentence)\n",
    "  print(result.shape, beam_scores.shape)\n",
    "  for beam, score in zip(result, beam_scores):\n",
    "    print(beam.shape, score.shape)\n",
    "    output = dataset[0].targ_word_tokenizer.sequences_to_texts(beam)\n",
    "    output = [a[:a.index('#')] for a in output]\n",
    "    beam_score = [a.sum() for a in score]\n",
    "    print('Input: %s' % (sentence))\n",
    "    for i in range(len(output)):\n",
    "      print('{} Predicted translation: {}  {}'\n",
    "            .format(i+1, \"\".join(output[i].split(\" \")).replace(\"#\", \"\"), \n",
    "                    beam_score[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "smart-collector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 16, 1024]] : (3, 22, 128)\n",
      "(1, 3, 11) (1, 3, 11)\n",
      "(3, 11) (3, 11)\n",
      "Input: shibobrota\n",
      "1 Predicted translation: शिबोब्रोता  -9.176847457885742\n",
      "2 Predicted translation: शिबोबृता  -26.535249710083008\n",
      "3 Predicted translation: शिबोद्रोता  -31.69438934326172\n"
     ]
    }
   ],
   "source": [
    "beam_translate(u'shibobrota')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "excess-possession",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 16, 1024]] : (3, 22, 128)\n",
      "(1, 3, 5) (1, 3, 5)\n",
      "(3, 5) (3, 5)\n",
      "Input: ankan\n",
      "1 Predicted translation: अंकन  -4.155017852783203\n",
      "2 Predicted translation: अंकण  -8.40755558013916\n",
      "3 Predicted translation: अनकन  -12.241342544555664\n"
     ]
    }
   ],
   "source": [
    "beam_translate(u'ankan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "partial-scanning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.08165357,  0.05029269, -0.09704748, ..., -0.44273356,\n",
       "         -0.26297987,  0.01786955],\n",
       "        [ 0.01017082, -0.08278988, -0.04250364, ...,  0.16279984,\n",
       "          0.16259655, -0.08927383],\n",
       "        [-0.06464306, -0.01419611,  0.23512425, ...,  0.07390577,\n",
       "         -0.00276474, -0.21397069],\n",
       "        ...,\n",
       "        [-0.16647176, -0.30783314,  0.09533082, ..., -0.11026798,\n",
       "          0.06647341, -0.02215811],\n",
       "        [-0.03693328, -0.09875582, -0.01646317, ...,  0.21598664,\n",
       "         -0.24307843, -0.09363152],\n",
       "        [ 0.30616313,  0.14936265,  0.20444284, ...,  0.01357164,\n",
       "          0.16959912,  0.05045156]], dtype=float32)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.attention_mechanism.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "frozen-essay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "  fig = plt.figure(figsize=(10, 10))\n",
    "  ax = fig.add_subplot(1, 1, 1)\n",
    "  ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "  fontdict = {'fontsize': 14}\n",
    "\n",
    "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-nothing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
