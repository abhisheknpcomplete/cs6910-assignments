{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "competent-thermal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using numpy: 1.19.5\n",
      "Using tensorflow: 2.4.1\n",
      "Using tensorflow Addons: 0.12.1\n",
      "Using keras: 2.4.0\n",
      "Using pandas: 1.2.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU, SimpleRNN, SimpleRNNCell, LSTMCell, GRUCell\n",
    "from keras.models import Sequential\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(\"Using numpy:\",np.__version__)\n",
    "print(\"Using tensorflow:\",tf.__version__)\n",
    "print(\"Using tensorflow Addons:\",tfa.__version__)\n",
    "print(\"Using keras:\",keras.__version__)\n",
    "print(\"Using pandas:\",pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "alive-contemporary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded to Dataframes!\n"
     ]
    }
   ],
   "source": [
    "val_df = pd.read_csv(\"./lexicons/hi.translit.sampled.dev.tsv\", sep='\\t', header=None)\n",
    "train_df = pd.read_csv(\"./lexicons/hi.translit.sampled.train.tsv\", sep='\\t', header=None)\n",
    "test_df = pd.read_csv(\"./lexicons/hi.translit.sampled.test.tsv\", sep='\\t', header=None)\n",
    "print(\"Data Loaded to Dataframes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "affected-ancient",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LexDataset:\n",
    "    def __init__(self, input_tensor, target_tensor, inp_word_tokenizer, targ_word_tokenizer):\n",
    "        self.input_tensor = input_tensor\n",
    "        self.target_tensor = target_tensor\n",
    "        self.inp_word_tokenizer = inp_word_tokenizer\n",
    "        self.targ_word_tokenizer = targ_word_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "remarkable-trunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(words, tokenizer):\n",
    "    tensor = tokenizer.texts_to_sequences(words)\n",
    "    \n",
    "    #Pad the smaller words\n",
    "    tensor = pad_sequences(tensor, padding='post')\n",
    "    \n",
    "    # Return the tensor and the tokenizer\n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "undefined-williams",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the dataframe to \n",
    "def create_dataset(data_frame):\n",
    "    input_words = []\n",
    "    target_words = []\n",
    "    for x, y in zip(data_frame[1], data_frame[0]):\n",
    "        # Add words to respective lists\n",
    "        input_words.append(\"@\"+str(x)+\"#\")\n",
    "        target_words.append(\"@\"+str(y)+\"#\")\n",
    "    return input_words, target_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "velvet-branch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(data_frame_list):\n",
    "    # Initialize the tokenizer\n",
    "    input_tokenizer = Tokenizer(num_words = None, char_level = True)\n",
    "    target_tokenizer = Tokenizer(num_words = None, char_level = True)\n",
    "    \n",
    "    dataset_list = []\n",
    "    \n",
    "    for df in data_frame_list:\n",
    "        # Get the words list\n",
    "        input_words, target_words = create_dataset(df)\n",
    "        # Fit on the set of words\n",
    "        input_tokenizer.fit_on_texts(input_words)\n",
    "        target_tokenizer.fit_on_texts(target_words)\n",
    "        dataset_list.append((input_words, target_words))\n",
    "    \n",
    "    words_data = []\n",
    "    \n",
    "    target_tokenizer.index_word.update({0:\" \"})\n",
    "    input_tokenizer.index_word.update({0:\" \"})\n",
    "    \n",
    "    for (input_words, target_words) in dataset_list:\n",
    "        # Tokenize the words\n",
    "        input_tensor, inp_word_tokenizer = tokenize(input_words, input_tokenizer)\n",
    "        target_tensor, targ_word_tokenizer = tokenize(target_words, target_tokenizer)\n",
    "        words_data.append(LexDataset(input_tensor, target_tensor, inp_word_tokenizer, targ_word_tokenizer))\n",
    "\n",
    "    return words_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "appreciated-growth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Val input tensor: (4358, 20) | Shape of Val target tensor: (4358, 16)\n",
      "Shape of Train input tensor: (44204, 22) | Shape of Train target tensor: (44204, 21)\n",
      "Shape of Test input tensor: (4502, 18) | Shape of Test target tensor: (4502, 17)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset([val_df, train_df, test_df])\n",
    "\n",
    "print(f'Shape of Val input tensor: {np.shape(dataset[0].input_tensor)} | Shape of Val target tensor: {np.shape(dataset[0].target_tensor)}')\n",
    "print(f'Shape of Train input tensor: {np.shape(dataset[1].input_tensor)} | Shape of Train target tensor: {np.shape(dataset[1].target_tensor)}')\n",
    "print(f'Shape of Test input tensor: {np.shape(dataset[2].input_tensor)} | Shape of Test target tensor: {np.shape(dataset[2].target_tensor)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "pointed-beast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(tk, tensor):\n",
    "    for t in tensor:\n",
    "        if t != 0:\n",
    "            print(f'{t} ----> {tk.index_word[t]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "rational-grounds",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Input Word; index to character mapping\n",
      "2 ----> @\n",
      "1 ----> a\n",
      "4 ----> n\n",
      "13 ----> k\n",
      "1 ----> a\n",
      "4 ----> n\n",
      "3 ----> #\n",
      "\n",
      "Val Target Word; index to character mapping\n",
      "1 ----> @\n",
      "31 ----> अ\n",
      "10 ----> ं\n",
      "8 ----> क\n",
      "6 ----> न\n",
      "2 ----> #\n"
     ]
    }
   ],
   "source": [
    "print(\"Val Input Word; index to character mapping\")\n",
    "convert(dataset[0].inp_word_tokenizer, dataset[0].input_tensor[0])\n",
    "print()\n",
    "print(\"Val Target Word; index to character mapping\")\n",
    "convert(dataset[0].targ_word_tokenizer, dataset[0].target_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "center-mambo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 67)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_encoder_tokens = len(dataset[0].inp_word_tokenizer.index_word)+1\n",
    "num_decoder_tokens = len(dataset[0].targ_word_tokenizer.index_word)+1\n",
    "num_encoder_tokens, num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "universal-future",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_seq_length = max([np.shape(dataset[i].input_tensor)[1] for i in range(len(dataset))])\n",
    "max_decoder_seq_length = max([np.shape(dataset[i].target_tensor)[1] for i in range(len(dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "numeric-archives",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[0].targ_word_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-protest",
   "metadata": {},
   "source": [
    "## Tensorflow Dataset from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "precious-corps",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1381"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "embedding_dim = 16\n",
    "units = 128\n",
    "steps_per_epoch = np.shape(dataset[1].input_tensor)[0]//BATCH_SIZE\n",
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-montreal",
   "metadata": {},
   "source": [
    "#### Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "neural-franklin",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((dataset[1].input_tensor, dataset[1].target_tensor)).shuffle(len(dataset[1].input_tensor))\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-being",
   "metadata": {},
   "source": [
    "#### Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "completed-absorption",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = tf.data.Dataset.from_tensor_slices((dataset[0].input_tensor, dataset[0].target_tensor)).shuffle(len(dataset[0].input_tensor))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-franchise",
   "metadata": {},
   "source": [
    "#### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "similar-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices((dataset[2].input_tensor, dataset[2].target_tensor)).shuffle(len(dataset[2].input_tensor))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "younger-outdoors",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([32, 22]), TensorShape([32, 21]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(train_dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "legislative-yahoo",
   "metadata": {
    "id": "nZ2rI24i3jFg"
   },
   "outputs": [],
   "source": [
    "##### \n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "#     ##-------- LSTM layer in Encoder ------- ##\n",
    "#     self.lstm_layer = tf.keras.layers.LSTM(self.enc_units,\n",
    "#                                    return_sequences=True,\n",
    "#                                    return_state=True,\n",
    "#                                    recurrent_initializer='glorot_uniform')\n",
    "    \n",
    "    ##-------- LSTM layer in Encoder ------- ##\n",
    "        self.layer = GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "#     output, h, c = self.layer(x, initial_state = hidden)\n",
    "#     return output, h, c\n",
    "        output, h = self.layer(x, initial_state = hidden)\n",
    "        return output, h, None\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "#     return [tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))]\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "answering-secondary",
   "metadata": {
    "id": "60gSVh05Jl6l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (32, 22, 128)\n",
      "Encoder h vecotr shape: (batch size, units) (32, 128)\n"
     ]
    }
   ],
   "source": [
    "## Test Encoder Stack\n",
    "\n",
    "encoder = Encoder(num_encoder_tokens, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_h, sample_c = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder h vecotr shape: (batch size, units) {}'.format(sample_h.shape))\n",
    "# print ('Encoder c vector shape: (batch size, units) {}'.format(sample_c.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "covered-analysis",
   "metadata": {
    "id": "yJ_B3mhW3jFk"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, attention_type='luong'):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.attention_type = attention_type\n",
    "\n",
    "        # Embedding Layer\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        #Final Dense layer on which softmax will be applied\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # Define the fundamental cell for decoder recurrent structure\n",
    "#         self.decoder_rnn_cell = tf.keras.layers.LSTMCell(self.dec_units)\n",
    "        self.decoder_rnn_cell = GRUCell(self.dec_units)\n",
    "\n",
    "\n",
    "        # Sampler\n",
    "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "        # Create attention mechanism with memory = None\n",
    "        self.attention_mechanism = self.build_attention_mechanism(self.dec_units, \n",
    "                                                                  None, self.batch_sz*[max_decoder_seq_length], self.attention_type)\n",
    "\n",
    "        # Wrap attention mechanism with the fundamental rnn cell of decoder\n",
    "        self.rnn_cell = self.build_rnn_cell(batch_sz)\n",
    "\n",
    "        # Define the decoder with respect to fundamental rnn cell\n",
    "        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler=self.sampler, output_layer=self.fc)\n",
    "\n",
    "    \n",
    "    def build_rnn_cell(self, batch_sz):\n",
    "        rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnn_cell, \n",
    "                                      self.attention_mechanism, attention_layer_size=self.dec_units)\n",
    "        return rnn_cell\n",
    "\n",
    "    def build_attention_mechanism(self, dec_units, memory, memory_sequence_length, attention_type='luong'):\n",
    "        # ------------- #\n",
    "        # typ: Which sort of attention (Bahdanau, Luong)\n",
    "        # dec_units: final dimension of attention outputs \n",
    "        # memory: encoder hidden states of shape (batch_size, max_length_input, enc_units)\n",
    "        # memory_sequence_length: 1d array of shape (batch_size) with every element set to max_length_input (for masking purpose)\n",
    "\n",
    "        if(attention_type=='bahdanau'):\n",
    "            return tfa.seq2seq.BahdanauAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
    "        else:\n",
    "            return tfa.seq2seq.LuongAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
    "\n",
    "    def build_initial_state(self, batch_sz, encoder_state, Dtype):\n",
    "        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size=batch_sz, dtype=Dtype)\n",
    "        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n",
    "        return decoder_initial_state\n",
    "\n",
    "\n",
    "    def call(self, inputs, initial_state):\n",
    "        x = self.embedding(inputs)\n",
    "        outputs, _, _ = self.decoder(x, initial_state=initial_state, sequence_length=self.batch_sz*[max_decoder_seq_length-1])\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "scenic-garbage",
   "metadata": {
    "id": "DaiO0Z6_Ml1c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder Outputs Shape:  (32, 20, 67)\n"
     ]
    }
   ],
   "source": [
    "# Test decoder stack\n",
    "\n",
    "decoder = Decoder(num_decoder_tokens, embedding_dim, units, BATCH_SIZE, 'luong')\n",
    "sample_x = tf.random.uniform((BATCH_SIZE, max_decoder_seq_length))\n",
    "decoder.attention_mechanism.setup_memory(sample_output)\n",
    "# initial_state = decoder.build_initial_state(BATCH_SIZE, [sample_h, sample_c], tf.float32)\n",
    "initial_state = decoder.build_initial_state(BATCH_SIZE, sample_h, tf.float32)\n",
    "\n",
    "\n",
    "sample_decoder_outputs = decoder(sample_x, initial_state)\n",
    "\n",
    "print(\"Decoder Outputs Shape: \", sample_decoder_outputs.rnn_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-emergency",
   "metadata": {
    "id": "_ch_71VbIRfK"
   },
   "source": [
    "## Define the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "offshore-respect",
   "metadata": {
    "id": "WmTHr5iV3jFr"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "      # real shape = (BATCH_SIZE, max_length_output)\n",
    "      # pred shape = (BATCH_SIZE, max_length_output, tar_vocab_size )\n",
    "      cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "      loss = cross_entropy(y_true=real, y_pred=pred)\n",
    "      mask = tf.logical_not(tf.math.equal(real,0))   #output 0 for y=0 else output 1\n",
    "      mask = tf.cast(mask, dtype=loss.dtype)  \n",
    "      loss = mask* loss\n",
    "      loss = tf.reduce_mean(loss)\n",
    "      return loss  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-logistics",
   "metadata": {
    "id": "DMVWzzsfNl4e"
   },
   "source": [
    "## Checkpoints (Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "stone-board",
   "metadata": {
    "id": "Zj8bXQTgNwrF"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-great",
   "metadata": {
    "id": "8Bw95utNiFHa"
   },
   "source": [
    "## One train_step operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "earlier-might",
   "metadata": {
    "id": "sC9ArXSsVfqn"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_h, enc_c = encoder(inp, enc_hidden)\n",
    "\n",
    "\n",
    "        dec_input = targ[ : , :-1 ] # Ignore <end> token\n",
    "        real = targ[ : , 1: ]         # ignore <start> token\n",
    "\n",
    "        # Set the AttentionMechanism object with encoder_outputs\n",
    "        decoder.attention_mechanism.setup_memory(enc_output)\n",
    "\n",
    "    # Create AttentionWrapperState as initial_state for decoder\n",
    "    #     decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, [enc_h, enc_c], tf.float32)\n",
    "        decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, enc_h, tf.float32)\n",
    "        pred = decoder(dec_input, decoder_initial_state)\n",
    "        logits = pred.rnn_output\n",
    "        loss = loss_function(real, logits)\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-australian",
   "metadata": {
    "id": "pey8eb9piMMg"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "devoted-satellite",
   "metadata": {
    "id": "ddefjBMa3jF0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0.0 Loss 1.3407\n",
      "Epoch 1 Batch 1.0 Loss 1.0256\n",
      "Epoch 1 Batch 2.0 Loss 1.1095\n",
      "Epoch 1 Batch 3.0 Loss 0.9006\n",
      "Epoch 1 Batch 4.0 Loss 0.9051\n",
      "Epoch 1 Batch 5.0 Loss 0.8700\n",
      "Epoch 1 Batch 6.0 Loss 0.8163\n",
      "Epoch 1 Batch 7.0 Loss 0.7353\n",
      "Epoch 1 Batch 8.0 Loss 0.4466\n",
      "Epoch 1 Batch 9.0 Loss 0.4855\n",
      "Epoch 1 Batch 10.0 Loss 0.4019\n",
      "Epoch 1 Batch 11.0 Loss 0.4331\n",
      "Epoch 1 Batch 12.0 Loss 0.2803\n",
      "Epoch 1 Batch 13.0 Loss 0.2818\n",
      "Epoch 1 Loss 0.6894\n",
      "Time taken for 1 epoch 123.46099758148193 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    # print(enc_hidden[0].shape, enc_hidden[1].shape)\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fixed-stake",
   "metadata": {
    "id": "EbQpyYs13jF_"
   },
   "outputs": [],
   "source": [
    "def evaluate_sentence(sentence):\n",
    "    sentence = \"@\"+sentence+\"#\"\n",
    "\n",
    "    inputs = [dataset[0].inp_word_tokenizer.word_index[i] for i in sentence]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                          maxlen=max_encoder_seq_length,\n",
    "                                                          padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    inference_batch_size = inputs.shape[0]\n",
    "    result = ''\n",
    "\n",
    "#     enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size,units))]\n",
    "    enc_start_state = tf.zeros((inference_batch_size, units))\n",
    "    enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\n",
    "\n",
    "    dec_h = enc_h\n",
    "    dec_c = enc_c\n",
    "\n",
    "    start_tokens = tf.fill([inference_batch_size], dataset[0].targ_word_tokenizer.word_index['@'])\n",
    "    end_token = dataset[0].targ_word_tokenizer.word_index['#']\n",
    "\n",
    "    greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
    "\n",
    "    # Instantiate BasicDecoder object\n",
    "    decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder.rnn_cell, sampler=greedy_sampler, output_layer=decoder.fc)\n",
    "    # Setup Memory in decoder stack\n",
    "    decoder.attention_mechanism.setup_memory(enc_out)\n",
    "\n",
    "    # set decoder_initial_state\n",
    "#     decoder_initial_state = decoder.build_initial_state(inference_batch_size, [enc_h, enc_c], tf.float32)\n",
    "    decoder_initial_state = decoder.build_initial_state(inference_batch_size, enc_h, tf.float32)\n",
    "\n",
    "    ### Since the BasicDecoder wraps around Decoder's rnn cell only, you have to ensure that the inputs to BasicDecoder \n",
    "    ### decoding step is output of embedding layer. tfa.seq2seq.GreedyEmbeddingSampler() takes care of this. \n",
    "    ### You only need to get the weights of embedding layer, which can be done by decoder.embedding.variables[0] and pass this callabble to BasicDecoder's call() function\n",
    "\n",
    "    decoder_embedding_matrix = decoder.embedding.variables[0]\n",
    "\n",
    "    outputs, _, _ = decoder_instance(decoder_embedding_matrix, start_tokens = start_tokens, end_token= end_token, initial_state=decoder_initial_state)\n",
    "    return outputs.sample_id.numpy()\n",
    "\n",
    "def translate(sentence):\n",
    "    result = evaluate_sentence(sentence)\n",
    "    print(result)\n",
    "    result = dataset[1].targ_word_tokenizer.sequences_to_texts(result)\n",
    "    print(f'Input: {sentence}')  \n",
    "    print(f'Predicted translation: {\"\".join(result[0].split(\" \")).replace(\"#\", \"\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "organic-preparation",
   "metadata": {
    "id": "UJpT9D5_OgP6"
   },
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "hydraulic-sally",
   "metadata": {
    "id": "WYmYhNN_faR5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[51  8  3 19  2]]\n",
      "Input: okay\n",
      "Predicted translation: ओकाय\n"
     ]
    }
   ],
   "source": [
    "translate(u'okay')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-bishop",
   "metadata": {
    "id": "IRUuNDeY0HiC"
   },
   "source": [
    "## Use tf-addons BeamSearchDecoder \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-somalia",
   "metadata": {
    "id": "AJ-RTQ0hsJNL"
   },
   "outputs": [],
   "source": [
    "def beam_evaluate_sentence(sentence, beam_width=3):\n",
    "    sentence = \"@\"+sentence+\"#\"\n",
    "\n",
    "    inputs = [dataset[0].inp_word_tokenizer.word_index[i] for i in sentence]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                          maxlen=max_encoder_seq_length,\n",
    "                                                          padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    inference_batch_size = inputs.shape[0]\n",
    "    result = ''\n",
    "\n",
    "    enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size,units))]\n",
    "    enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\n",
    "\n",
    "    dec_h = enc_h\n",
    "    dec_c = enc_c\n",
    "\n",
    "    start_tokens = tf.fill([inference_batch_size], dataset[0].targ_word_tokenizer.word_index['@'])\n",
    "    end_token = dataset[0].targ_word_tokenizer.word_index['#']\n",
    "\n",
    "    # From official documentation\n",
    "    # NOTE If you are using the BeamSearchDecoder with a cell wrapped in AttentionWrapper, then you must ensure that:\n",
    "    # The encoder output has been tiled to beam_width via tfa.seq2seq.tile_batch (NOT tf.tile).\n",
    "    # The batch_size argument passed to the get_initial_state method of this wrapper is equal to true_batch_size * beam_width.\n",
    "    # The initial state created with get_initial_state above contains a cell_state value containing properly tiled final state from the encoder.\n",
    "\n",
    "    enc_out = tfa.seq2seq.tile_batch(enc_out, multiplier=beam_width)\n",
    "    decoder.attention_mechanism.setup_memory(enc_out)\n",
    "    print(\"beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 16, 1024]] :\", enc_out.shape)\n",
    "\n",
    "    # set decoder_inital_state which is an AttentionWrapperState considering beam_width\n",
    "    hidden_state = tfa.seq2seq.tile_batch([enc_h, enc_c], multiplier=beam_width)\n",
    "    decoder_initial_state = decoder.rnn_cell.get_initial_state(batch_size=beam_width*inference_batch_size, dtype=tf.float32)\n",
    "    decoder_initial_state = decoder_initial_state.clone(cell_state=hidden_state)\n",
    "\n",
    "    # Instantiate BeamSearchDecoder\n",
    "    decoder_instance = tfa.seq2seq.BeamSearchDecoder(decoder.rnn_cell,beam_width=beam_width, output_layer=decoder.fc)\n",
    "    decoder_embedding_matrix = decoder.embedding.variables[0]\n",
    "\n",
    "    # The BeamSearchDecoder object's call() function takes care of everything.\n",
    "    outputs, final_state, sequence_lengths = decoder_instance(decoder_embedding_matrix, start_tokens=start_tokens, end_token=end_token, initial_state=decoder_initial_state)\n",
    "    # outputs is tfa.seq2seq.FinalBeamSearchDecoderOutput object. \n",
    "    # The final beam predictions are stored in outputs.predicted_id\n",
    "    # outputs.beam_search_decoder_output is a tfa.seq2seq.BeamSearchDecoderOutput object which keep tracks of beam_scores and parent_ids while performing a beam decoding step\n",
    "    # final_state = tfa.seq2seq.BeamSearchDecoderState object.\n",
    "    # Sequence Length = [inference_batch_size, beam_width] details the maximum length of the beams that are generated\n",
    "\n",
    "\n",
    "    # outputs.predicted_id.shape = (inference_batch_size, time_step_outputs, beam_width)\n",
    "    # outputs.beam_search_decoder_output.scores.shape = (inference_batch_size, time_step_outputs, beam_width)\n",
    "    # Convert the shape of outputs and beam_scores to (inference_batch_size, beam_width, time_step_outputs)\n",
    "    final_outputs = tf.transpose(outputs.predicted_ids, perm=(0,2,1))\n",
    "    beam_scores = tf.transpose(outputs.beam_search_decoder_output.scores, perm=(0,2,1))\n",
    "\n",
    "    return final_outputs.numpy(), beam_scores.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-circumstances",
   "metadata": {
    "id": "g_LvXGvX8X-O"
   },
   "outputs": [],
   "source": [
    "def beam_translate(sentence):\n",
    "  result, beam_scores = beam_evaluate_sentence(sentence)\n",
    "  print(result.shape, beam_scores.shape)\n",
    "  for beam, score in zip(result, beam_scores):\n",
    "    print(beam.shape, score.shape)\n",
    "    output = dataset[0].targ_word_tokenizer.sequences_to_texts(beam)\n",
    "    output = [a[:a.index('#')] for a in output]\n",
    "    beam_score = [a.sum() for a in score]\n",
    "    print('Input: %s' % (sentence))\n",
    "    for i in range(len(output)):\n",
    "      print('{} Predicted translation: {}  {}'\n",
    "            .format(i+1, \"\".join(output[i].split(\" \")).replace(\"#\", \"\"), \n",
    "                    beam_score[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-seminar",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_translate(u'shibobrota')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-structure",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_translate(u'ankan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-script",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.attention_mechanism.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-subscriber",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "  fig = plt.figure(figsize=(10, 10))\n",
    "  ax = fig.add_subplot(1, 1, 1)\n",
    "  ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "  fontdict = {'fontsize': 14}\n",
    "\n",
    "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-number",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
